Id,Title,Like,URL,Author,Reasons,Year,Venues
1,end-to-end training of deep visuomotor policies,11,https://arxiv.org/abs/1504.00702,Sergey Levine|Chelsea Finn|Trevor Darrell|Pieter Abbeel,"The first convincing application of deep learning in robotics control~This was the paper which first seriously popularized robotics as a problem of interest to the deep learning community. Many of the previous papers that applied deep learning techniques to robotics tasks looked at the perception problem in isolation. This paper hinted at the fact that the entire stack|from perception to actuation|could potentially be recast as a learning problem without suffering a catastrophic degradation in sample efficiency.~This paper clearly deserves to be on this list as it will likely be the most highly-cited robot learning paper of the decade. The paper started a conversation about the viability of end-to-end learning in robotics. I believe that this paper is interesting|not because of theoretical or algorithmic novelty as others on the list|but because it reads like a systems paper. From this work we learn that: (1) locally linear models combined with reasonable kinodynamic planning is ""close enough"" for a variety of manipulation and locomotion tasks|(2) the power of deep learning is not yet in replacing classical planning components but more to facilitate generalization across task instances|and (3) the connections between policy learning and probabilistic inference. More importantly it also illustrates several key limitations: (1) carefully constructed tasks and observability|(2) account for non-linearities due to contact conditions|and (3) the data collection/reset process.~This is an excellent example of reinforcement learning applied to closed-loop visual control for challenging robotics tasks and a good example of the application of deep-learning to real-world robotics.~For me the first paper that really showed the potential of deep learning for robot movements.~This paper was one of the first demonstrations of applying Reinforcement Learning (RL) and policy search to a real world robotics task. It demonstrated the feasibility of learning new tasks with a very limited number of trials|on a real robot. Most impressively|the policy was trained on directly on the camera data from the robot.~This work has drawn attention to end-to-end learning with neural networks|which I think was the beginning of the big boom of the deep learning in robotics.  ~The paper by Sergey Levine|Chelsea Finn et al shows how the perception and control system of a vision-based manipulation robot can be trained jointly with a trajectory-centric reinforcement learning approach (building on the guided-policy search framework).  In particular|this paper promotes the idea of ""end-to-end training‚"" in robot learning|as well as the use of deep neural networks. Both of these aspects have been very influential in the area and inspired many follow-up works. Overall|this paper (or the series of works around this one) significantly pushed the state-of-the-art in robot learning and constitutes one of the most powerful results and methods in modern RL.~This paper uses deep reinforcement learning to get a PR2 to pretty robustly hang a coat hanger on a clothes rack|insert a block into a shape sorting cube|fit the claw of a toy hammer under a nail|and screw on a bottle cap.  There were no prior demonstrations used|the resulting network takes in raw camera images and outputs robot motor torques directly|and each task took less than 300 learning trials to train.  Especially given how wobbly/inaccurate PR2 arms are|that is quite impressive.~It introduced end-to-end training with impressive results going from pixels to torques on several interesting tasks.~This work has shown a robot performing a variety of contact-rich manipulation tasks with learned controllers that close the loop around RGB images. This work spawned a flurry of research in reinforcement and representation learning.",2016,Journal of Machine Learning Research
2,pilco: a model-based and data-efficient approach to policy search,8,http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf,Marc Peter Deisenroth|Carl Edward Rasmussen,One of the first papers to really take uncertainty seriously in the RL + robotics space. Probably the first paper to convince me that model-based RL is worthwhile to think about|even in hard-to-model robotics domains.~it is a nice answer to the problem of learning models. ~Demonstrates a way to efficiency learn a task through model-based reinforcement learning.~The paper shows how data-efficient model-based RL control methods can actually get. PILCO is an idea that in different variations is still around.~In principle|model based RL offers many advantages for robot learning|such as efficient use of data and the ability to predict in advance how a trajectory will roll out. In practice|however|getting model based RL to work has proved to be very difficult. In this work|the authors tackle a key difficulty F112 when optimizing a policy for a dynamics model that was learned from data|model errors get exploited by the optimization algorithm. A very elegant solution is proposed: uncertainty estimation should be incorporated into the decision making process|thereby discouraging the optimization to visit states where model uncertainty is high and the predictions are likely to be wrong. This intuitive idea is implemented using Gaussian processes|which offer a principled approach to modeling uncertainty in continuous dynamical systems. The resulting algorithm - PILCO - is demonstrated to be very efficient in sample complexity|improving upon the state of the art  by orders of magnitude. This paper introduced several key ideas that have since been implemented in many subsequent works on robot learning and model based RL.~The paper by Marc Deisenroth and Carl Rasmussen promotes the use of Gaussian processes (GPs) for model-based reinforcement learning and proposes the PILCO algorithm|one of the most influential algorithms in recent reinforcement learning. GPs are by now heavily used in control and robotics communities. While this paper wasn't the first to use GPs in this context|it's arguably one of the most influential ones. Moreover|this work addresses the problem of data-efficiency in RL|which is of crucial importance for RL in the real world (such as in robotics). The PILCO algorithms has since been used in many different applications and extended in many ways. I consider the PILCO algorithm (or the underlying approach to model-based RL) as one of the state-of-the-art methods in modern RL.~This paper showed in an impressive way how to leverage modern probabilistic methods and model-based reinforcement learning to enable fast policy search. It  has become THE reference modeling and inference in nondeterministic tasks. The authors use analytical gradients for efficient policy updates|thereby eschewing the typical problems related to sampling methods. The result is an approach that can learn the cart-pole swing up on a real device in about 20 seconds. If you are doing anything related to reinforcement learning with probabilistic methods|this is a must-read. ~PILCO is an extremely data-efficient model-based approach for learning policies. At the core of the approach is the Gaussian process transition model. This nonparametric Bayesian representation allows the robot to capture its uncertainty of the state transitions and thus reason about a distribution of potential models given the robot's past experiences. This approach allows the robot to compute analytical gradients for updating its polices and results in highly data efficient learning. PILCO is a great example for highlighting the benefits of using model-based approaches and capturing uncertainty. PILCO has been used for a number of robotics projects over the years|including the control of a low-cost robotic arm for stacking blocks using visual feedback.,2011,International Conference of Machine Learning
3,dynamical movement primitives: learning attractor models for motor behaviors,6,https://homes.cs.washington.edu/~todorov/courses/amath579/reading/DynamicPrimitives.pdf,Auke Jan Ijspeert|Jun Nakanishi|Heiko Hoffmann|Peter Pastor|Stefan Schaal,DMPs proved to be a very useful representation for robot learning. The paper gives the clearest presentation ten years after they were invented~It provides a detailed explanation of derivation and usage of dynamic movement primitives|a learning by demonstration method that encodes motion with a set of weighted kernel function|which offers great means of further learning|modification etc.|and has been extensively applied in robotics.~Foundation for motion planning using iterative learning methods~The right parametrization is often the key in a learning system. Dynamical movement primitives (Ijspeert|Nakanishi|Schaal|2003) are a very successful way to encode movements in robots. The idea is to use dynamical systems with desired properties|such as stable attractors or rhythmic solutions|as building blocks. This provides a low-dimensional parametrization and combining them linearly  allows for effective learning.  So far it was mainly used for learning from demonstration.~Not the first paper on Dynamical movement primitives|but a great update on DMP.~Dynamic Movement Primitives (DMPs) specify a way to model goal-directed behaviours as non-linear dynamical system with a learnable attractor behaviour. In this way|the movement trajectory can be of almost arbitrary complexity but remains well-behaved and stable. DMPs are interesting for robot learning as they provide a simple way to learn from demonstrations. The forcing term that shapes the movement trajectory is linear in a set of learnable weights. Any function approximator can be used to learn these parameters. Locally weighted regression has been of particular interest as it is a very easy one-shot learning procedure.,2013,Neural Computation (Volume 25|Issue 2)
4,a reduction of imitation learning and structured prediction to no-regret online learning,5,https://arxiv.org/abs/1011.0686,Stephane Ross|Geoffrey J. Gordon|J. Andrew Bagnell,"Dagger points to a problem that keeps popping up in everyone's research. Every robot learning person should know about it.~This paper provides the first formal analysis of the (dynamic) covariate shift problem|where the suboptimal execution behavior of a policy drives the system to different states than those observed during training. While the general problem itself was well-known at the time (""Behavioral Cloning: A Correction"" Michie 1995; Alvinn: ""An autonomous land vehicle in a neural network"" Pomerleau 1989)|a disciplined analysis was lacking in the community. Ross et al. use a regret analysis to analyze and theoretically control the effects of dynamic covariate shift. The theory and algorithmic tools proposed in this work are still an active area of research today.~Imitation learning is a very appealing approach to learning robot skills. This paper shows that the straightforward technique of 'behavioral cloning' - simply copying the expert demonstrations|is actually not a good idea in sequential tasks. The reason is due to an effect of accumulating errors - once the learning agent strays away from states seen in the demonstration|it's learned policy is no longer accurate|causing it to stray even further away from the demonstration. There beauty of the paper is in capturing this idea mathematically|using no regret theoretical framework|and suggesting a simple algorithmic solution to the problem. The method|dubbed Dataset Aggregation (DAgger)|asks for additional expert actions *on states visited by the policy*.
The idea of controlling the distribution shift between the expert and the learner has since been fundamental to robotic imitation learning|and has manifested in various other methods.~This paper proposes 'Dagger'|an online learning algorithm for sequential decision making problems - a ubiquitous problem especially in robotics. It solves the problem that a policy trained on expert demonstrations may encounter new states that it has not seen in the training data. This will lead to error propagation and finally divergence of the policy. To bound this error|the authors propose to keep aggregating data when rolling out a learned policy and keep retraining the policy on this new data. ~Introduces Data Aggregation and the general approach of viewing policy optimization as online learning. Formalizes the notion of interaction with an expert as surrogate objective to the usual policy optimization objective.",2011,14th International Conference on Artificial Intelligence and Statistics
5,alvinn: an autonomous land vehicle in a neural network,4,https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf,Dean A. Pomerleau,"On the theoretical side|the first paper to recognize covariate shift in imitation learning and provide a simple data-augmentation style strategy to improve it. On the implementation side|a real self-driving first that led to ""No Hands Across America"".~This work presents the first successful application of imitation learning to an autonomous system. I think this is a pivotal work in the early years of robot learning. ~This work was pioneering with respect to machine learning in robotics broadly|learning from demonstration specifically|and also autonomous driving. It applied a neural net to learn steering angles from examples of human driving (even online!)|way back in 1989. By today's deep learning standards the net was tiny (5 hidden units) and the sensor input extremely limited (30x32 image pixels)|but it worked... and at a time when robots rarely operated outside of the lab or factory|and machine learning was rarely deployed on real hardware. It is the first* example of using demonstration-based learning for high-stakes control|that required (comparatively) fast sampling (25Hz) and operated a large van at regular road speeds (20mph). The vehicle was a part of NavLab|which was the precursor to CMU's DARPA Grand and Urban Challenges entires in the early 2000's|and those challenges in turn played a big role in accelerating today's driverless car boom.

* To my knowledge!

...also|there actually are two papers (and my words above mix the two):
[first publication] D. Pomerleau. ALVINN: An Autonomous Land Vehicle in a Neural Network. In Advances in Neural Information Processing Systems|1989.
[real world driving results]  D. Pomerleau. Efficient Training of Artificial Neural Networks for Autonomous Navigation. Neural Compuation|3(1)|88-97|1991.
~This was probably the first real learning-based controller for an autonomous vehicle.  It pioneered techniques such as data-augmentation to handle the problem of reaching states on which it wasn't trained.",1989,MITP
6,autonomous helicopter aerobatics through apprenticeship learning,6,http://www.robotics.stanford.edu/~ang/papers/ijrr10-HelicopterAerobatics.pdf,Pieter Abbeel|Adam Coates and Andrew Y. Ng,"This paper demonstrated to many in the ML community the then under-appreciated fact that reinforcement learning could be a viable framework to address very complex control tasks.~The helicopter stunts achieved in this work are some of the most compelling examples in robotics of both imitation learning and reinforcement learning.  (The combination of the two is called apprenticeship learning.)  In this work|multiple|imperfect trajectory demonstrations are used to generate ideal trajectories|and then reinforcement learning is used to learn sequences of linear feedback controllers that reproduce those trajectories.  When people say things like ""but there haven't really been many successes in using reinforcement learning on *real* robots|right?"" you can point to this work and say|""sure there are!  Have you *seen* these crazy helicopter tricks?""~Still one of the coolest demos of robot learning.~This paper presents a beautiful and compelling demonstration of the strength of learning dynamical models and using optimal control to learn complex tasks on intrinsically unstable systems even if the learned models rather crude and the optimal controllers are based on linearization|both strong approximations of reality. Furthermore|it addresses the problem of learning from demonstrations and improving from such demonstrations to beat human performance. To the best of my knowledge|on of the first paper demonstrating the use of learning by demonstration|model learning and optimal control together to achieve acrobatic tasks.~Real application of methods based on inverse reinforcement learning to learn to control a helicopter in highly difficult aerobatic maneuvers.",2010,International Journal of Robotics Research
7,robotic grasping of novel objects using vision,4,http://pr.cs.cornell.edu/grasping/IJRR_saxena_etal_roboticgraspingofnovelobjects.pdf,Ashutosh Saxena|Justin Driemeyer|Andrew Y. Ng,A key paper in grasp learning. The approach is relatively simple in nature (hack to get grasp orientation|simple features as anchors)|but it was at the time a very clear example of an approach that starkly contrasted with mainstream grasping.~This is one of the first works in literature that utilized machine learning for the robotic manipulation problem. The proposed framework is still useful to design similar robot learning solutions. The particular importance of this work is to identify local features that are related to manipulation planning~This paper lead a generation of PhD students to reimagine how grasping|and manipulation more generally|could be approached as a machine learning problem. Treating the grasp learning problem as a supervised learning problem without explicit human demonstrations or reinforcement learning|Saxena and colleagues' work stood as an example of how manipulation could be approached from a perceptual angle. A decade before deep learning made a splash in robotics|this work showed how robots could be trained to manipulate previously unseen objects without a need for complete 3D or dynamics models. While the learning techniques and features may have changed|the general formulation still stands as the initial approach many researchers take when implementing a grasp planning algorithm.~One of the first papers using general visual features for grasping,2008,International Journal of Robotics Research
8,from skills to symbols: learning symbolic representations for abstract high-level planning,3,http://irl.cs.brown.edu/pubs/orig_sym_jair.pdf,George Konidaris|Leslie Pack Kaelbling|Tomas Lozano-Perez,"As we get better at low-level robotic control|the community will need to start thinking more about longer-horizon problems and how to smoothly flow between reasoning at different levels of abstraction.  This paper presents a theoretically-ground formal treatment of the problem|proves some nice stuff about what constitutes necessary and sufficient symbols for various types of planning|and shows some nice demos on a real robot.  It is by far the best analysis of hierarchical learning / planning that I know of and provides a much-needed theoretical foundation for moving this area of research forward.~There exists a representational gap between the continuous sensorimotor world of a robot and the discrete symbols used by advanced AI planning methods. Many existing studies typically assume the existence of precoded planning symbols|and investigate how to learn the relations between these pre-coded symbols and continuous world of the robot. Few others argue that symbols should be formed in relation to the experience of agents through their sensorimotor experience.  This paper presents a structured approach|which is built on Markov-decision process formalism|to discover symbolic abstract representations from low-level high-dimensional continuous sensorimotor experience. The learned symbols and rules can automatically and effectively expressed in PDDL|a canonical high-level planning domain language|enabling high-level planning with traditional off-the-shelf AI planners.~Abstraction is an important aspect of robot learning. This paper addresses the issue of learning state abstractions for efficient high-level planning. Importantly|the state abstraction should be induced from the set of skills/options that the robot is capable of executing. The resulting abstraction can then be used to determine if any plan is feasible. The paper addresses both deterministic and probabilistic planning. It is also a great example of learning the preconditions and effects of skills for planning complex tasks.
",2018,Journal of Artificial Intelligence Research
9,maximum entropy inverse reinforcement learning,3,https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf,Brian D. Ziebart|Andrew Maas|J.Andrew Bagnell|and Anind K. Dey,This is a seminal paper for IRL. It has not only become a standard way to think about IRL|but the observation model for a demonstration given the reward has propagated to many other related areas|like goal inference|human prediction|etc.~This work is one of the first to connect probabilistic inference with robot policy learning. Maximum Entropy Inverse Reinforcement Learning poses the classical Inverse Reinforcement Learning problem|well-studied for several years before this work|as maximizing the likelihood of observing a state distributing given a noisily optimal agent w.r.t an unknown reward function. The inference method|model|and general principles not only inspired future IRL works (such as RelEnt-IRL|GP-IRL|and Guided Cost Learning)|they also have been applied in Human Robot Interaction and general policy search algorithms.~This work provides a novel and useful approach to the problem of inverse reinforcement learning. It is commonly used in practice and has influenced many follow up works in modeling humans in human-robot interaction.,2008,AAAI Conference on Artificial Intelligence
10,reinforcement learning: an introduction,3,https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf,Richard S. Sutton and Andrew G. Barto,"It presents the definite theoretical basis of reinforcement learning|used widely in robotics.~Reinforcement learning is the branch of machine learning that is concerned with decision making under uncertainty|and can be treated as sitting at the intersection of stochastic optimal control theory and machine learning. As such|it is one of the primary tools that is used for learning on robots|where it has appeared in many forms from mobile robots learning to navigate|to manipulators learning to handle different kinds of objects. This book is really the primary text on reinforcement learning|and covers everything from the basic concepts in the field to more recent developments. It is a must-read for anyone interested in robot learning.~Lays the foundation for all RL work|and therefore often referred to as ""the book"". Should not be forgotten in times of DeepRL!~Somewhat repeating myself from the last suggestion: for learning robot behavior|reinforcement learning is an essential tool. While Sutton & Barto do not focus specifically on the case of robotics|their book is a very accessible text that nevertheless manages to cover many aspects|techniques|and challenges in reinforcement learning.~Great introductory text book to the underpinnings of of a lot of the modern approaches in ML/RL for robotics. ",2018,Book
11,movement imitation with nonlinear dynamical systems in humanoid robots,3,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.7189&rep=rep1&type=pdf,Auke Jan Ijspeert|Jun Nakanishi|Stefan Schaal,"First work that proproses practical movement primitive representation for robotics. Very concise paper: shows how much can be packed into 6 pages.~This paper introduced Dynamic Motor Primitives (DMPs) - a very prominent representation for robot motion used in many learning approaches. While originally introduced as an imitation learning approach|DMPs have gone on to become central to many reinforcement learning papers. Many modern approaches that end with the word ""primitive"" are descendants of this work|including Probabilistic Motion Primitives (ProMPs)|or Interaction Primitives.  ~In this work|a robust and scaleable movement primitive learning approach is proposed. The key insight is the embedding of motion trajectories in a 2nd order dynamical system. Goal attractors enable the generalization to different targets and simplify the learning of the model parameters from rewards. Complex motion can be learned through least squares regression from demonstrations.",2002,IEEE International Conference on Robotics and Automation (ICRA)
12,intrinsic motivation systems for autonomous mental development,3,http://www.pyoudeyer.com/ims.pdf,Pierre-Yves Oudeyer|Frederic Kaplan|and Verena V. Hafner,"This paper proposes exploration algorithms based on the idea of intrinsic motivations|in particular motivations to explore in order to maximise the learning progress of a robot. This is a prominent example of the work of the Developmental Robotics community that ties link between developmental psychology|neurosciences and concrete robotics implementation and shows that exploring with this approach to learn to predict action consequences (forward models) results in behavior that is organized and shows similarity with human behavior.~This article describes some of the first successful experiments about ""curious robots"" and intrinsic motivation. It is one of the foundational articles in the ""developmental robotics"" field and inspired hundreds of papers about intrinsic motivation in reinforcement learning.~This work contributes to the general question of obtaining life-long learning robotic systems. Large body of the existing robot learning literature mostly focus on methods that enable the robots to learn particular pre-defined skills and achieve particular tasks. Life-long learning|on the other hand|requires the robots to learn skills and adapt to situations that were not (and cannot be) foreseen.  Inspired from human development|intrinsic motivation is an important drive that guides the robots towards regions that can be most effectively and efficiently learned with the capabilities developed so far; exploiting metrics such as novelty|curiosity|diversity|etc. This paper|in particular|is a seminal study that exploits maximization of learning progress in a real robot that explores its continuous sensorimotor space. It nicely shows that the robot exhibits stage-like development|learning easy tasks first|and focusing to more complex problems later; progressively developing more advanced skills.",2007,IEEE Transactions on Evolutionary Computation (Volume 11|Issue 2)
13,supersizing self-supervision: learning to grasp from 50k tries and 700 robot hours,3,https://arxiv.org/abs/1509.06825,Lerrel Pinto|Abhinav Gupta,It demonstrates how you can scale end-to-end robot learning of grasping by using self-supervision without human involvement.~This paper demonstrated that it's possible to have a robot interact in a self-supervised way with the environment in order to learn useful tasks|like grasping. By running a robot for a long period of time|it's possible to collect enough data to train policies using simple algorithms. This lead the way for a lot of follow up work from Google and others|and is likely an area where we'll see a lot of interest in the future.~Pinto et al.|were the first paper to exploit deep learning techniques to process large amounts of data collected by a robot running 24x7 for significantly improving the grasping accuracy without making any object specific assumptions or requiring 3D models of objects. This paper inspired several works in using large scale data to learn intuitive physics|manipulation of deformable objects and also impressive grasping works such as Google's arm farm and DexNet.,2015,IEEE International Conference on Robotics and Automation (ICRA)
14,apprenticeship learning via inverse reinforcement learning,2,https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf,Pieter Abbeel|Andrew Y. Ng,Provided a convincing demonstration of the usefulness of inverse reinforcement learning~Key paper on inverse reinforcement learning.,2004,International Conference on Machine Learning
15,learning and generalization of motor skills by learning from demonstration,2,https://h2t.anthropomatik.kit.edu/pdf/Pastor2009.pdf,Peter Pastor|Heiko Hoffmann|Tamim Asfour|and Stefan Schaal,Not the first DMP paper|but the most understandable and with fixes to some annoying problems with the original formulation.  Incredibly simple idea|but that's the nice thing about it -- it is a great starting point for talking about what generalization means in policy learning and how a restricted policy representation with the right inductive bias can allow you to learn something meaningful from a single trajectory|as well as learn quickly from practice.~DMPs (Dynamic Movement Primitives) are a good representation for learning robot movements from demonstration|as well as for doing reinforcement learning based on demonstrations.  This paper explains a variant of the original DMP formulation that makes them stable when generalizing movements to accommodate new goals|or obstacles in the robot's path.  It then shows how the new DMPs can be used for one-shot learning of tasks such as pick-and-place operations or water serving.  More robust than just a trajectory|and less complex than learning with many trials|this is a nice tool to have in your robot learning toolkit.,2009,IEEE International Conference on Robotics and Automation (ICRA)
16,hindsight experience replay,2,https://arxiv.org/abs/1707.01495,Marcin Andrychowicz|Filip Wolski|Alex Ray|Jonas Schneider|Rachel Fong|Peter Welinder|Bob McGrew|Josh Tobin|Pieter Abbeel|Wojciech Zaremba,"A really nice|simple idea for learning parameterized skills (building on UVFAs) and efficiently dealing with sparse reward.  I think Learning Parameterized Motor Skills on a Humanoid Robot (Castro Da Silva et. al) has a much better description of the parameterized skill learning problem than the HER or UVFA papers|but the HER paper has better practical ideas.~HER addresses the issue of sample inefficiency in DRL|especially for those problems with sparse and binary reward functions. It has become one of the most effective algorithms for learning problems with multiple goals which have the potential to solve many challenging manipulation tasks. The idea of ""EVERY experience is a good experience for SOME task"" is a powerful insight that succinctly reflects how we teach our children to be lifelong learners. We should teach our robots the same way.",2018,Neural Information Processing Systems Conference (NeurIPS)
17,probabilistic robotics,2, http://www.probabilistic-robotics.org/,Sebastian Thrun|Wolfram Burgard|Dieter Fox,It laid out basis for robotics in uncertain real world.~Probabilistic Robotics is a tour de force|replete with material for students and practitioners alike.,2005,Book
18,policy gradient reinforcement learning for fast quadrupedal locomotion,2,https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/icra04.pdf,Nate Kohl|Peter Stone,"The work is practical in that it allowed the authors to	improve the
walking speed of Aibos|something essential to creating top-flight
robocup players.  The reason I adore this work and frequently cite it
in my talks on machine learning is the fantastic way it allowed the
robots to learn autonomously. In particular|for the Aibo robots to
succeed in robocup|they need to be able to localize on the field
based on their perception of provided markers. The authors enabled the
robots to measure their own walking speed leveraging this
capability. By marching a team of robots back and forth across the
width of the pitch|experimenting with and evaluating different gaits
each time|the robots were able to find movement patterns that
surpassed hand-designed ones. It's a beautiful example of exploiting
measurable quantities to drive learning---a key enabling technology
for robot learning.~The paper is one of the first impressive applications of policy gradient algorithms on real robots. The policy gradient algorithm is rather simple|but is able to optimize the gait of the AIBO robot efficiently.",2004,IEEE International Conference on Robotics and Automation (ICRA)
19,probabilistic movement primitives,2,https://papers.nips.cc/paper/5177-probabilistic-movement-primitives.pdf,Alexandros Paraschos|Christian Daniel|Jan Peters|and Gerhard Neumann,This and the following papers using ProMPs|because they provided a very nice formulation for representing probabilistic movement primitives. ProMPs have many advantages and I found them better than classical DMPs in many robotics applications|from gestures to whole-body manipulations.~This work proposes a probabilistic movement primitive representation that can be trained through least squares regression from demonstrations. The most important feature of this model is its ability to model coupled systems. Thus|through exploiting the learned covariance between limbs or other dimensions whole body motion can be completed and predicted. Also the approach provides a closed form solution of optimal feedback controller in each time step assuming local Gaussian models.  ,2013,Neural Information Processing Systems Conference (NeurIPS)
20,a survey on policy search for robotics,2,https://ieeexplore.ieee.org/document/8186816,Marc Peter Deisenroth|Gerhard Neumann|Jan Peters,A great unifying view on policy search~For learning optimal robot behavior|reinforcement learning is an essential tool. Whereas the standard textbook by Sutton & Barto mainly covers value-function based methods|this survey covers policy-based method that are very popular in robotics application|with a specific focus on a robotics context. ,2013,Book
21,relative entropy policy search,2,https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264,Jan Peters|Katharina Mülling|Yasemin Altün,This work introduced for the first time an analytical KL bound to policy search algorithms in order to make them more stable and efficient. This work started a whole branch of new policy search algorithms and also many deep RL algorithms are inspired by this idea. ~This work proposes an information theoretic gradient based policy learning learning algorithm with adaptive step sizes. This adaptive step sizes or learning rates are essential for real robot implementations where large jumps in policy updates might damage a real system. ,2010,AAAI Conference on Artificial Intelligence
22,trust region policy optimization,3,https://arxiv.org/abs/1502.05477,John Schulman|Sergey Levine|Philipp Moritz|Michael I. Jordan|Pieter Abbeel,The TRPO paper lead the way towards practical Reinforcement Learning (RL) for robotics (and other domains). It's much more sample efficient and robust than previous approaches|and scales to high dimensional continuous action spaces. It's become a very popular method for training RL algorithms applied to robotics|although its slowly being replaced with descendants like Proximal Policy Optimization (PPO). TRPO has played a big part in reviving RL for robotics.~The work discusses a practical policy optimization method that although can be brittle|it is commonly used in many machine learning and robotics systems. It is also an algorithm that has started conversations around reproducibility of machine learning algorithms.~Schulman et al.|introduced an iterative method for optimizing policies that guaranteed monotonic improvement. At the time|it was developed|it was significantly more stable (lower variance) than other on and off-policy methods such as Deep Q-learning for learning large non-linear policies. TRPO|to date requires substantially less parameter tuning than other deep reinforcement learning algorithms and its variants such as PPO are a popular choice.  While model-free learning algorithms have not met with big successes in robot learning|we might not be far away. ,2017,International Conference on Machine Learning
23,human-level control through deep reinforcement learning,2,https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf,Volodymyr Mnih|Koray Kavukcuoglu|David Silver|Andrei A. Rusu|Joel Veness|Marc G. Bellemare|Alex Graves|Martin Riedmiller|Andreas K. Fidjeland|Georg Ostrovski|Stig Petersen|Charles Beattie|Amir Sadik|Ioannis Antonoglou|Helen King|Dharshan Kumaran|Daan Wierstra|Shane Legg|Demis Hassabis,Because it demonstrates that with modern approaches to deep reinforcement learning one can achieve human-level performance even in end-to-end settings. ~Although not strictly robotics paper|this one made a breakthrough in end-to-end deep reinforcement learning. ,2015,Nature
24,model-agnostic meta-learning for fast adaptation of deep networks,2,https://arxiv.org/abs/1703.03400,Chelsea Finn|Pieter Abbeel|Sergey Levine,Most people probably wouldn't compare MAMAL with HER because the algorithms and the problems they address are vastly different; MAML tackles transfer learning while HER tackles sparse reward issues. But from certain perspective|HER and MAML can make a very complementary pair of parents. HER is like an encouraging parent who|in hindsight|thinks everything the child did is a useful learning experience. MAML|on the other hand|thinks in foresight and wants the child to only learn skills for future job prospects. Does that sound like your parents?~Model-Agnostic Meta-Learning (MAML) has been a paradigm shift in robotics and has been used in number of applications.,2017,International Conference on Machine Learning
25,autonomous helicopter control using reinforcement learning policy search methods,2,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.8751&rep=rep1&type=pdf,J. Andrew Bagnell|Jeff G. Schneider,"I think this is one of the first work that shows the potential of reinforcement learning in realistic robotics problems. This paper together with ""Inverted autonomous helicopter flight via reinforcement learning"" by Andrew Y. Ng|et.al. are useful to encourage students see the importance and impact of applying concepts and methods to real-world setting.~One of the first real demonstrations of RL on an actual robot performing a complex control problem.",2001,IEEE International Conference on Robotics and Automation (ICRA)
26,learning attractor landscapes for learning motor primitives,2,https://papers.nips.cc/paper/2140-learning-attractor-landscapes-for-learning-motor-primitives.pdf,Auke Jan Ijspeert|Jun Nakanishi|and Stefan Schaal,This is the basis for large body of work on learning movement primitives. This first paper on the topic was published by the same authors at ICRA 2002~Dynamical Movement Primitives have definitely been very influential in mainly learning from demonstration studies. They encode the demonstrated trajectory as a set of differential equations|and offers advantages such as one-shot learning of non-linear movements|real-time stability and robustness under perturbations with guarantees in reaching the goal state|generalization of the movement for different goals|and linear combination of parameters. DMPs can be easily extended with additional terms: e.g. memorized force and tactile profiles can be utilized in modulating learned movement primitives in difficult manipulation tasks that contain high degrees of noise in perception or actuation. DMPs are further intuitive|easy to understand and implement; and therefore have been widely used.,2003,Neural Information Processing Systems Conference (NeurIPS)
27,robot programming by demonstration,1,https://calinon.ch/papers/Billard-handbookOfRobotics.pdf,Aude Billard and Sylvain Calinon|Ruediger Dillmann|Stefan Schaal,Provides a clear presentation of robot learning from demonstration from the authors who made the approach popular,2008,Book
28,robot learning from demonstration,2,http://www.mcgovern-fagg.org/amy/courses/cs5973_fall2005/lfd.pdf,Atkeson and Schaal,Some the key ideas in learning for robotics.~An early work with a stunning actual robot result of learning to swing up a pendulum|on a real robot|from a very few demonstrations and a little bit of trial and error.,1997,International Conference of Machine Learning
29,belief space planning assuming maximum likelihood observations,1,http://groups.csail.mit.edu/robotics-center/public_papers/Platt10.pdf,Robert Platt Jr.|Russ Tedrake|Leslie Kaelbling|Tomas Lozano-Perez,This isn't a learning paper|but a planning paper.  Nonetheless|I feel the need to include it|as it strongly influenced my thinking about manipulation learning.  It was the first work that I had seen make POMDPs work for real robotics problems.  It teaches students that reasoning about uncertainty is important|but that you need to make the right assumptions in order to make it work for any reasonably sized problem.  Whereas PILCO aims to reduce model uncertainty|this work assumes a correct model and leverages information-gathering actions to reduce state uncertainty.  Thus|these papers are complementary when discussing uncertainty.,2010,Robotics: Science and Systems VI
30,stanley: the robot that won the darpa grand challenge,1,http://isl.ecst.csuchico.edu/DOCS/darpa2005/DARPA%202005%20Stanley.pdf,Sebastian Thrun|Mike Montemerlo|Hendrik Dahlkamp|David Stavens|Andrei Aron|James Diebel|Philip Fong|John Gale|Morgan Halpenny|Gabriel Hoffmann|Kenny Lau|Celia Oakley|Mark Palatucci|Vaughan Pratt|and Pascal Stang,There would not be this much focus on robotics and learning if not for self-driving cars. Self-driving cars would not be a thing without Stanley.,2006,Journal of Robotic Systems
31,applied nonlinear control,1,https://www.amazon.com/Applied-Nonlinear-Control-Jean-Jacques-Slotine/dp/0130408905,Jean-Jacques E Slotine|Weiping Li,It laid the basis for adaptive nonlinear control commonly used in robotic control.,2001,Book
32,on learning|representing and generalizing a task in a humanoid robot,1,https://www.cs.utexas.edu/users/sniekum/classes/RLFD-F15/papers/Calinon07.pdf,Sylvain Calinon|Florent Guenter and Aude Billard,First work to explicitly represent motion variance for motion generation. ,2007,IEEE Transactions on Systems|Man|and Cybernetics|Part B (Cybernetics) (Volume 27|Issue 2)
33,everyday robotic action: lessons from human action control,1,https://www.frontiersin.org/articles/10.3389/fnbot.2014.00013/full,Roy de Kleijn|George Kachergis|Bernhard Hommel,Roboticists are not the only researchers working on motion representation and generation. Researchers on human motor control approach the problem from a different angle|and their work has often served as an inspiration for me. This paper provides a very nice|easy to understand overview of some topics in the field of human action control|with many interesting citations to follow up. ,2014,Frontiers in NeuroRobotics
34,a survey of iterative learning control,1,https://ieeexplore.ieee.org/document/1636313,D.A. Bristow|M. Tharayil|A.G. Alleyne,The content of the paper provides the reader with a broad perspective of the important ideas|potential|and limitations of iterative learning control - ILC. Besides the design techniques|it discusses problems in stability|performance|learning transient behavior|and robustness.,2006,IEEE Control Systems Magazine (Volume 26|Issue 3)
35,efficient reinforcement learning with relocatable action models,1,https://www.aaai.org/Papers/AAAI/2007/AAAI07-090.pdf,Bethany R. Leffler|Michael L. Littman|Timothy Edmunds,"This paper from 2007 was the culmination of a thread of reinforcement 
learning research on relocatable action models.  The premise is that 
states can be clustered into types such that actions taken in states 
of the same type have similar effects.  This paper was the first to 
study implementation of such an idea on a real robot|and shows 
impressive results.  It's a great example of the creative robot 
demonstrations from Michael Littman's lab at a time when most learning 
was limited to simulation.",2007,AAAI Conference on Artificial Intelligence
36,aibo ingenuity,1,https://www.youtube.com/watch?v=y06TMZB-Qvk,Michael Littman|Tom Walsh|Ali Nourl|Bethany Leffler|Timothy Edmunds|Jill Littman|Cameron Detulleo|Christopher Metcalf|Jonathan Metcalf|Charles Isbell|Joni Isbell,"This brief video from 2007 summarizes some very creative 
demonstrations of learning on Aibo robots in an environment that 
requires exploration.  I love this work because they created a sort of 
""playpen"" for the robot that had interesting features at just the 
right level of difficulty to allow the robot to learn intereting 
behaviors - in the same way that playpen environments are designed for 
human infants.  It's a great example of the creative robot 
demonstrations from Michael Littman's lab at a time when most learning 
was limited to simulation.  Though not associated with any 
publication|some related research|also including robot experiments|
appears in 
https://link.springer.com/content/pdf/10.1007/s10994-010-5202-y.pdf",2007,YouTube
37,coordination of multiple behaviors acquired by vision-based reinforcement learning,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.472.7162&rep=rep1&type=pdf,Minoru Asada|Eiji Uchibe|Shoichi Noda|Sukoya Tawaratsumida|Koh Hosoda,"This paper is representative of a body of work from Minoru Asada's lab in the mid 1990's that was some of the first examples of machine learning in the robot soccer domain|some of the first reinforcement learning on real robots|some of the first RL from visual inputs|and some of the first multirobot learning|all wrapped into one.  They introduced several methods for aggressively reducing the sample complexity needed for learning on real robots|such as ""learning from easy missions.""",1994,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
38,optimization-based iterative learning for precise quadrocopter trajectory tracking,1,https://link.springer.com/article/10.1007/s10514-012-9283-2,Angela Schoellig. Raffaello D'Andrea,"The authors propose an iterative approach to improve the flight controller in a quadcopter during repetitive task execution. While at a high-level|the paper has the same general setting as recent work in policy learning and robotics|it takes a very different approach that is grounded in control theory and state estimation. In my opinion|this paper is one of the best examples of ""model-based"" learning in robotics from both an algorithmic and a systems perspective. The task studied is dynamic|has non-trivial dynamic disturbances|and the proposed control technique is theoretically justified while being simple enough to analyze---also it worked.",2012,Autonomous Robots Journal
39,simultaneous adversarial multi-robot learning,1,http://www.cs.cmu.edu/~mmv/papers/03ijcai-grawolf.pdf,Michael Bowling|Manuela Veloso,This paper is one of the earliest works demonstrating multiagent reinforcement learning on a real robot. The work was also an early demonstration of improving policies learned in a physics simulation using learning on robots (sim2real).  Finally|the video of the robots' learned policies was one of the first robot demo videos to be slowed down on playback rather than sped up.,2003,International Joint Conferences on Artificial Intelligence 
40,automatic gait optimization with gaussian process regression,1,https://www.ijcai.org/Proceedings/07/Papers/152.pdf,Daniel Lizotte|Tao Wang|Michael Bowling|Dale Schuurmans,"This paper is from the line of papers on Aibo Gate optimization 
started by Kohl and Stone in 2004.  This paper introduced the idea of 
using Gaussian process regression for learning so as to avoid local 
optima|make full use of all historical data|and explicitly model 
noise in gait evaluation.  The authors acheived impressive results for 
optimizing both speed and smoothness with dramatically fewer gait 
evaluations than prior approaches.",2007,International Joint Conference on Artificial Intelligence
41,learning object affordances: from sensory - motor coordination to imitation,2,https://ieeexplore.ieee.org/document/4456755,Luis Montesano|Manuel Lopes|Alexandre Bernardino|Member|IEEE|Jose Santos-Victor,This is a very good illustration of the concept of affordance|that present an approach to learn object related actions and exploit these learned affordances for imitation. Affordances is a key concept to present in robotics learning as it tightly integrate perception and action. I may have included Gibson's paper on affordances|but I preferred a concrete example of robotic application.~Affordances have been very influential in robotics in the last decade and this study laid the foundations of affordance-based robot learning research that emphasizes importance of exploration; and learning the relation between objects|actions and the observed effects. They showed how learned affordances can be used for goal-oriented action execution and for imitation.,2008,IEEE Transactions on Robotics (Volume 24|Issue 1)
42,embed to control: a locally linear latent dynamics model for control from raw images,1,https://arxiv.org/abs/1506.07365,Manuel Watter|Jost Tobias Springenberg|Joschka Boedecker|Martin Riedmiller,This work shows how the idea of representation learning fundamental in deep learning can take advantage of the robotics context. The learned representation is specifically constrained to be efficient in an optimal control setting and is made to produce simple|locally linear models from vision that made control efficient. This is also an excellent example on how to integrate learning while taking advantage of known efficient algorithms (optimal control) instead of going to monolithic end-to-end approaches.,2015,Neural Information Processing Systems Conference (NeurIPS)
43,robots that can adapt like animals,1,https://arxiv.org/abs/1407.3501,Antoine Cully|Jeff Clune|Danesh Tarapore|Jean-Baptiste Mouret,because it shows how you can leverage models in simulation for learning how to recover from damages|without necessarily re-learning the damaged model. Also|they learn in very few trials on the real robot|which is fundamental when working with real robots and experiments are expensive,2015,Nature
44,scalable techniques from nonparametric statistics for real time robot learning,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.2374&rep=rep1&type=pdf,Stefan Schaal|Christopher G. Atkeson|Sethu Vijayakumar,Milestone paper on model learning|which inspired me for the next decade to come. Subsumes much of the later work|e.g.|of Russ Tedrake.,2002,Applied Intelligence Journal
45,prioritized sweeping: reinforcement learning with less data and less time,1,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7C74DA6B9B7C60A26D9EAF62242C10B9?doi=10.1.1.28.7241&rep=rep1&type=pdf,Andrew W. Moore|Christopher G. Atkeson,Quintessential technical insights|super important up to today.,1993,Machine Learning Journal
46,biped dynamic walking using reinforcement learning,1,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=ABA8B9A9C6FC9C1B1AA05779A20B2714?doi=10.1.1.99.6229&rep=rep1&type=pdf,Hamid Benbrahim,Totally overlooked work that employs policy gradients with multiple nested CMAC neural networks. One could give Benbrahim credit for having done much of OpenAI's stuff 20 years earlier.,1997,University of New Hampshire
47,variational inference for policy search in changing situations,1,https://icml.cc/2011/papers/441_icmlpaper.pdf,Gerhard Neumann,Breakthrough work on policy search which preceded much following work on inference-based RL and is totally under appreciated by many authors who have (often knowingly) build upon it. It was kind of direction change in the field but is not given the proper credit.,2011,International Conference on Machine Learning
48,probabilistic inference as a model of planned behavior,1,https://pdfs.semanticscholar.org/ab01/cdba07ffc7163ea53640965e0057025a5456.pdf,Marc Toussaint,Very inspiring paper,2009,KI
49,teachable robots: understanding human teaching behavior to build more effective robot learners,1,https://www.cc.gatech.edu/~athomaz/papers/ThomazBreazeal-AIJ.pdf,Andrea L. Thomaz|Cynthia Breazeal,One of the first work emphasizing and studying the importance of understanding the behaviour of non-expert users in defining robot learning problems and solutions.,2006,Artificial Intelligence Journal
50,robot skill learning: from reinforcement learning to evolution strategies,1,https://www.degruyter.com/downloadpdf/j/pjbr.2013.4.issue-1/pjbr-2013-0003/pjbr-2013-0003.pdf,Freek Stulp|Olivier Sigaud,Synthesizing and unifying several generations of work using RL for robotics|converging to algorithms that are identifiable with evolution strategies coming from the evolutionary computation literature. ,2013,Paladyn
51,developmental robotics: from babies to robots,1,https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots,A Cangelosi|M Schlesinger,This book gave the first broad overview of the field of developmental robotics. ,2015,Book
52,robot learning from human teachers,1,https://www.morganclaypool.com/doi/abs/10.2200/S00568ED1V01Y201402AIM028,Sonia Chernova|Andrea L. Thomaz,A great introduction and overview to Learning from Demonstration,2014,Chapter in Synthesis Lectures on Artificial Intelligence and Machine Learning
53,a bayesian view on motor control and planning,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.3243&rep=rep1&type=pdf,Marc Toussaint|Christian Goerick,This paper nicely introduces the relation between classical robot control algorithms and probabilistic inference.  Not all of the introduced concepts have been novel but I think this paper contains nice examples and a good overview and helped me to start to think about robot control in a new way.,2010,Studies in Computational Intelligence (SCI|volume 264)
54,motion planning under uncertainty using iterative local optimization in belief space,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.307.6750&rep=rep1&type=pdf,Jur van den Berg|Sachin Patil|Ron Alterovitz,This paper presents on of the first efficient solutions for continuous PoMDPs.  Many follow up papers for solving PoMDPs used similar ideas (i.e.|trajectory optimization in belief state). While the application of this algorithm was mainly limited to simulation|solving continuous MDPs  is a very important topic for robotics. which I expect to also have much more impact in the future.,2012,International Journal of Robotics Research
55,mujoco (software),1,https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf,Emanuel Todorov|Tom Erez and Yuval Tassa,Mujoco (together with Bullet) is probably the most popular simulator for learning based robotics. It's built with robotics research in mind (as opposed to many other simulators that were initially built for games)|and consequently has a lot of flexibility around things that matter for robotics|like physical realism|sensors|actuation models|tendons|and so on. It has a clean and easy-to-use interface and is extremely fast|both which are useful features for iterating on new research.,2012,Software 
56,cad2rl: real single-image flight without a single real image,1,https://arxiv.org/abs/1611.04201,Fereshteh Sadeghi|Sergey Levine,The CAD2RL paper demonstrated that it was possible to train a policy|using Reinforcement Learning|entirely in simulation and zero-shot transfer it to a real world robot. It paved the way for a lot of follow up work on domain transfer and domain randomization for robotic perception and control.,2017,Robotics: Science and Systems Conference
57,algorithms for inverse reinforcement learning,1,https://ai.stanford.edu/~ang/papers/icml00-irl.pdf,Andrew Y. Ng|Stuart Russell,Another influential work that gives a new and useful perspective on inverse optimal control|and has many interesting followups including the PhD work of Pieter Abbeel.,2000,International Conference on Machine Learning
58,imagenet classification with deep convolutional neural networks,1,https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf,Alex Krizhevsky|Ilya Sutskever|Geoffrey E. Hinton,Because it significantly boosts perception and deep learning for robot vision these days is heavily relying on this work. ,2012,Communications of the ACM
59,generative adversarial nets,1,https://arxiv.org/abs/1406.2661,Ian J. Goodfellow|Jean Pouget-Abadie|Mehdi Mirza|Bing Xu|David Warde-Farley|Sherjil Ozair|Aaron Courville|Yoshua Bengio,Because it introduces a new way of learning that shows a substantially improved behavior.  ,2014,Neural Information Processing Systems Conference (NeurIPS)
60,deep residual learning for image recognition,1,https://arxiv.org/abs/1512.03385,Kaiming He|Xiangyu Zhang|Shaoqing Ren|Jian Sun,Because it introduces away to train substantially deeper networks and thus provides substantially better results.,2015, IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
61,is imitation learning the route to humanoid robots?,1,http://web.media.mit.edu/~cynthiab/Readings/schaal-TICS1999.pdf,Stefan Schaal,"I think this work is a seminal work on imitation learning|which is a fundamental framework in the field of robot learning.~This landmark review paper is a great introduction to learning from demonstration|learning from observation and imitation learning both from a computational and biological standpoint. It highlights three important issues in imitation learning|""efficient motor learning|the connection between action and perception|and modular motor control in form of movement primitives‚"". From the paper's abstract|""Computational approaches to imitation learning will also be described|initially from the perspective of traditional AI and robotics|but also from the perspective of neural network models and statistical learning research. Parallels and differences between biological and computational approaches to imitation will be highlighted.‚""",1999,Trends in Cognitive Sciences
62,policy search for motor primitives in robotics,1,https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf,Jens Kober|Jan Peters,This work was published before the recent AI boom. It presents impressive results of imitation and reinforcement learning|which are still remarkable now.,2009,Machine Learning Journal
63,iterative linearization methods for approximately optimal control and estimation of non-linear stochastic system,1,https://homes.cs.washington.edu/~todorov/papers/LiIntJCon07.pdf,W. LI|E. TODOROV,This paper presents one of the effective and fundamental optimal control framework for nonlinear systems. This framework (including DDP) and its extensions have been widely applied to in motion planning and generation in complex robotic systems. This work is quite influential in the field of motion generation and control of robotic systems. After the publication of this paper|there have been many follow-up studies on the application of this kind of optimal control approaches to robot motion control.,2007,International Journal of Control
64,an autonomous manipulation system based on force control and optimization,1,https://www.academia.edu/12910741/An_autonomous_manipulation_system_based_on_force_control_and_optimization,Ludovic Righetti|Mrinal Kalakrishnan|Peter Pastor|Jonathan Binney|Jonathan Kelly|Randolph C. Voorhies|Gaurav Sukhatme|Stefan Schaal,This paper presents a comprehensive description of a complete system architecture for autonomous manipulation of challenging objects covering from low-level control|sensing and calibration to high-level planning and behavior generation. This paper experimentally demonstrates robust and competitive performance in a wide variety of manipulation tasks on robot hardware in a real-world situation. ,2014,Autonomous Robots Journal
65,learning for control from multiple demonstrations,1,https://ai.stanford.edu/~ang/papers/icml08-LearningForControlFromMultipleDemonstrations.pdf,Adam Coates|Pieter Abbeel|Andrew Y. Ng,An early paper that made learning control popular and showed that ML based control methods can be successfully applied to the real world. ,2008,International conference on Machine Learning
66,qt-opt: scalable deep reinforcement learning for vision-based robotic manipulation,1,https://arxiv.org/abs/1806.10293,Dmitry Kalashnikov|Alex Irpan|Peter Pastor|Julian Ibarz|Alexander Herzog|Eric Jang|Deirdre Quillen|Ethan Holly|Mrinal Kalakrishnan|Vincent Vanhoucke|Sergey Levine,The paper shows that replay buffer based RL methods can be successfully applied to large scale robotic applications. We will see more of this kind of work in the future.,2018,conference on robot learning
67,discovery of complex behaviors through contact-invariant optimization,1,https://homes.cs.washington.edu/~todorov/papers/MordatchSIGGRAPH12.pdf,Igor Mordatch|Emanuel Todorov|Zoran Popovic,The paper demonstrates that with an accurate internal model|planning of complex behaviors including contacts and dynamic interaction with the environment is possible from scratch. I see it as an important result supporting the need for good internal representations|which in the case of real-world interactions need to be at least partially learned.,2012,ACM Transactions on Graphics
68,intrinsically motivated goal exploration processes with automatic curriculum learning,1,https://arxiv.org/abs/1708.02190,Sebastien Forestier|Yoan Mollard|Pierre-Yves Oudeyer,The paper shows how an agent/robot can find out by itself how to manipulate its environment from a simple intrinsic motivation. In my eyes|this is the first practical demonstration of Schmidhuber's idea on learning progress maximization which is probably one of the most powerful generic drives. The paper shows how an agent can  discover more and more complex interactions with an environment without a specific task in mind. I believe these kinds of studies are important steps towards an intelligently learning robot.,2017,arXiv
69,deep reinforcement learning in a handful of trials using probabilistic dynamics models,1,https://arxiv.org/abs/1805.12114,Kurtland Chua|Roberto Calandra|Rowan McAllister|Sergey Levine,Model-based reinforcement learning had the image of not performing well in comparison to model-free methods on complicated problems. However|conceptually|model-based methods have many advantages when it comes to data-efficiency and transfer between tasks. This paper shows that with ensemble models state-of-the-art robotic learning benchmarks (Gym-Mujoco environments) can be solved with high performance in significantly fewer steps. The data-efficiency is particularly important for real robot applications.,2018,Neural Information Processing Systems Conference (NeurIPS)
70,robotics|vision and control - fundamental algorithms in matlab,1,http://www.petercorke.com/MVTB/vision.pdf,Peter Corke,I've encouraged countless students to read this book. It provides a broad overview of robotics|with inline code samples that instantly generate interactive demos using the author's robotics toolbox for Matlab. I personally dislike Matlab|but the quality of the author's toolbox and its illustrative power were worth the Matlab pain.,2015,Book
71,pattern recognition and machine learning,1,http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf,Christopher M. Bishop,A wonderful overview of pattern matching/machine learning minus DNNs.,2006,Book
72,data-driven grasp synthesis-a survey,1,https://arxiv.org/abs/1309.2660.pdf,Jeannette Bohg|Antonio Morales|Tamim Asfour|Member|Danica Kragic,Most awesome review paper ;-),2016,IEEE Transactions on Robotics (Volume 30|Issue 2)
73,a simple learning strategy for high-speed quadrocopter multi-flips,1,https://www.flyingmachinearena.ethz.ch/wp-content/publications/2010/lupashin2010-AdaptiveFlips.pdf,Sergei Lupashin|Angela Schoellig|Michael Sherback|Raffaello D'Andrea,This work is the first to show highly agile|multiple flips with state-of-the-art quadrotors. The proposed learning strategy leverages first-principles modeling to obtain a suitable parameterization for the quadcopter multi-flips|and a systematic approach for finding optimal parameters from data. The method is thus an excellent example for combining the strength of both first-principles models and data-based learning. Each approach on its own (i.e. a purely model-based approach or a purely data-driven approach) would have had difficulty to succeed for this challenging application because modeling all effects relevant for high-speed flips is extremely challenging|and purely data-based approaches can't be applied (easily) because the problem is highly unstable. Another strength of the paper lies in the (conceptual) simplicity of the proposed learning strategy|which makes it widely applicable for learning problems in robotics.,2010,IEEE International Conference on Robotics and Automation (ICRA)
74,dynamic programming and optimal control (vol. i+ii),1,https://www.amazon.com/Dynamic-Programming-Optimal-Control-Vol/dp/1886529086,D.P. Bertsekas,"The optimal control formulation and the dynamic programming algorithm are the theoretical foundation of many approaches on learning for control and reinforcement learning (RL). In brief|many RL problems can be understood as optimal control|but without a-priori knowledge of a model. Thus|many algorithms and understanding in RL and robot learning build on optimal control. The series of books by Bertsekas provide an excellent introduction and reference into this field. While the first volume addresses primarily classical (model-based) optimal control|the second volume treats approximate dynamic programming|which includes addressing optimal control/dynamic programming problems with sampling-based methods. While these books do not directly target (machine) learning techniques|the underlying principles are key for addressing learning in robotics|and I thus consider these books as absolutely fundamental for this area.  (Coincidentally|the new edition of Bertsekas' textbooks|which is announced for this year|will be called ""Reinforcement Learning and Optimal Control‚"".)",2017,Book
75,planning and acting in partially observable stochastic domains,1,https://people.csail.mit.edu/lpk/papers/aij98-pomdp.pdf,Leslie Pack Kaelbling|Michael L. Littman|Anthony R. Cassandra,This paper provides an easy to understand introduction to MDP and POMDP|which is the basis for understanding Reinforcement Learning and Bayesian Reinforcement Learning ---two learning techniques commonly used to combine planning and learning in robotics. I usually ask beginning research students (honours / 1st year MPhil/PhD) to read this paper.,1998,Artificial Intelligence 101
76,reinforcement learning: a survey,1,https://arxiv.org/abs/cs/9605103.pdf,Leslie Pack Kaelbling|Michael L. Littman|Andrew W. Moore,"This work provides a relatively short and easy to understand introduction to Reinforcement Learning. Although rather old and therefore does not cover the new approaches to reinforcement learning|it covers the problem of RL very well. I usually ask beginning students interested in reinforcement learning to read this paper together with the more recent ""Reinforcement Learning in Robotics: A Survey"" by Jens Kober|Andrew Bagnell|and Jan Peters|and deep learning approaches to reinforcement learning.",1996,Journal of Artificial Intelligence Research
77,value iteration networks,1,https://arxiv.org/abs/1602.02867,Aviv Tamar|Yi Wu|Garrett Thomas|Sergey Levine|Pieter Abbeel,To my knowledge|this is the first paper that embeds a planner into a deep neural net framework|combining learning and planning in a more seamless manner. The paper provides some semantic on the convolution operation and shows the potential of such an approach. I think many useful methods in robotics could be derived from this work.,2017,Advances in Neural Information Processing Systems
78,reinforcement learning of motor skills with policy gradients,1,http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/Neural-Netw-2008-21-682_4867[0].pdf,Jan Peters|Stefan Schaal,This paper presents some of the early method to learn high-dimensional parameters|which is important in robot manipulation. It presents a non-trivial combination of the authors' earlier work in learning motor skills|that aside from learning how reinforcement learning can be used to learn high-dimensional parameters|it also provide a good short summary of relevant work (up to that point).,2008,Robotics and Neuroscience
79,trajectories and keyframes for kinesthetic teaching: a human-robot interaction perspective,1,http://www.cs.utexas.edu/users/sniekum/classes/RLFD-F16/papers/Akgun12.pdf,Baris Akgun|Maya Cakmak|Jae Wook Yoo|Andrea L. Thomaz,Most industrial robots are still programmed by manually jogging positions via a teach pendant|but kinesthetic (lead-by-the-nose) interfaces for specifying robot tasks are becoming more common.  This paper presents a user study comparing kinesthetic teaching via trajectory recording to teaching via demonstrating sparse keyframes that can be connected to perform a skill.  The authors show that both are useful--the latter for demonstrating goal-oriented skills|and the former for demonstrating means-oriented skills--and also discuss an intuitive hybrid interface that allows users to use both types in a single demonstration.  Simple yet practical methods like this for teaching robot skills are often ignored in academic papers.,2012,ACM/IEEE international conference on Human-Robot Interaction
80,learning grasping points with shape context,1,https://is.tuebingen.mpg.de/uploads_file/attachment/attachment/308/2009_ijra_bk.pdf,Jeannette Bohg|Danica Kragic,This is one of the first works in literature that utilized machine learning for the robotic manipulation problem. The proposed framework is still useful to design similar robot learning solutions. The particular importance of this work is to use a global representation of a target object (goal) for manipulation planning,2009,International Conference on Advanced Robotics
81,domain randomization for transferring deep neural networks from simulation to the real world,1,https://arxiv.org/abs/1703.06907,Josh Tobin|Rachel Fong|Alex Ray|Jonas Schneider|Wojciech Zaremba|Pieter Abbeel,The work focuses on one of the most important problems related to utilizing CNNs for robotics problems: transferring policies from simulations to real world. Effective solutions are presented together with promising results.,2017,IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
82,one-shot imitation learning,1,https://arxiv.org/abs/1703.07326,Yan Duan|Marcin Andrychowicz|Bradly C. Stadie|Jonathan Ho|Jonas Schneider|Ilya Sutskever|Pieter Abbeel|Wojciech Zaremba,This paper focuses on a very challenging problem in robot learning|which is obtaining a generalized policy of a task using a very limited user supervision. The presented framework has a great potential to design robot learning algorithms with realistic data expectations.,2017,Neural Information Processing Systems Conference (NeurIPS)
83,reinforcement learning and optimal control,1,http://www.mit.edu/~dimitrib/RL_Frontmatter-SHORT-INTERNET-POSTED.pdf,Dimitri P. Bertsekas,an accessible take on reinforcement learning that pairs well with the classic and influential book(s) on Dynamic Programming by Bertsekas,2019,Book
84,an algorithmic perspective on imitation learning,1,https://arxiv.org/abs/1811.06711,Takayuki Osa|Joni Pajarinen|Gerhard Neumann|J. Andrew Bagnell|Pieter Abbeel|Jan Peters,A focused overview of imitation learning and perspectives from some of the leaders in the field. Not a complete review but an excellent highlighting of important contributions in the field and perspective on future challenges.,2018,Foundations and Trends in Robotics
85,evolution of corridor following behavior in a noisy world,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.953&rep=rep1&type=pdf,Craig W. Reynolds,The work features the automatic synthesis of a symbolic robot controller in a non-deterministic environment via genetic programming. Despite being an early paper on robot learning it features a combination of many aspects that are often not found in modern papers|i.e.|(1) learning of explainable|symbolic code|(2) automatic sensor placement|(3) strong non-determinism. Reynolds even goes to great lengths to analyse the code generated by the evolutionary process and identifies a more general framework for how a good solution looks like. Structure  and interpretability play an important role in this paper.,1994,International Conference on Simulation of Adaptive Behavior
86,reinforcement learning for robot soccer,1,http://ml.informatik.uni-freiburg.de/former/_media/publications/gr_09.pdf,Martin Riedmiller|Thomas Gabel|Roland Hafner|Sascha Lange,This paper describes a decade's worth of research on reinforcement learning in the RoboCup competition by the group led by Martin Riedmiller. In essence|it describes the foundation of deep reinforcement learning before the modern hype started. This includes algorithms that have become a stable of deep RL|such as neural-fitted Q iteration. However|what makes this paper stand out even today is that it shows how to put together an entire system only out of RL-synthesized components. Martin and his team repeatedly won world championships using this approach and he later went on to become an influential figure at DeepMind.,2009,Auton Robot
87,evolving virtual creatures,1,https://www.karlsims.com/papers/siggraph94.pdf,Karl Sims,This paper demonstrated that machine learning and optimization do not have to be restricted to the generation of behavior of a robot. Rather|morphology and shape of an agent can be changed and optimized in an automatic fashion|too. In doing so|the paper created some of the first complex (an extremely impressive and life-like) examples of artificial creatures whose brain and body are fully synthesized. The video accompanying this paper is one of the best research videos out there. The paper has also spawned a number of follow ups|in particular by the group of Hod Lipson at Columbia.,1994,SIGGRAPH
88,the cityscapes dataset for semantic urban scene understanding,1,https://arxiv.org/abs/1604.01685,Marius Cordts|Mohamed Omran|Sebastian Ramos|Timo Rehfeld|Markus Enzweiler|Rodrigo Benenson|Uwe Franke|Stefan Roth|Bernt Schiele,Deep Learning has transformed the Robot Learning field completely in the last year. Whereas e.g. Computer Vision applications have access to large volumes of data (necessary to train DNNs)|Robotics applications generally suffer from the problem of collecting enough data for learning tasks. The KITTI dataset is one example of a great effort to provide relevant data for Robot (or autonomous vehicle) Learning.,2016,IEEE Conference on Computer Vision and Pattern Recognition
89,a large dataset to train convolutional networks for disparity|optical flow|and scene flow estimation,1,https://arxiv.org/abc/1512.02134.pdf,Nikolaus Mayer|Eddy Ilg|Philip Hausser|Philipp Fischer|Daniel Cremers|Alexey Dosovitskiy|Thomas Brox,Deep Learning has transformed the Robot Learning field completely in the last year. Whereas e.g. Computer Vision applications have access to large volumes of data (necessary to train DNNs)|Robotics applications generally suffer from the problem of collecting enough data for learning tasks. A very useful approach is to generate synthetic data. The FlowNet dataset is such an example.,2015,IEEE Conference on Computer Vision and Pattern Recognition
90,deep reinforcement learning,1,https://arxiv.org/abs/1810.06339,Yuxi Li,A huge bottleneck in Robot Learning is supervision during training. A breakthrough came when DNNs could be leveraged for reinforcement learning. This paper|I believe|is a good introduction.,2018,Under review for Morgan & Claypool: Synthesis Lectures in Artificial Intelligence and Machine Learning
91,guided policy search,1,https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf,Sergey Levine|Vladlen Koltun,This paper|as well as its successors|try to make learning complex behaviors from experience more tractable where only little data is available (which is of course a common situation for learning robots). In particular|I like that the paper combines well-established planning methods that have long been studied in AI and robotics with learning methods to establish a new procedure that combines advantages from both worlds.,2013,International Conference on Machine Learning
92,using inaccurate models in reinforcement learning,1,https://ai.stanford.edu/~ang/papers/icml06-usinginaccuratemodelsinrl.pdf,Pieter Abbeel|Morgan Quigley|Andrew Y. Ng,This is another example that tries to make (usually data-inefficient) reinforcement learning techniques more feasible on real systems. While many techniques have been proposed to use simulators to train robot behaviors|this paper stands out to me in that it combines very pragmatic observations (we need simulators to learn complex behaviors|but they are often inaccurate)|with precise theoretical insights into RL algorithms.,2006,International Conference on Machine Learning
93,model learning for robot control: a survey,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.438.8621&rep=rep1&type=pdf,Duy Nguyen-Tuong|Jan Peters,The only non-RL paper on my list :). Modelling of robots is part of both very classical control approaches as well as modern learning approaches. There are many excellent papers|I chose this one for providing a wide overview. One of my favourite papers on this topic|by the same authors|included in this survey combines insights from analytic modelling (allowing fast identification of a small set of parameters) with Gaussian process modelling (allowing precise and flexible modelling|but at the cost of requiring more data). I chose this survey instead|as it provides a wider overview and is thus something I would be more likely to suggest to a student or mentee to get a wider overview.,2011,Cognitive Science
94,a brief survey of deep reinforcement learning,1,https://arxiv.org/abs/1708.05866,Kai Arulkumaran|Marc Peter Deisenroth|Miles Brundage|Anil Anthony Bharath,This comprehensive survey greatly covers the field of deep reinforcement learning approaches and algorithms. It is written to be accessible to the wide audience and generally easy to understand.,2017,IEEE Signal Processing Magazine
95,world models,1,https://arxiv.org/abs/1803.10122,David Ha|Jürgen Schmidhuber,A different take on the deep model-based RL based on use of RNNs to model the world so that the agent can train by hallucination.,2018,Neural Information Processing Systems Conference (NeurIPS)
96,locally weighted learning and locally weighted learning for control,6,https://am.is.tuebingen.mpg.de/publications/atkeson_air_1997,Atkeson|C. G.|Moore|A. W.|and Schaal|S.,"These companion papers champion a statistical view of machine learning|and local lazy methods as a way to learn online from streaming data. The first paper is a survey that dives into the details of design decisions and training data implications. The companion paper relates locally weighted learning specifically to robot control|discussing the application of lazy learning to a variety of control formulations (forward and inverse models|policies and value functions) and for each detailing the authors' own hardware implementations. Each was a state-of-the-art control implementation in its own right (e.g. robotic billiards|devil stick)|at a time when machine learning rarely outperformed human hand design of controllers.

* Note the ""Title"" entry above contains two titles... ""AND"" is part of neither.~It provides the basics of locally weighted learning|which has been exploited in robotics to a great extent|and it is a sort of a backbone to many methods.~The combination of machine learning and control is quintessential for learning on robotic systems because learning on robots happens in the sensor-actuator-control-loop. This paper is one of the first to discuss the connection of learning and control and already points out many key research challenges|which have inspired many follow-up works. In my opinion|this paper represents important pioneering work|and it has had significant impact in creating and shaping this whole research area. ~This paper is an overview of (rather) early results in model learning for control. Beyond the particular results presented in the paper|which are already impressive and could perhaps be used to calibrate claims in current research in robot learning|the paper provides a thoughtful discussion and important insights on issues related to model learning for controlling mechanical systems from data generation issues to inverse/forward models|representations|etc. Certainly a must read for a compelling vision on robot learning.~The work has been the bacis for many works on robot motor learning.~Provides an in depth presentation of a new approach to regression that proved seminal and particularly useful for learning models in robotics",1997,Artificial Intelligence Review
97,reinforcement learning in robotics: a survey,1,https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf,Jens Kober|J. Andrew Bagnell|Jan Peters,"This survey was published at a time when there was still a significant gap between reinforcement learning and its practical employment on real robot hardware. For the majority of real world domains|rollouts are impractical to perform on actual hardware---because|for example|the state/action spaces are continuous|exploration can be dangerous|and rollouts take much longer when physically executed---plus often simulators are too dissimilar to the real world and hardware for what is learned to transfer well. To get reinforcement learning to be effective on a real hardware system|therefore|the devil is in the details|and this article addresses just that. Today the gap is narrowing|in part because of advances in computation|but also because of implementation ""tricks‚"" becoming codified. This article is a bit of a one-stop-shop for pulling together a lot of these tricks|and putting some theoretical rigor and thought behind why and when they work.",2014,International Journal of Robotics Research
98,an evolutionary approach to gait learning for four-legged robots,1,http://www.cs.cmu.edu/~mmv/papers/04iros-sonia.pdf,Sonia Chernova|Manuela Veloso,"This paper presents a clear and concrete mapping of genetic algorithms to a compelling hardware domain: Sony AIBO walking gait and the RoboCup soccer competition. The AIBO was an example of a platform where parameter tuning by hand is particularly tedious (54 parameters)|plus the platform was safe to have ""practice‚"" on its own overnight (i.e. without human supervision|a rarity for mobile robots)---offering an opportunity for fully autonomous and on-hardware optimization-based learning|where it was feasible for each generation to be evaluated (according to the fitness function) on the actual robot platform without human supervision or intervention. The learned walk that resulted outperformed all hand-tuned and learned walks that participated in (including that which won) the RoboCup 2003 competition.

** This recommendation is getting a bit into the weeds of specific algorithms---not quite sure if the list is planning to go that deep. It's a work I would present in a class|as a great example of a CS algorithm being translated for use on real robot hardware. Again|not quite sure if that sort of categorization fits the bill.",2004,International Conference on Intelligent Robots and Systems
99,modeling and learning walking gaits of biped robots,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.7142&rep=rep1&type=pdf,Matthias Hebbel|Ralf Kosse and Walter Nistico,"This paper describes the open loop modelling of a robot gait which mimics the human walking style. The authors develop a parameterized model for the leg and arm motions. They then compare various machine learning methods for finding the best parameters|i.e.|the ones that provide the best walk. The paper is very interesting as:
- it is one of the pioneer works on robot gait learning
- it rises many issues related to practical application of machine learning methods on real hardware
- it gives many insights (again|mainly practical) on how to develop a robot learning framework.
While the scientific contribution may be limited|the paper has a great importance for its presentation of practical issues. For this reason|I recommend its reading to young students interested in studying this topic for the first time.",2006,IEEE-RAS International Conference of Humanoid Robots
100,adjustable bipedal gait generation using genetic algorithm optimized fourier series formulation,1,https://ieeexplore.ieee.org/document/4059114,L. Yang|C. M. Chew|A. N. Poo|T. Zielinska,"This paper presents a method for optimally generating stable bipedal walking gaits|based on a Truncated Fourier Series Formulation with coefficients tuned by Genetic Algorithms. It also provides a way to adjust the stride-frequency|
step-length or walking pattern in real-time. The proposed approach can be adapted to the robot kinematic structure and to different terrains. 
As for the my previous suggestion|albeit simple the paper is useful to bridge the gap between robot kinematics (model-based design) and machine learning (model-free design). This is why I recommend it. ",2006,IEEE/RSJ International Conference on Intelligent Robots and Systems
101,cognitive developmental robotics: a survey,1,https://www.ece.uvic.ca/~bctill/papers/ememcog/Asada_etal_2009.pdf,Minoru Asada|Koh Hosoda|Yasuo Kuniyoshi|Hiroshi Ishiguro|Toshio Inui|Yuichiro Yoshikawa|Masaki Ogino|and Chisato Yoshida,I really like the overview of cognitive robotics|where learning can be applied and what the required parts are essential to learning and cognitive (artificial) systems.,2009,IEEE Transactions on Autonomous Mental Development
102,on the adaptive control of robot manipulator,1,https://journals.sagepub.com/doi/10.1177/027836498700600303,J.J. Slotine and W. Li	,I selected this paper for several reasons: 1) to remind us that learning system dynamics at the same time as performing control is an old control topic|known as adaptive control|and that fundamental results in adaptive control and especially fundamental limitations will certainly carry over in other learning problems (parametric or not) when learning and control time-scales are mixed|2) it is an incredibly elegant paper and its results have been further used on real robotic systems to provide fast adaptation to changing dynamics (e.g. plane catching experiments done at MIT using adaptive control in the early 90s) and they have also been extended to use non-parametric models (e.g. neural networks and Gaussian radial basis function) in subsequent papers.,1987,International Journal of Robotics Research
103,robot trajectory optimization using approximate inference,1,https://www.icml.cc/Conferences/2009/papers/271.pdf,Marc Toussaint,In this study a direct link between optimal control and the general framework of probabilistic inference is draw. The inference framework formulation allows to exchange the inference algorithms or solver and enables the prioritization of multiple concurrent objectives in a principled way. ,2009,International Conference on Machine Learning
104,reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es),1,https://ieeexplore.ieee.org/document/6790790,Hansen|Nikolaus|Sibylle D. MYller|and Petros Koumoutsakos.,This work proposes one of the most efficient black box optimizers based on 2nd order stochastic optimization. The strengths of this approach are that it only has one hyper parameter|the initial learning rate|and that it is simple which has resulted in numerous open access implementations in various  programming languages.,2003,Evolutionary Computation
105,forward models: supervised learning with a distal teacher,1,https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1603_1,Michael I. Jordan|David E. Rumelhart,This landmark provides a very good exposition on learning internal models of the environment for selecting actions. The idea of a distal teacher|in-between learning from demonstration (i.e. traditional supervised way of learning policies) and reinforcement learning (that relies on environment rewards)|is to provide supervision for learning policies by matching the predictions of a forward model with observation of the expert's demonstration. The paper also summarizes the advantages and challenges of learning both forward and inverse models.  Another fantastic and related paper is: MOSAIC Model for Sensorimotor Learning and Control.~explains that inverse model learning may be flawed|and one way how to circumvent this flaw,1992,Cognitive Science
106,optimality principles in sensorimotor control,1,https://homes.cs.washington.edu/~todorov/papers/TodorovNatNeurosci04.pdf,Emanuel Todorov,"From the paper's abstract: ""The sensorimotor system is a product of evolution|development|learning and adaptation-which work on different time scales to improve behavioral performance. Consequently|many theories of motor function are based on 'optimal performance': they quantify task goals as cost functions|and apply the sophisticated tools of optimal control theory to obtain detailed behavioral predictions. The resulting models|although not without limitations|have explained more empirical phenomena than any other class.‚"" This paper provides a solid theoretical perspective on how to think about control principally in terms of objectives. It makes a very good case for sensory feedback|utilizing which is a key aspect of robot learning works. ",2004,Nature Neuroscience
107,inverted autonomous helicopter flight via reinforcement learning,1,https://people.eecs.berkeley.edu/~jordan/papers/ng-etal03.pdf,Andrew Y. Ng|H. Jin Kim|Michael I. Jordan|and Shankar Sastry,totally transformativer,2003,International Symposium on Experimental Robotics
108,learning by watching: extracting reusable task knowledge from visual observation of human performance,1,https://www.researchgate.net/publication/2753425_Learning_by_Watching_Extracting_Reusable_Task_Knowledge_from_Visual_Observation_of_Human_Performance,Yasuo Kuniyoshi|Masayuki Inaba|Hirochika Inoue,One of the most significant works on Robot Imitation Learning. This set the stages for all works followed.,1996,IEEE Transactions on Robotics and Automation
109,representations for robot knowledge in the knowrob framework,1,https://www.sciencedirect.com/science/article/abs/pii/S0004370215000843,M. Tenorth|M. Beetz,A key paper showing the importances of ontology|specifically designed for robots.,2017,Artificial Intelligence
110,what are the computations of the cerebellum|the basal ganglia and the cerebral cortex?,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.7989&rep=rep1&type=pdf,Kenji Doya,A paper that document the functional models of types of learning algorithms in the brain: supervised learning|reinforcement learning and unsupervised learning.,1999,Neural Networks
111,learning to control a low-cost manipulator using data-efficient reinforcement learning,1,https://rse-lab.cs.washington.edu/papers/robot-rl-rss-11.pdf,Marc Peter Deisenroth|Carl Edward Rasmussen|Dieter Fox,While this was not the first nor the last publication from Deisenroth and colleagues on PILCO (probabilistic inference for learning control)|this is the paper that I remember my colleagues talking about that led me to learn about the approach. This paper was prescient bringing our attention to|and attempting to address|a number of problems in robot learning that still remain important today: data-efficient learning and transfer between related tasks. This paper has a had lasting impact on the field forming the basis of other impressive works in areas of robotics ranging from manipulation to learning underwater swimming gaits.,2011,Robotics: Science and Systems VII
112,automatic design and manufacture of robotic lifeforms,1,http://www.mit.edu/~hlipson/papers/design.pdf,Hod Lipson|Jordan B. Pollack,"This paper describes the first ""complete"" autonomous robotics design experiment|from the design to the 3D printing. Technically|the paper leverages all the work from the 1990's in evolutionary robotics|combined with 3D printing (recent at that time)|and demonstrates that a machine can design a machine. This paper inspired numerous researchers.",2000,Nature
113,gaussian processes for data-efficient learning in robotics and control,1,https://arxiv.org/abs/1502.02860,Marc Peter Deisenroth|Dieter Fox|Carl Edward Rasmussen,"This paper shows the power of model-based reinforcement learning for robot control. It nicely illustrates the power of Gaussian Processes to capture the uncertainty and demonstrates how to leverage it in a highly data-efficient reinforcement learning algorithm. Overall|PILCO (the algorithm described in this paper) might be the most data-efficient algorithm I know. 

Please note that conference versions of this paper were published at ICML (2011 - PILCO: A model-based and data-efficient approach to policy search) and RSS (2011 - Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning).",2017,IEEE Transactions on Pattern Analysis and Machine Intelligence
114,abandoning objectives: evolution through the search for novelty alone,1,https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf,Joel Lehman and Kenneth O. Stanley,This works nicely demonstrates that optimizing a reward function is not necessarily the best way to find a solution in a complex search space (especially when the search space is deceptive). It proposes to replace the reward function by a behavioral novelty score|which echoes many of the work in developmental robotics. The experiments described in this paper led to an inspirational book (Why greatness cannot be planned: The myth of the objective|Springer|2015).,2011,Evolutionary Computation
115,resilient machines through continuous self-modeling,1,https://science.sciencemag.org/content/314/5802/1118.abstract,Josh Bongard|Victor Zykov|and Hod Lipson,This article shows the potential of active model learning for adaptation|and applies it to damage recovery for a legged robot. While the technique was not new for robotics (active model learning)|the paper pushed by identifying more than parameters (e.g.|the presence of a leg). The application to damage recovery inspired a lot of my own work.,2006,Science
116,an introduction to deep reinforcement learning.,1,https://arxiv.org/abs/1811.12560,Vincent Francois-Lavet|Peter Henderson|Riashat Islam|Marc G. Bellemare|Joelle Pineau,There have been astounding achievements in Deep Reinforcement Learning in recent years with complex decision-making problems suddenly becoming solvable. This book is written by experts in the field and on top of that|it is free!,2018,Foundations and Trends in Machine Learning
117,particle filter networks with application to visual localization,1,https://arxiv.org/abs/1805.08975,Peter Karkus|David Hsu|Wee Sun Lee,makes clear how the algorithmic ideas from before and end-to-end learning can be combined,2018,Proceedings of The 2nd Conference on Robot Learning
118,robotic grasping of novel objects,1,https://papers.nips.cc/paper/3010-robotic-grasping-of-novel-objects.pdf,Ashutosh Saxena|Justin Driemeyer|Justin Kearns|Andrew Y. Ng,This has been seminal work which showed that object grasping can be learned in a supervised manner from labeled training images. It spawned a string of work in this area culminating in current approaches that use entire robot farms to collect training data. ,2007,Advances in Neural Information Processing Systems 
119,adaptive representation of dynamics during learning a motor task,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.8137&rep=rep1&type=pdf,Reza Shadmehr and Ferdinando A. Mussa-lvaldi,The reason why I picked these articles and books  is because I think that robot learning cannot be separated from the cognitive architecture supporting the learning processes. The first two reference highlight the importance and role of embodiment (in humans and robots) and the fact that in physical systems part of the learning process is embedded in the morphology and material.,1994,The Journal of Neuroscience
120,how the body shapes the way we think: a new view of intelligence,1,https://pdfs.semanticscholar.org/2910/099b7a7c555af9f14bfb2bc20e9475d0588f.pdf,Rolf Pfeifer and Josh Bongard,The reason why I picked these articles and books  is because I think that robot learning cannot be separated from the cognitive architecture supporting the learning processes. The first two reference highlight the importance and role of embodiment (in humans and robots) and the fact that in physical systems part of the learning process is embedded in the morphology and material. ,2006,How the Body Shapes the Way We Think: A New View of Intelligence
121,a roadmap for cognitive development in robots,1,https://www.springer.com/gp/book/9783642169038,David Vernon|Claes von Hofsten|and Luciano Fadiga,David Vernon's two references highlight the importance of the architecture and the role of its components in learning how to execute and interpret actions. The first|co-authored with a Neuroscientist (Luciano Fadiga) and a Developmental Psychology (Claes von Hofsten) highlights how the development of sensory|motor and cognitive skills in childhood can be analyzed as a process supporting the acquisition of complex behaviors  in humans and artificial systems as well as the parallel between action execution and action understanding in the acquisition of cognitive skills (the mirror neuron approach).,2011,Book
122,artificial cognitive systems: a primer,1,http://www.vernon.eu/publications/14_Vernon_Artificial_Cognitive_Systems_Preamble.pdf,David Vernon,David Vernon's two references highlight the importance of the architecture and the role of its components in learning how to execute and interpret actions. The second is a must-read book for who is interested in the field of artificial cognitive systems.,2014,Book
123,muscleless motor synergies and actions without movements: from motor neuroscience to cognitive robotics,1,https://www.ncbi.nlm.nih.gov/pubmed/29903532,Mohan V.|Bhat A.|Morasso P.,The last is a recent article reporting the state of art about the relationship between how to execute and how to imagine (an aspect which has important consequences in robot learning).,2019,Physics of life reviews
124,a new approach to linear filtering and prediction problems,1,http://www.unitedthc.com/DSP/Kalman1960.pdf,R. E. Kalman,"The paper that introduced the Kalman Filter: probably the most used inference algorithm in science and engineering. The paper *also* introduced the linear quadratic regulator as a bonus: probably the most used optimal control algorithm. It is clearly written and easy to understand.~Important to point out that this is a Bayesian (probabilistic) approach|long before Bayesian approaches became popular in ML.
",1960,Transactions of the ASME–Journal of Basic Engineering
125,agnostic system identification for model-based reinforcement learning,1,https://arxiv.org/abs/1203.1007,Stephane Ross|J. Andrew Bagnell,Formalizes the practice of performing system id|followed by attempting the task with an optimal controller|and repeating until the system works. Powerful technique that works and provides performance guarantees even when the true system model cannot be realized.,2012,International Conference on Machine Learning
126,square root sam: simultaneous localization and mapping via square root information smoothing,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.474.756&rep=rep1&type=pdf,Frank Dellaert|Michael Kaess,This paper|as well as the follow up iSAM2 paper|focuses on treating localization and mapping problems as nonlinear least squares and then optimizing as efficiently as possible. This is a powerful technique that serves as the backbone for many SAM solvers|and|can be applied more generally to all sorts of inference problems in robotics. Techniques building on this work have been used in planning|manipulation|and control.,2006,Intl. J. of Robotics Research
127,using local trajectory optimizers to speed up global optimization in dynamic programming,1,http://www.cs.cmu.edu/~cga/papers/local.pdf,Christopher G. Atkeson,A forerunner to guided policy search: introduces the idea of using trajectory optimizers to speed up policy learning. ,1994,Neural Information Processing Systems
128,the coordination of arm movements: an experimentally confirmed mathematical model,1,https://pdfs.semanticscholar.org/8237/66d74febae73bdafa91c63f5c29f3c8c68e5.pdf,Tamar Flash|Neville Hogans,"This paper is part of a set of papers that outlines important points about synergy formation in neuroscience and robotics along a fil-rouge during the last 40 years: a) the coordination of multiple joints in goal directed movement|b) the characterization of ""biological motion‚""|c) the equilibrium point hypothesis|d) the role of force fields for motor coordination|e) the extension of the equilibrium point hypothesis from real (overt) movements to covert (real) movements|f) the characterization of synergy formation as the simulation of an internal body model. In particular|this paper explained the bell-shape of the speed profile in human arm reaching movements in terms of optimal control. The observation had been first made in Morasso P (1981) Spatial control of arm movements. Experimental Brain Research.",1985,Journal of Neuroscience
129,affordances in psychology|neuroscience and robotics: a survey,1,https://ieeexplore.ieee.org/document/7523298,Lorenzo Jamone|Emre Ugur|Angelo Cangelosi|Luciano Fadiga|Alexandre Bernardino|Justus Piater and Jose Santos-Victor,"Affordances is an important term for robot learning|but also one that tends to be overloaded and can lead to confusion. If an object allows an agent to perform an action|then the object is said to afford the action to that agent. Affordances can generally be learned autonomously and are thus a fundamental aspect of self-supervised learning for autonomous robots. The nuances of the term however are still widely discussed in robotics and other fields. As a result|one should be aware of the ambiguity and different perspectives regarding the term when talking about affordances. This survey paper discusses some of the nuanced interpretations of the term affordances. 
",2018,IEEE Transactions on Cognitive and Developmental Systems
130,assessing grasp stability based on learning and haptic data,1,https://www.it.lut.fi/mvpr/data/Bekiroglu-TRO2011.pdf,Yasemin Bekiroglu|Janne Laaksonen|Jimmy Alison Jurgensen|Ville Kyrki and Danica Kragic,"""Learning to grasp"" can actually imply a lot of different learning problems. We often think about grasp synthesis|i.e.|the problem of determining where to place the hand to achieve a stable grasp (I strongly recommend reading Data-Driven Grasp Synthesis- a Survey for more on this topic). This paper focuses on the important problem of using multiple sensor modalities to determine if an executed grasp attempt resulted in a stable grasp. As robot learning researchers|it is important to consider how problems can be approached from different directions and how different information sources can be incorporated and change the problem. One should also think about robustness and consider how learning factors into monitoring skill executions for errors.
",2011,IEEE Transactions on Robotics
131,grounding semantic categories in behavioral interactions: experiments with 100 objects,1,https://www.eecs.tufts.edu/~jsinapov/papers/Sinapov_RAS2014.pdf,Jivko Sinapov|Connor Schenck|Kerrick Staley|Vladimir Sukhoy|Alexander Stoytchev,"Interactive perception and multimodal sensing are fundamental aspects of robotics and robot learning. The ability to execute actions to interact with the environment provides robots with a rich source of information|especially when combined with haptic|vision|and audio feedback. Grounding the object representations in the robot's actions provides the robot with representations that are not only well suited for future manipulation tasks|but that the robot can estimate through autonomous experimentation. The paper also touches on actively selecting actions to quickly reduce uncertainty.
",2012,Robotics and Autonomous Systems
132,optimal control and estimation,1,https://www.amazon.com/Optimal-Control-Estimation-Dover-Mathematics/dp/0486682005,Robert Stengel,Robotics Learning Practitioners must be aware of and understand Optimal Control. :) ,1994,Book
133,estimation of inertial parameters of rigid body links of manipulators,1,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.989.7461&rep=rep1&type=pdf,Christopher G. Atkeson|Chae H. An|John M. Hollerbach,For an early example of model based learning. ,1986,International Journal of Robotics Research
134,policy gradient methods for reinforcement learning with function approximation,1,https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf,Richard S. Sutton|David McAllester|Satinder Singh|Yishay Mansour,Revived policy gradient methods for RL|and for trajectory-based RL in particular in follow up work,2000,Advances in Neural Information Processing Systems
135,using local models to control movement,1,https://pdfs.semanticscholar.org/1985/de1691024c1ca6bd416e0b2007be6aaa0bce.pdf,Christopher G. Atkeson,seminal work on using learned local linear models for robot learning,1990,Advances in Neural Information Processing Systems
136,automatic programming of behavior-based robots using reinforcement learning,1,https://www.aaai.org/Papers/AAAI/1991/AAAI91-120.pdf,Sridhar Mahadevan and Jonathan Connell,"This is a seminal paper that really broke open the application of RL to 
learning policies. It was the first that I am aware of|and certainly 
the earliest high-profile case. It blew a lot of minds when it came out.",1991,Artificial Intelligence
137,a robot controller using learning by imitation,1,https://www.researchgate.net/publication/2652979_A_Robot_Controller_Using_Learning_by_Imitation,Gillian Hayes|Yiannis Demiris,"This paper introduced learning from imitation. This has proved useful in 
and of its own (i.e.|as a means for non-expert users to program 
robots)|and also as the means for initializing robot controllers (most 
prominently by Schaal and later Peters) to a reasonable policy that is 
later refined by learning. Schaal's work on this was probably more 
influential but it was preceded|and possibly inspired by|Gillian Hayes.
",1995,Neural Information Processing Systems Conference (NeurIPS)
138,natural actor critic,1,https://homes.cs.washington.edu/~todorov/courses/amath579/reading/NaturalActorCritic.pdf,Jan Peters|Sethu Vijayakumar|and Stefan Schaal,This paper established policy-search approaches as the algorithms most naturally (!) suited for motor skill learning|in part by pairing them with DMPs. That combination was a major advance and accounts for almost all notable successful instances of robot motor skill learning until deep nets. All existing robot policy learning algorithms - including the latest|most fashionable deep approaches - are technically descended from this algorithm|and many researchers were inspired by its immense success.,2008,European Conference on Machine Learning
139,sequential composition of dynamically dexterous robot behaviors,1,https://deepblue.lib.umich.edu/bitstream/handle/2027.42/67990/10.1177_02783649922066385.pdf,Robert R. Burridge|Alfred A. Rizzi|and Daniel E. Koditschek,Although not a recent work|this paper introduces the idea of solving complex robotic tasks by chaining together several local feedback controllers that are conditioned on state information. If safety guarantees are maintained for each of the local controllers|then this property can also hold for their composition. This is an important principle that can still be widely applied to many robotics tasks.,1999,The International Journal of Robotics Research
140,map learning with uninterpreted sensors and effectors,1,https://www.sciencedirect.com/science/article/pii/S0004370296000513,David Pierce|and Benjamin J. Kuipers,This was an important paper in developmental robotics|which showed by means of an extended worked example that complex state and action representations can be derived for a robot from raw|unstructured signals. This shows it is possible to build a hierarchy of representations|leading to agents that can build simple maps with behaviours such as wall-following controllers. Particularly in the modern context of deep learning|it is very instructional that this is all achieved in this paper by using simple techniques to exploit the structure and regularities in the inputs and outputs of the robot.,1997,Artificial Intelligence
141,experiments in synthetic psychology,1,https://www.amazon.com/Vehicles-Experiments-Psychology-Valentino-Braitenberg/dp/0262521121,Valentino Braitenberg,This book gave rise to the concept of Braitenberg Vehicles. It follows a sequence of simple thought experiments|starting with the idea that light sensors attached to motors can generate a machine that can simulate a desire for or aversion to light. These concepts are made progressively more complex|ranging up to machines that can display sophisticated behaviours. This is an interesting foundational take on emergent intelligence.,1986,MIT Press
142,learning agile and dynamic motor skills for legged robots,1,https://robotics.sciencemag.org/content/4/26/eaau5872,Jemin Hwangbo|Joonho Lee|Alexey Dosovitskiy|Dario Bellicoso|Vassilios Tsounis|Vladlen Koltun|and Marco Hutter,Very nice work that combines supervised learning for learning internal models (deep networks) of the series-elastic actuators dynamics with reinforcement learning (specifically|Trust Region Policy Optimization) for learning locomotion policies. They obtained excellent locomotion gaits and were able to learn complex standing-up sequences.,2019,Science Robotics
143,active learning for vision based grasping,1,https://link.springer.com/content/pdf/10.1007/BF00117446.pdf,Marcos Salganicoff|Lyle H. Ungar|Ruzena Bajcsy,This work as far as I can detect is the first introducing Active learning and Forgetting for the Perception-Action paradigm. It was the PhD thesis of Marcos Salganicoff in 1992. The paper (in fact several papers) cited here was published later. The new approach allowed the learner to control when and where in the input space the new examples should be gathered. It balances the cost of gathering the experiences (Exploration) with the cost of misclassification and the execution of the task (Exploitation ).,1996,"Machine Learning, 23, 251-278"
144,learning control in robotics,1,https://www.researchgate.net/publication/224144041_Learning_Control_in_Robotics,Stefan Schaal|Christopher G. Atkeson,"This review from Schaal and Atkeson does an excellent job of concisely covering the many approaches to learning control in robotics. It is useful  not only as an overview of this subtype of robot learning, but also as a   jumping off point for further research, as the works cited are extensive. This paper is also of note because it considers the problem of robot learning from a control perspective, rather than the more common computer science or statistical perspectives. The authors also discuss the practical aspects of learning control, such as the robustness of learned control policies to unexpected perturbation.",2010,IEEE Robotics & Automation Magazine
145,"a review of robot learning for manipulation: challenges, representations, and algorithms",1,https://arxiv.org/abs/1907.03146.pdf,Oliver Kroemer|Scott Niekum|George Konidaris,"This paper present an incredibly extensive recent survey on learning in robot manipulation (440 citations!!). Surveys are always use especially for new grad students. This one presents a single framework to formalise the robot manipulation problem.",2019,arXiv
146,"closing the sim-to-real loop: adapting simulation randomization with real world experience",1,https://arxiv.org/pdf/1810.05687.pdf,Yevgen Chebotar|Ankur Handa|Viktor Makoviychuk|Miles Macklin|Jan Issac|Nathan Ratliff|Dieter Fox,"TODO expert comment",2019,arXiv
