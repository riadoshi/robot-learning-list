{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import textract\n",
    "from typing import Dict\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e694157e4cba423e96d809f7af59d586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: to get textract to work here, we had to modify textract/parsers/utils.py\n",
    "# decode function to return UTF8 whenever the chardet confidence was not high\n",
    "# enough, eg:\n",
    "#\n",
    "#   result = chardet.detect(text)\n",
    "#   encoding = result['encoding'] if result['confidence'] > 0.85 else 'utf-8'\n",
    "#   return text.decode(encoding, 'ignore')\n",
    "#\n",
    "# We'd otherwise just get a bunch of UnicodeDecodeErrors\n",
    "\n",
    "# Paper struct\n",
    "@dataclass\n",
    "class Paper:\n",
    "    title: str\n",
    "    contents: str\n",
    "    extension: str\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "with open(\"../masterdatafinal.csv\") as file:\n",
    "    csv_contents = list(csv.reader(file))\n",
    "\n",
    "paper_map: Dict[int, Paper] = {}\n",
    "\n",
    "paper_paths = glob.glob(\"papers/*\")\n",
    "for path in tqdm(paper_paths):\n",
    "    paper_id, extension = os.path.basename(path).split(\".\")\n",
    "    paper_id = int(paper_id)\n",
    "    contents = textract.process(path).decode(\"utf8\")\n",
    "    paper_map[paper_id] = Paper(\n",
    "        title=csv_contents[paper_id][1], contents=contents, extension=extension\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP stuff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just define a helper for pulling out keywords.\n",
    "\n",
    "We'll just use TextRank, which is maybe not state of the art but will hopefully be sufficient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download language model\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "import spacy\n",
    "import pytextrank\n",
    "import en_core_web_lg\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Keyword:\n",
    "    keyword: str\n",
    "    count: int\n",
    "    rank: float\n",
    "\n",
    "\n",
    "def get_keywords(text):\n",
    "    # Get keywords\n",
    "    output = []\n",
    "    doc = nlp(text)\n",
    "    max_rank = 0\n",
    "    for p in doc._.phrases:\n",
    "        output.append(Keyword(keyword=p.text, count=p.count, rank=p.rank))\n",
    "        max_rank = max(max_rank, p.rank)\n",
    "\n",
    "    # Normalize ranks\n",
    "    for keyword in output:\n",
    "        keyword.rank = keyword.rank / max_rank\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54d2f69bbd84d51a4414538eeebf7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 .48 .2 .52 .83 .34 .15 .21 .97 .38 .56 .130 .101 .1 .104 .42 .91 .49 .26 .126 .9 .36 .113 .28 .118 .110 .99 .131 .37 .23 .114 .106 .80 .14 .98 .127 .71 .5 .78 .103 .85 .123 .18 .45 .87 .11 .61 .46 .22 .4 .63 .72 .133 .107 .20 .17 .41 .136 .13 .66 .96 .44 .39 .122 .62 .3 .137 .89 .7 .112 .100 .90 .76 .124 .31 .117 .33 .58 .93 .65 .64 .67 .40 .86 .134 .108 .84 .139 .142 .119 .88 .53 .47 .16 .60 .54 .6 .27 .10 .128 .95 .25 .35 .121 .29 .59 .43 .111 .57 .82 .30 .51 .68 .135 .79 .8 .69 .116 .129 .32 .73 .125 .120 .138 .94 .19 .92 .81 .55 .12 .75 .115 .77 .\n"
     ]
    }
   ],
   "source": [
    "max_length = 300000  # absolute max is 1000000 (words?)\n",
    "keyword_map = {}\n",
    "\n",
    "for paper_id, paper in tqdm(paper_map.items()):\n",
    "    for keyword in get_keywords(\n",
    "        paper.contents[:max_length].rpartition(\" \")[0]\n",
    "    ):\n",
    "        if keyword.keyword in keyword_map:\n",
    "            keyword_map[keyword.keyword].count += keyword.count\n",
    "            keyword_map[keyword.keyword].rank += keyword.rank\n",
    "        else:\n",
    "            keyword_map[keyword.keyword] = keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out top 1000 keywords in alphabetical order\n",
    "sorted_keywords = sorted(\n",
    "    keyword_map.keys(), key=lambda k: -keyword_map[k].rank\n",
    ")\n",
    "\n",
    "\n",
    "def is_valid(k):\n",
    "    if len(k) <= 4:\n",
    "        return False\n",
    "    if len(set(k) - set(\"abcdefghijklmnopqrstuvwxyz \")) > 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "## Uncomment this line to print out top ~1000 keywords, sorted alphabetically\n",
    "# print(sorted(filter(is_valid, sorted_keywords[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41 curated keywords\n"
     ]
    }
   ],
   "source": [
    "from curated_keywords import curated_keywords\n",
    "\n",
    "print(f\"Loaded {len(curated_keywords)} curated keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
