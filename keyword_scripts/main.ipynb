{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import textract\n",
    "from typing import Dict\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import dataclasses\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4cbb4ace7d4d9782f6f78028c7e7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: to get textract to work here, we had to modify textract/parsers/utils.py\n",
    "# decode function to return UTF8 whenever the chardet confidence was not high\n",
    "# enough, eg:\n",
    "#\n",
    "#   result = chardet.detect(text)\n",
    "#   encoding = result['encoding'] if result['confidence'] > 0.85 else 'utf-8'\n",
    "#   return text.decode(encoding, 'ignore')\n",
    "#\n",
    "# We'd otherwise just get a bunch of UnicodeDecodeErrors\n",
    "\n",
    "# Paper struct\n",
    "@dataclasses.dataclass\n",
    "class Paper:\n",
    "    title: str\n",
    "    contents: str\n",
    "    extension: str\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "with open(\"../masterdatafinal.csv\") as file:\n",
    "    csv_contents = list(csv.reader(file))\n",
    "\n",
    "# Load up all papers\n",
    "paper_from_id: Dict[int, Paper] = {}\n",
    "\n",
    "paper_paths = glob.glob(\"papers/*\")\n",
    "for path in tqdm(paper_paths):\n",
    "    paper_id, extension = os.path.basename(path).split(\".\")\n",
    "    paper_id = int(paper_id)\n",
    "\n",
    "    contents = textract.process(path).decode(\"utf8\")\n",
    "\n",
    "    # Keep only the first 70% or 500000 characters\n",
    "    # This is kind of a hack: gets rid of citations, speeds up computation, saves memory, etc\n",
    "    max_length = 500000\n",
    "    desired_length = min(int(len(contents) * 0.7), max_length)\n",
    "\n",
    "    contents = contents[:desired_length].rpartition(\" \")[0]\n",
    "    paper_from_id[paper_id] = Paper(\n",
    "        title=csv_contents[paper_id][1], contents=contents, extension=extension\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP stuff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just define a helper for pulling out keywords.\n",
    "\n",
    "We'll just use TextRank, which is maybe not state of the art but will hopefully be sufficient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download language model\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "# Keyword struct\n",
    "@dataclasses.dataclass\n",
    "class Keyword:\n",
    "    keyword: str\n",
    "    count: int\n",
    "    rank: float\n",
    "\n",
    "\n",
    "# NLP helper\n",
    "import spacy\n",
    "import pytextrank\n",
    "import en_core_web_lg\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "# Parsing helper\n",
    "def get_keywords(text):\n",
    "    # Get keywords\n",
    "    output = []\n",
    "    doc = nlp(text)\n",
    "    max_rank = 0\n",
    "    for p in doc._.phrases:\n",
    "        output.append(Keyword(keyword=p.text, count=p.count, rank=p.rank))\n",
    "        max_rank = max(max_rank, p.rank)\n",
    "\n",
    "    # Normalize ranks\n",
    "    for keyword in output:\n",
    "        keyword.rank = keyword.rank / max_rank\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword extraction took: 236.40346097946167 seconds\n"
     ]
    }
   ],
   "source": [
    "# Parallelized keyword extraction\n",
    "\n",
    "\n",
    "def _():\n",
    "    import multiprocessing\n",
    "    import time\n",
    "\n",
    "    pool = multiprocessing.Pool()\n",
    "\n",
    "    start_time = time.time()\n",
    "    keywords_list = pool.map(\n",
    "        get_keywords, [paper.contents for paper in paper_from_id.values()],\n",
    "    )\n",
    "    print(\"Keyword extraction took:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "keywords_from_id = dict(zip(paper_from_id.keys(), _()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['a function', 'a kendama learning robot', 'a large number', 'a linear function', 'a new approach', 'a real robot', 'a reward function', 'a robot arm', 'a set', 'a small number', 'a variety', 'abbeel', 'abbeel et al', 'abstract', 'academic press', 'access', 'account', 'accuracy', 'acknowledgments', 'action', 'action execution', 'action selection', 'actions', 'active learning', 'actuators', 'adaptation', 'adaptive behavior', 'adaptive control', 'addition', 'advance', 'advanced robotics', 'advances', 'advantage', 'agents', 'algorithm', 'algorithms', 'analysis', 'animals', 'application', 'applications', 'apprenticeship learning', 'approach', 'approaches', 'approximate inference', 'approximate inference methods', 'approximation', 'areas', 'artificial intelligence', 'artificial intelligence research', 'artificial life', 'artificial neural networks', 'arxiv', 'arxiv preprint', 'atkeson', 'attention', 'authors', 'automatic control', 'automation', 'autonomous agents', 'autonomous helicopter control', 'autonomous learning', 'autonomous mental development', 'autonomous robots', 'autonomous systems', 'background', 'bagnell', 'barto', 'basis functions', 'bayes', 'bayesian', 'bayesian inference', 'behavior', 'behaviors', 'bellman', 'berlin', 'better performance', 'biological systems', 'boxes', 'brain', 'california', 'cambridge', 'cambridge university press', 'carnegie mellon university', 'cartesian', 'cases', 'center', 'challenges', 'change', 'changes', 'children', 'classification', 'closed form', 'cognition', 'cognitive robotics', 'cognitive science', 'cognitive sciences', 'collisions', 'comparison', 'complex environments', 'complex tasks', 'complexity', 'computation', 'computation time', 'computational intelligence', 'computational models', 'computer science', 'computer science department', 'computer vision', 'computing', 'conclusion', 'conditions', 'conference', 'connectionist reinforcement learning', 'connections', 'constraints', 'contact', 'contact dynamics', 'context', 'continuous control', 'continuous states', 'contrast', 'control', 'control algorithms', 'control policies', 'control problems', 'control signals', 'control systems', 'control theory', 'controller', 'controllers', 'convergence', 'convolutional networks', 'convolutional neural networks', 'cost function', 'cost functions', 'costs', 'covariance', 'covariant policy search', 'cybernetics', 'data collection', 'data points', 'datasets', 'decisions', 'deep convolutional neural networks', 'deep generative models', 'deep learning', 'deep networks', 'deep neural networks', 'deep reinforcement', 'deep reinforcement learning', 'deep visuomotor policies', 'deisenroth et al', 'delayed rewards', 'demonstrated trajectories', 'demonstration', 'demonstrations', 'department', 'depth', 'design', 'detail', 'details', 'deterministic policies', 'development', 'different actions', 'different approaches', 'different behaviors', 'different levels', 'different objects', 'different tasks', 'different types', 'different values', 'different ways', 'difficult learning control problems', 'direct learning', 'discrete actions', 'discrete states', 'discussion', 'distribution', 'distributions', 'domain knowledge', 'domains', 'duan et al', 'dynamic programming', 'dynamic systems', 'dynamical systems', 'dynamics', 'dynamics models', 'editors', 'effect', 'efficient learning', 'energy', 'engineering', 'environment', 'environments', 'equation', 'equations', 'error', 'errors', 'estimation', 'et al', 'evaluation', 'evidence', 'evolution', 'evolutionary computation', 'example', 'examples', 'execution', 'existing methods', 'expected reward', 'experience', 'experimental results', 'experimental robotics', 'experiments', 'expert demonstrations', 'exploration', 'fast learning', 'features', 'feedback', 'feedback control', 'field', 'figure', 'first', 'fitness', 'forces', 'freedom', 'friction', 'front', 'function', 'function approximation', 'functions', 'future actions', 'future rewards', 'future states', 'future work', 'games', 'gaussian', 'gaussian distributions', 'gaussian noise', 'gaussian processes', 'generalization', 'genetic algorithms', 'germany', 'goal states', 'goals', 'good performance', 'good policies', 'good results', 'good solutions', 'gradient', 'gradient descent', 'gradient methods', 'gradients', 'graphs', 'grasping objects', 'grasping points', 'grasps', 'guided policy search', 'height', 'helicopters', 'hidden markov models', 'hidden state', 'hierarchical reinforcement learning', 'human demonstration', 'human motion', 'human motor control', 'human performance', 'humanoid robot', 'humanoid robotics', 'humanoid robots', 'humans', 'ieee conference', 'ieee int', 'ieee international conference', 'ieee international conference on robotics and automation', 'ieee trans', 'ieee transactions', 'image classification', 'image patches', 'images', 'imitation', 'imitation learning', 'improvement', 'inaccurate models', 'incremental learning', 'individuals', 'inference', 'information', 'initial state', 'initial states', 'input', 'inputs', 'instance', 'intelligence', 'intelligent robots', 'intelligent systems', 'interaction', 'interactions', 'interest', 'internal models', 'international conference', 'international conference on intelligent robots and systems', 'international conference on machine learning', 'international joint conference', 'international journal', 'international symposium', 'introduction', 'inverse reinforcement learning', 'iteration', 'iterations', 'iterative learning', 'iterative learning control', 'joint angles', 'joint space', 'journal', 'kalman', 'knowledge', 'known objects', 'large numbers', 'layers', 'learn', 'learned models', 'learning', 'learning algorithms', 'learning control', 'learning methods', 'learning representations', 'learning systems', 'legged robots', 'length', 'linear', 'linear models', 'linear regression', 'local models', 'localization', 'locally weighted learning', 'locomotion', 'los angeles', 'machine', 'machine intelligence', 'machine learning', 'machine learning methods', 'machine learning research', 'machine learning techniques', 'machines', 'magnitude', 'manipulation', 'many algorithms', 'many applications', 'many cases', 'many problems', 'many tasks', 'markov', 'markov decision processes', 'massachusetts', 'means', 'measurements', 'memory', 'method', 'methods', 'mit press', 'mnih et al', 'mobile robots', 'model', 'model bias', 'model errors', 'model learning', 'model parameters', 'model predictive control', 'model uncertainty', 'modeling', 'models', 'monte carlo', 'moore', 'more data', 'more detail', 'more details', 'more information', 'morgan kaufmann', 'motion', 'motion planning', 'motor', 'motor commands', 'motor control', 'motor learning', 'motor primitives', 'motor skills', 'movement', 'movement generation', 'movement imitation', 'movement primitives', 'movement recognition', 'movements', 'multiple demonstrations', 'multiple objects', 'multiple tasks', 'nature', 'navigation', 'network', 'networks', 'neural', 'neural computation', 'neural information processing systems', 'neural network', 'neural network policies', 'neural networks', 'neurons', 'neuroscience', 'new actions', 'new data', 'new objects', 'new situations', 'new tasks', 'new york', 'next state', 'ng et al', 'noise', 'nonlinear dynamical systems', 'nonlinear systems', 'novel objects', 'number', 'object affordances', 'object detection', 'object localization', 'object recognition', 'object segmentation', 'object shape', 'objects', 'observation', 'observations', 'obstacle avoidance', 'obstacles', 'online learning', 'operations research', 'optimal actions', 'optimal behavior', 'optimal control', 'optimal control problems', 'optimal policies', 'optimal policy', 'optimization', 'order', 'orientation', 'other agents', 'other approaches', 'other methods', 'other tasks', 'other words', 'others', 'our approach', 'output', 'pages', 'parallel', 'parameter space', 'parameters', 'pattern recognition', 'perception', 'performance', 'peters', 'phd thesis', 'physical systems', 'pieter abbeel', 'pixels', 'place', 'planning', 'plans', 'point', 'points', 'policies', 'policy', 'policy evaluation', 'policy gradient', 'policy gradient methods', 'policy gradient reinforcement', 'policy gradient reinforcement learning', 'policy gradients', 'policy improvement', 'policy invariance', 'policy iteration', 'policy learning', 'policy optimization', 'policy parameters', 'policy search', 'policy search methods', 'policy space', 'position', 'positions', 'possible actions', 'possible states', 'practice', 'prediction', 'predictions', 'press', 'previous approaches', 'previous work', 'principle', 'prior knowledge', 'prior work', 'probabilistic', 'probabilistic inference', 'probabilistic models', 'probability', 'probability distributions', 'problem', 'problems', 'proceedings', 'processes', 'progress', 'properties', 'psychology', 'q learning', 'quadruped robots', 'questions', 'r eferences', 'random variables', 'raw images', 'real data', 'real images', 'real robots', 'real time', 'reality', 'recent work', 'recognition', 'recurrent neural networks', 'references', 'regions', 'regression', 'reinforcement', 'reinforcement learning', 'reinforcement learning algorithms', 'reinforcement learning methods', 'reinforcement learning policy search methods', 'related work', 'representation', 'representations', 'research', 'respect', 'response', 'results', 'reward', 'reward function', 'reward functions', 'rewards', 'rhythmic movements', 'rl algorithms', 'rl methods', 'robot', 'robot control', 'robot dynamics', 'robot learning', 'robot manipulation', 'robot manipulators', 'robot motion', 'robot programming', 'robot skills', 'robot soccer', 'robot trajectory optimization', 'robotic control', 'robotic grasping', 'robotic manipulation', 'robotic systems', 'robotic tasks', 'robotics', 'robotics and autonomous systems', 'robotics research', 'robots', 'robustness', 'sample trajectories', 'samples', 'sampling', 'san francisco', 'schaal', 'science', 'scratch', 'search', 'second', 'section', 'section iv', 'sensor data', 'sensorimotor control', 'sensors', 'sequences', 'sergey levine', 'several examples', 'signal processing', 'similar objects', 'similar states', 'simplicity', 'simulation', 'situations', 'skills', 'social learning', 'solutions', 'space', 'sparse rewards', 'specific objects', 'speed', 'springer', 'stability', 'standard deviation', 'stanford', 'stanford university', 'state', 'state estimation', 'state s', 'state space', 'state transitions', 'state variables', 'states', 'states s', 'statistical learning', 'statistics', 'stefan schaal', 'steps', 'stochastic gradient descent', 'stochastic models', 'stochastic optimal control', 'stochastic policies', 'stochastic systems', 'strategies', 'structure', 'studies', 'success', 'such actions', 'such approaches', 'such cases', 'such methods', 'such models', 'such problems', 'such systems', 'such tasks', 'supervised learning', 'supervised learning methods', 'support', 'sutton', 'sutton et al', 'system', 'system dynamics', 'systems', 'table', 'task space', 'tasks', 'technical report', 'techniques', 'technology', 'terms', 'test time', 'testing', 'the ability', 'the action space', 'the amount', 'the best action', 'the center', 'the context', 'the current policy', 'the current state', 'the dynamics', 'the end', 'the field', 'the goal', 'the goal state', 'the ieee international conference', 'the initial state', 'the learned model', 'the learned policies', 'the learned policy', 'the learning algorithm', 'the learning process', 'the learning rate', 'the learning system', 'the network', 'the next section', 'the next state', 'the number', 'the optimal policy', 'the other hand', 'the policy parameters', 'the previous section', 'the problem', 'the process', 'the real world', 'the resulting policy', 'the same time', 'the search space', 'the space', 'the state space', 'the system', 'the system dynamics', 'the total number', 'the training data', 'the training set', 'the use', 'theory', 'third', 'this paper', 'this work', 'time step', 'time step t', 'time steps', 'time t', 'training', 'training data', 'training examples', 'training iterations', 'training time', 'trajectories', 'trajectory', 'trajectory formation', 'trajectory optimization', 'trajectory planning', 'transitions', 'trends', 'trial', 'trials', 'trust region policy optimization', 'uncertainty', 'understanding', 'university', 'unknown objects', 'unsupervised learning', 'value', 'value function', 'value function approximation', 'value functions', 'value iteration', 'values', 'variables', 'variance', 'vectors', 'velocities', 'velocity', 'vision', 'visual features', 'visual feedback', 'visual information', 'visual perception', 'volume', 'walking', 'wang et al', 'weights']\n"
     ]
    }
   ],
   "source": [
    "# Consolidate keywords: help us determine which ones to actually use\n",
    "def list_top_keywords():\n",
    "    all_keyword_map = {}\n",
    "    for keywords in tqdm(keywords_from_id.values()):\n",
    "        for keyword in keywords:\n",
    "            if keyword.keyword in all_keyword_map:\n",
    "                all_keyword_map[keyword.keyword].count += keyword.count\n",
    "                all_keyword_map[keyword.keyword].rank += keyword.rank\n",
    "            else:\n",
    "                all_keyword_map[keyword.keyword] = Keyword(\n",
    "                    **dataclasses.asdict(keyword)\n",
    "                )\n",
    "\n",
    "    # Print out top 1000 keywords in alphabetical order\n",
    "    sorted_keywords = sorted(\n",
    "        all_keyword_map.keys(), key=lambda k: -all_keyword_map[k].rank\n",
    "    )\n",
    "\n",
    "    def is_valid(k):\n",
    "        if len(k) <= 4:\n",
    "            return False\n",
    "        if len(set(k) - set(\"abcdefghijklmnopqrstuvwxyz \")) > 0:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # Print out top ~1000 keywords, sorted alphabetically\n",
    "    print(sorted(filter(is_valid, sorted_keywords[:1000])))\n",
    "\n",
    "\n",
    "list_top_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 123 total keywords, 24 unique\n"
     ]
    }
   ],
   "source": [
    "from curated_keywords import curated_keyword_map\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(curated_keyword_map)} total keywords, {len(set(curated_keyword_map.values()))} unique\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact dynamics (4) genetic algorithms (5) evolution (5) nonlinear systems (6) unsupervised learning (7) cognitive sciences (9) trajectory optimization (9) legged robots (9) locomotion (10) dynamic programming (11) humanoid robotics (11) manipulation (12) mobile robots (15) policy gradients (17) planning (18) gaussians (19) state estimation (19) optimal control (22) learning from demonstration (32) neural networks (33) dynamical systems (37) visual perception (38) probabilistic models (38) reinforcement learning (58) \n"
     ]
    }
   ],
   "source": [
    "def _():\n",
    "    top_keywords_from_id = {}\n",
    "\n",
    "    rank_threshold = 0.7\n",
    "    count_threshold = 2\n",
    "    min_keywords = 4\n",
    "\n",
    "    import collections\n",
    "\n",
    "    keyword_counts = collections.defaultdict(lambda: 0)\n",
    "\n",
    "    for paper_id, keywords in keywords_from_id.items():\n",
    "        paper_keyword_counts = collections.defaultdict(lambda: 0)\n",
    "        for k in keywords:\n",
    "            if (\n",
    "                k.rank <= rank_threshold\n",
    "                and len(paper_keyword_counts) >= min_keywords\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            if k.keyword not in curated_keyword_map:\n",
    "                continue\n",
    "\n",
    "            nominal = curated_keyword_map[k.keyword]\n",
    "            if nominal not in paper_keyword_counts:\n",
    "                keyword_counts[nominal] += 1\n",
    "            paper_keyword_counts[nominal] += k.count\n",
    "\n",
    "        top_keywords = (\n",
    "            []\n",
    "        )  # A set would work here, but ordering is not supported natively\n",
    "        for keyword, count in paper_keyword_counts.items():\n",
    "            if count >= count_threshold and keyword not in top_keywords:\n",
    "                top_keywords.append(keyword)\n",
    "\n",
    "        for keyword, nominal in curated_keyword_map.items():\n",
    "            if (\n",
    "                keyword in paper_from_id[paper_id].title\n",
    "                and nominal not in top_keywords\n",
    "            ):\n",
    "                top_keywords.append(nominal)\n",
    "\n",
    "        if len(top_keywords) > 0:\n",
    "            top_keywords_from_id[paper_id] = top_keywords\n",
    "\n",
    "    for k in sorted(keyword_counts.keys(), key=lambda k: keyword_counts[k]):\n",
    "        print(f\"{k} ({keyword_counts[k]}) \", end=\"\")\n",
    "    print()\n",
    "    return top_keywords_from_id\n",
    "\n",
    "\n",
    "top_keywords_from_id = _()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{24: ['policy gradients',\n",
       "  'reinforcement learning',\n",
       "  'neural networks',\n",
       "  'visual perception'],\n",
       " 48: ['optimal control', 'probabilistic models', 'planning'],\n",
       " 2: ['policy gradients'],\n",
       " 52: ['learning from demonstration'],\n",
       " 83: ['optimal control',\n",
       "  'dynamic programming',\n",
       "  'neural networks',\n",
       "  'reinforcement learning'],\n",
       " 34: ['nonlinear systems', 'gaussians'],\n",
       " 15: ['learning from demonstration'],\n",
       " 21: ['policy gradients', 'probabilistic models'],\n",
       " 97: ['reinforcement learning',\n",
       "  'learning from demonstration',\n",
       "  'optimal control',\n",
       "  'mobile robots'],\n",
       " 38: ['optimal control'],\n",
       " 56: ['visual perception', 'neural networks', 'reinforcement learning'],\n",
       " 130: ['visual perception', 'contact dynamics', 'manipulation'],\n",
       " 101: ['humanoid robotics', 'cognitive sciences', 'visual perception'],\n",
       " 1: ['neural networks', 'trajectory optimization', 'visual perception'],\n",
       " 104: ['evolution'],\n",
       " 42: ['optimal control', 'dynamical systems', 'visual perception'],\n",
       " 91: ['trajectory optimization', 'neural networks'],\n",
       " 49: ['reinforcement learning', 'dynamical systems'],\n",
       " 26: ['reinforcement learning', 'humanoid robotics'],\n",
       " 126: ['mobile robots',\n",
       "  'state estimation',\n",
       "  'visual perception',\n",
       "  'probabilistic models'],\n",
       " 9: ['probabilistic models',\n",
       "  'learning from demonstration',\n",
       "  'reinforcement learning'],\n",
       " 113: ['gaussians', 'dynamical systems', 'probabilistic models'],\n",
       " 28: ['learning from demonstration', 'state estimation'],\n",
       " 118: ['manipulation', 'visual perception', 'probabilistic models'],\n",
       " 110: ['reinforcement learning', 'state estimation', 'dynamic programming'],\n",
       " 99: ['legged robots', 'genetic algorithms', 'evolution', 'locomotion'],\n",
       " 131: ['visual perception', 'manipulation'],\n",
       " 37: ['probabilistic models',\n",
       "  'dynamical systems',\n",
       "  'visual perception',\n",
       "  'reinforcement learning'],\n",
       " 23: ['neural networks', 'reinforcement learning', 'visual perception'],\n",
       " 114: ['evolution', 'neural networks', 'locomotion'],\n",
       " 106: ['optimal control', 'dynamical systems'],\n",
       " 80: ['manipulation', 'visual perception'],\n",
       " 14: ['reinforcement learning',\n",
       "  'learning from demonstration',\n",
       "  'probabilistic models'],\n",
       " 98: ['legged robots', 'genetic algorithms', 'locomotion', 'evolution'],\n",
       " 127: ['optimal control', 'dynamic programming'],\n",
       " 71: ['probabilistic models',\n",
       "  'gaussians',\n",
       "  'unsupervised learning',\n",
       "  'reinforcement learning'],\n",
       " 5: ['neural networks', 'visual perception'],\n",
       " 78: ['policy gradients', 'reinforcement learning', 'dynamical systems'],\n",
       " 103: ['optimal control', 'trajectory optimization'],\n",
       " 85: ['evolution'],\n",
       " 123: ['optimal control'],\n",
       " 18: ['locomotion',\n",
       "  'legged robots',\n",
       "  'policy gradients',\n",
       "  'reinforcement learning'],\n",
       " 45: ['probabilistic models', 'reinforcement learning', 'neural networks'],\n",
       " 87: ['dynamical systems', 'genetic algorithms'],\n",
       " 11: ['humanoid robotics',\n",
       "  'learning from demonstration',\n",
       "  'dynamical systems',\n",
       "  'nonlinear systems'],\n",
       " 61: ['learning from demonstration', 'planning', 'humanoid robotics'],\n",
       " 46: ['reinforcement learning', 'dynamical systems', 'legged robots'],\n",
       " 22: ['policy gradients', 'reinforcement learning', 'visual perception'],\n",
       " 4: ['learning from demonstration',\n",
       "  'visual perception',\n",
       "  'probabilistic models',\n",
       "  'dynamical systems'],\n",
       " 63: ['optimal control', 'dynamical systems', 'state estimation'],\n",
       " 72: ['visual perception', 'manipulation'],\n",
       " 133: ['state estimation', 'dynamical systems', 'legged robots'],\n",
       " 107: ['learning from demonstration',\n",
       "  'reinforcement learning',\n",
       "  'state estimation'],\n",
       " 17: ['probabilistic models'],\n",
       " 41: ['humanoid robotics', 'learning from demonstration', 'visual perception'],\n",
       " 136: ['reinforcement learning'],\n",
       " 13: ['visual perception',\n",
       "  'manipulation',\n",
       "  'reinforcement learning',\n",
       "  'neural networks'],\n",
       " 66: ['manipulation',\n",
       "  'neural networks',\n",
       "  'visual perception',\n",
       "  'reinforcement learning'],\n",
       " 44: ['reinforcement learning'],\n",
       " 122: ['cognitive sciences',\n",
       "  'dynamical systems',\n",
       "  'neural networks',\n",
       "  'humanoid robotics'],\n",
       " 62: ['policy gradients',\n",
       "  'reinforcement learning',\n",
       "  'learning from demonstration',\n",
       "  'probabilistic models'],\n",
       " 3: ['dynamical systems', 'nonlinear systems', 'planning'],\n",
       " 137: ['reinforcement learning',\n",
       "  'learning from demonstration',\n",
       "  'dynamical systems'],\n",
       " 89: ['visual perception', 'state estimation'],\n",
       " 7: ['manipulation', 'visual perception', 'planning', 'probabilistic models'],\n",
       " 112: ['evolution', 'neural networks'],\n",
       " 100: ['planning', 'genetic algorithms', 'locomotion'],\n",
       " 90: ['neural networks',\n",
       "  'reinforcement learning',\n",
       "  'policy gradients',\n",
       "  'learning from demonstration'],\n",
       " 76: ['reinforcement learning', 'probabilistic models'],\n",
       " 124: ['dynamical systems', 'probabilistic models', 'state estimation'],\n",
       " 31: ['visual perception'],\n",
       " 117: ['state estimation',\n",
       "  'neural networks',\n",
       "  'mobile robots',\n",
       "  'visual perception'],\n",
       " 33: ['visual perception', 'planning', 'manipulation'],\n",
       " 58: ['visual perception', 'neural networks'],\n",
       " 93: ['dynamical systems',\n",
       "  'optimal control',\n",
       "  'unsupervised learning',\n",
       "  'reinforcement learning'],\n",
       " 65: ['dynamical systems', 'learning from demonstration'],\n",
       " 64: ['manipulation'],\n",
       " 67: ['contact dynamics',\n",
       "  'trajectory optimization',\n",
       "  'locomotion',\n",
       "  'reinforcement learning'],\n",
       " 40: ['locomotion', 'legged robots', 'gaussians'],\n",
       " 86: ['reinforcement learning', 'neural networks'],\n",
       " 134: ['policy gradients', 'reinforcement learning'],\n",
       " 108: ['learning from demonstration'],\n",
       " 84: ['learning from demonstration', 'reinforcement learning', 'planning'],\n",
       " 139: ['visual perception'],\n",
       " 142: ['legged robots', 'locomotion', 'dynamical systems'],\n",
       " 119: ['dynamical systems', 'visual perception', 'planning'],\n",
       " 88: ['visual perception', 'neural networks', 'mobile robots'],\n",
       " 53: ['planning', 'probabilistic models'],\n",
       " 47: ['gaussians', 'probabilistic models'],\n",
       " 16: ['neural networks', 'reinforcement learning', 'probabilistic models'],\n",
       " 60: ['neural networks', 'visual perception'],\n",
       " 54: ['dynamical systems', 'gaussians', 'planning'],\n",
       " 6: ['learning from demonstration', 'optimal control', 'dynamical systems'],\n",
       " 27: ['humanoid robotics', 'learning from demonstration', 'dynamical systems'],\n",
       " 10: ['reinforcement learning',\n",
       "  'unsupervised learning',\n",
       "  'optimal control',\n",
       "  'genetic algorithms'],\n",
       " 95: ['neural networks'],\n",
       " 25: ['reinforcement learning',\n",
       "  'dynamic programming',\n",
       "  'learning from demonstration'],\n",
       " 35: ['probabilistic models', 'dynamical systems', 'reinforcement learning'],\n",
       " 121: ['humanoid robotics'],\n",
       " 29: ['dynamical systems', 'planning', 'gaussians'],\n",
       " 59: ['unsupervised learning',\n",
       "  'neural networks',\n",
       "  'probabilistic models',\n",
       "  'gaussians'],\n",
       " 43: ['gaussians', 'probabilistic models', 'locomotion'],\n",
       " 111: ['planning', 'gaussians', 'reinforcement learning'],\n",
       " 57: ['learning from demonstration', 'reinforcement learning'],\n",
       " 82: ['learning from demonstration',\n",
       "  'reinforcement learning',\n",
       "  'neural networks'],\n",
       " 30: ['visual perception', 'gaussians', 'state estimation'],\n",
       " 51: ['humanoid robotics', 'reinforcement learning', 'cognitive sciences'],\n",
       " 68: ['probabilistic models'],\n",
       " 135: ['cognitive sciences'],\n",
       " 79: ['learning from demonstration'],\n",
       " 8: ['probabilistic models', 'planning'],\n",
       " 69: ['probabilistic models',\n",
       "  'optimal control',\n",
       "  'dynamical systems',\n",
       "  'reinforcement learning'],\n",
       " 116: ['neural networks',\n",
       "  'reinforcement learning',\n",
       "  'policy gradients',\n",
       "  'learning from demonstration'],\n",
       " 129: ['visual perception', 'mobile robots', 'reinforcement learning'],\n",
       " 32: ['gaussians', 'humanoid robotics'],\n",
       " 125: ['optimal control',\n",
       "  'learning from demonstration',\n",
       "  'reinforcement learning'],\n",
       " 138: ['policy gradients', 'reinforcement learning'],\n",
       " 94: ['policy gradients', 'neural networks', 'reinforcement learning'],\n",
       " 19: ['gaussians',\n",
       "  'planning',\n",
       "  'learning from demonstration',\n",
       "  'probabilistic models'],\n",
       " 92: ['policy gradients', 'reinforcement learning'],\n",
       " 81: ['visual perception',\n",
       "  'dynamical systems',\n",
       "  'neural networks',\n",
       "  'reinforcement learning'],\n",
       " 55: ['contact dynamics', 'dynamical systems', 'reinforcement learning'],\n",
       " 12: ['reinforcement learning',\n",
       "  'evolution',\n",
       "  'neural networks',\n",
       "  'visual perception'],\n",
       " 75: ['probabilistic models', 'planning', 'state estimation'],\n",
       " 77: ['trajectory optimization', 'planning']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 a survey on policy search for robotics\n",
      "36 aibo ingenuity\n",
      "39 simultaneous adversarial multi-robot learning\n",
      "50 robot skill learning: from reinforcement learning to evolution strategies\n",
      "70 robotics|vision and control - fundamental algorithms in matlab\n",
      "73 a simple learning strategy for high-speed quadrocopter multi-flips\n",
      "74 dynamic programming and optimal control (vol. i+ii)\n",
      "96 locally weighted learning and locally weighted learning for control\n",
      "102 on the adaptive control of robot manipulator\n",
      "105 forward models: supervised learning with a distal teacher\n",
      "109 representations for robot knowledge in the knowrob framework\n",
      "115 resilient machines through continuous self-modeling\n",
      "120 how the body shapes the way we think: a new view of intelligence\n",
      "128 the coordination of arm movements: an experimentally confirmed mathematical model\n",
      "132 optimal control and estimation\n",
      "140 map learning with uninterpreted sensors and effectors\n",
      "141 experiments in synthetic psychology\n"
     ]
    }
   ],
   "source": [
    "# Print papers with missing keywords\n",
    "for id in sorted(\n",
    "    set(range(1, len(csv_contents))) - set(top_keywords_from_id.keys())\n",
    "):\n",
    "    print(id, csv_contents[id][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keywords_from_id = {\n",
    "    20: [\"reinforcement learning\"],  # a survey on policy search for robotics\n",
    "    36: [\"locomotion\"],  # aibo ingenuity\n",
    "    39: [\n",
    "        \"reinforcement learning\"\n",
    "    ],  # simultaneous adversarial multi-robot learning\n",
    "    50: [\n",
    "        \"reinforcement learning\",\n",
    "        \"evolution\",\n",
    "    ],  # robot skill learning: from reinforcement learning to evolution strategies\n",
    "    70: [\n",
    "        \"visual perception\"\n",
    "    ],  # robotics|vision and control - fundamental algorithms in matlab\n",
    "    73: [\n",
    "        \"policy gradients\"\n",
    "    ],  # a simple learning strategy for high-speed quadrocopter multi-flips\n",
    "    74: [\n",
    "        \"dynamic programming\",\n",
    "        \"optimal control\",\n",
    "    ],  # dynamic programming and optimal control (vol. i+ii)\n",
    "    96: [\n",
    "        \"optimal control\",\n",
    "        \"nonlinear systems\",\n",
    "    ],  # locally weighted learning and locally weighted learning for control\n",
    "    102: [\n",
    "        \"dynamical systems\",\n",
    "        \"manipulation\",\n",
    "    ],  # on the adaptive control of robot manipulator\n",
    "    105: [\n",
    "        \"dynamical systems\",\n",
    "        \"neural networks\",\n",
    "    ],  # forward models: supervised learning with a distal teacher\n",
    "    109: [],  # representations for robot knowledge in the knowrob framework\n",
    "    115: [\n",
    "        \"legged robots\",\n",
    "        \"locomotion\",\n",
    "    ],  # resilient machines through continuous self-modeling\n",
    "    120: [\n",
    "        \"cognitive sciences\"\n",
    "    ],  # how the body shapes the way we think: a new view of intelligence\n",
    "    128: [\n",
    "        \"cognitive sciences\",\n",
    "        \"dynamical systems\",\n",
    "    ],  # the coordination of arm movements: an experimentally confirmed mathematical model\n",
    "    132: [\n",
    "        \"optimal control\",\n",
    "        \"state estimation\",\n",
    "    ],  # optimal control and estimation\n",
    "    140: [],  # map learning with uninterpreted sensors and effectors\n",
    "    141: [\"cognitive sciences\"],  # experiments in synthetic psychology\n",
    "}\n",
    "\n",
    "# Validate\n",
    "for k, v in manual_keywords_from_id.items():\n",
    "    assert k not in top_keywords_from_id\n",
    "    for keyword in v:\n",
    "        assert curated_keyword_map[keyword] == keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep copy CSV contents\n",
    "new_csv_contents = [row[:] for row in csv_contents]\n",
    "new_csv_contents[0].append(\"Keywords\")\n",
    "\n",
    "for row in new_csv_contents[1:]:\n",
    "    paper_id = int(row[0])\n",
    "    if paper_id in top_keywords_from_id:\n",
    "        keywords = top_keywords_from_id[paper_id]\n",
    "    else:\n",
    "        keywords = manual_keywords_from_id[paper_id]\n",
    "    row.append(\",\".join(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./masterdatakeywords.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for row in new_csv_contents:\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
