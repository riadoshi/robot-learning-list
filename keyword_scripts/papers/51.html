<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns#">
    <meta charset="utf-8">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="referrer" content="origin">
    <meta name="Rg-Request-Token" id="Rg-Request-Token" content="aad-w/FRsSsewY1KUn85dWJ3hNxplR4WxOuF7lEz1Ez+gShnMVp4Kvwu6JtUtCDUXnC5XoQp33scW9jiC9D4xux5IneECaBweL2lQRxRjHgAilnhCCqzuwkPon8xTh+znhfwRwSkWmOD8/fSs7J1Fwg+SzeXx8XJKSQ7UjrN5SsmJ30INoClw85PeJ7a56WApl+Xzae3HBhRJXT8N8Fxz92KGbZmNX9QAyaQGeyNrdqYaIrMKh2GTVV4tpFHF1/qVUNfhLnovwoGVwl4uck39Ew=">
    <meta http-equiv="expires" content="0">
    <link rel="preload" as="script" href="https://c5.rgstatic.net/m/4142001695121077/javascript/lite/lite.js" crossOrigin="1">        <link rel="icon" type="image/png" href="https://c5.rgstatic.net/m/41542880220916/images/favicon/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="https://c5.rgstatic.net/m/42134677640462/images/favicon/favicon-16x16.png" sizes="16x16">
    <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/426351313275430/images/favicon/favicon.ico">
    <link rel="mask-icon" href="https://c5.rgstatic.net/m/441272785461785/images/favicon/favicon.svg" color="#00ccbb">    <link rel="manifest" href="https://www.researchgate.net/manifest.json">
    <meta name="application-name" content="ResearchGate">
    <meta name="msapplication-TileColor" content="#00ccbb">    <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png">
    <meta name="theme-color" content="#444444">     <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html">
    <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html">
    <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html">
    <base href="https://www.researchgate.net/"/>
    <script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">var pageConfig={"module":"lite","action":"lite.PublicationDetails","product":"application","stylesHome":"https:\/\/c5.rgstatic.net\/m\/","staticHost":"https:\/\/c5.rgstatic.net","yuiDisabled":true,"longRunningRequestIdentifier":"LongRunningRequest.lite.PublicationDetails","longRunningRequestFp":"72b825cfd06b891b98628b96f2fb7028d123ac29","platformFeatures":{"nova":[],"rigel":{"useRigelForgeryOverlay":false},"react":{"propTypesStrictMode":false}},"isLitePage":true,"applicationVersionTag":"v331.18.0"};var requestConfig={"correlationId":"rgreq-b47cfca8267a09b3db5e1215401b6cf4","isLoggedIn":false};window.rootUrl="https://www.researchgate.net/";</script>
    <script id="Rg-Request-Cache-Config" nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">var requestCachedConfig={"isCachedPage":true,"continent":"North America","country":"US","isEu":false,"isMobile":false,"isCrawler":false,"logExtendedRequest":true,"logTimeToFirstAd":true,"flag":"cloudflare","correlationId":"rgcf-5ae86288bedc6c68","backendTime":0,"fingerprint":"5ef0053c941015a6cb072139e85c3b3897eec58e"};</script>
            <style>
            .qc-cmp-ui .qc-cmp-arrow-down {
                background: url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' fill='none' stroke='%230080ff' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpath d='M2 5l6 6 6-6'/%3E%3C/svg%3E") 50% no-repeat !important;
            }
        </style>
        <script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">var __wd=["https://www.researchgate.net/", "https://c5.rgstatic.net/", "https://quantcast.mgr.consensu.org", "https://static.quantcast.mgr.consensu.org"];window.quantcastCountryDecision=true;window.useQuantcast=true;
            (function(d,k){function l(b){return(new RegExp("^(.*;)?\\s*"+b+"\\s*=")).test(k.cookie)}function m(b){return(b=(new RegExp("^(.*;)?\\s*"+b+"\\s*=(\\s*[^;])")).exec(k.cookie))&&b[2]||""}function n(){var b=!1;if(l("disable_quantcast")&&"1"===m("disable_quantcast"))return!1;if(d.quantcastCountryDecision)return!0;if(d.trackingConfig){if(d.trackingConfig.isCrawler)return!1;b=d.trackingConfig.isEU}l("cur_country_is_eu")&&(b="1"===m("cur_country_is_eu"));return b}function f(b,d){return d.some(function(c){return 0===
                b.indexOf(c)})}var p=!1,g=[],q=[],h=["appendChild","insertBefore","replaceChild"];if(n())for(var e=0;e<h.length;e++)Element.prototype[h[e]]=function(b){var e=Element.prototype[b];return function(){var c;if(c=0<arguments.length){c=arguments;var a;if(a=0<c.length&&c[0].tagName&&-1!==["script","iframe","link","style"].indexOf(c[0].tagName.toLowerCase()))a=c[0],a.src||a.href?(a=a.src||a.href,f(a,d.__wd)&&-1===a.indexOf("/delivery/manager.js")?a=!0:(a=f(a,q),a=p&&!a)):a=!0,a=!a;a?(g.push({s:this,m:b,a:c}),
                c=!0):c=!1}if(!c)return e.apply(this,arguments)}}(h[e]);d.loadDeferredObjects=function(b){b=b||[];p=!0;q=b;var d=[];g.forEach(function(c){var a=c.a[0];f(a.src||a.href,b)?d.push(c):Element.prototype[c.m].apply(c.s,c.a)});g=d};d.canAskForCookieConsent=n})(window,document);</script>
            <script async=true nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">
            if (window.canAskForCookieConsent() && (function () {
                var a = new XMLHttpRequest, e = window.location.hostname, b = document.createElement("script"),
                    d = document.getElementsByTagName("script")[0], f = (new Date).getTime(),
                    c = "https://quantcast.mgr.consensu.org".concat("/choice/", "AaqDBHcavs6a0", "/", e, "/choice.js").concat("?timestamp=", f);
                a.onreadystatechange = function () {
                    if (4 === this.readyState) {
                        b.async = !0;
                        b.type = "text/javascript";
                        if (200 === this.status) b.src = c; else {
                            var a = "https://quantcast.mgr.consensu.org".concat("/choice.js");
                            c = b.src = a
                        }
                        d.parentNode.insertBefore(b, d)
                    }
                };
                a.open("GET", c, !0);
                a.send()
            }(), "undefined" === typeof window.__cmp)) {
                var count = 0;
                window.__cmp = function () {
                    var a = arguments;
                    if ("object" != typeof window.__cmp.a) 10 > count ? (setTimeout(function () {
                        window.__cmp.apply(window.__cmp, a)
                    }, 400), count++) : console.warn("CMP not loaded after 4 seconds"); else return window.__cmp.apply(window.__cmp, a)
                }
            }
            ;
            (function (s, o, g) {
                var a, m;
                a = s.createElement(o), m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(document, 'script', 'https://www.researchgate.net/javascript/cookie-consent/cookie-consent.js?rnd=' + (new Date).getTime());</script>
    <noscript></noscript><meta property="twitter:site" content="@ResearchGate">
<meta property="twitter:creator" content="@ResearchGate">
<meta property="twitter:card" content="summary">
<meta property="twitter:url" content="https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots">
<meta property="og:site" content="ResearchGate">
<meta property="og:site_name" content="ResearchGate">
<meta property="og:type" content="website">
<meta property="og:title" content="Developmental Robotics: From Babies to Robots | Request PDF">
<meta property="og:description" content="Request PDF | Developmental Robotics: From Babies to Robots | Developmental robotics is a collaborative and interdisciplinary approach to robotics that is directly inspired by the developmental principles and... | Find, read and cite all the research you need on ResearchGate">
<meta property="og:url" content="https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots">
<meta property="og:image" content="https://www.researchgate.net/images/template/default_publication_preview_large.png">
<meta property="gs_meta_revision" content="1.1">
<meta property="citation_title" content="Developmental Robotics: From Babies to Robots">
<meta property="citation_abstract_html_url" content="https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots">
<meta property="citation_fulltext_html_url" content="https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots">
<meta property="citation_author" content="Angelo Cangelosi">
<meta property="citation_author" content="Matthew Schlesinger">
<meta property="citation_publication_date" content="2015/01/23">
<meta property="citation_isbn" content="9780262028011">
<meta property="citation_doi" content="10.7551/mitpress/9320.001.0001">
<meta property="DC.identifier" content="http://dx.doi.org/10.7551/mitpress/9320.001.0001">
<meta property="rg:id" content="PB:290749036">
<link rel="canonical" href="https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots">
<link rel="dns-prefetch" href="https://c5.rgstatic.net">
<link rel="dns-prefetch" href="https://i1.rgstatic.net">
<link rel="preconnect" href="https://securepubads.g.doubleclick.net">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://tags.crwdcntrl.net">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="robots" content="noarchive">
<script type="application/ld+json">{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2015-01-23","headline":"Developmental Robotics: From Babies to Robots","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/290749036_Developmental_Robotics_From_Babies_to_Robots","image":"https:\/\/www.researchgate.net\/images\/template\/default_publication_preview_large.png","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Angelo Cangelosi","url":"https:\/\/www.researchgate.net\/profile\/Angelo_Cangelosi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/784867574951938-1564138420404_Q64\/Angelo_Cangelosi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Manchester"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Matthew Schlesinger","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2139240686_Matthew_Schlesinger","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"MIT PRESS","url":"unknown","logo":"unknown"},"citation":[{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-06-30","headline":"Towards hybrid primary intersubjectivity: a neural robotics library for human science","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342542080_Towards_hybrid_primary_intersubjectivity_a_neural_robotics_library_for_human_science","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Hendry F. Chame","url":"https:\/\/www.researchgate.net\/profile\/Hendry_Chame","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/822236088901632-1573047768824_Q64\/Hendry_Chame.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Okinawa Institute of Science and Technology"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Ahmadreza Ahmadi","url":"https:\/\/www.researchgate.net\/profile\/Ahmadreza_Ahmadi5","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/279090490822656-1443551766219_Q64\/Ahmadreza_Ahmadi5.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The Commonwealth Scientific and Industrial Research Organisation"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Jun Tani","url":"https:\/\/www.researchgate.net\/scientific-contributions\/11210515_Jun_Tani","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-06-24","headline":"Explainable robotic systems: Interpreting outcome-focused actions in a reinforcement learning scenario","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342435632_Explainable_robotic_systems_Interpreting_outcome-focused_actions_in_a_reinforcement_learning_scenario","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Francisco Cruz","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2167031880_Francisco_Cruz","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Richard Dazeley","url":"https:\/\/www.researchgate.net\/profile\/Richard_Dazeley","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272659330105375-1442018458682_Q64\/Richard_Dazeley.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Deakin University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Peter Vamplew","url":"https:\/\/www.researchgate.net\/profile\/Peter_Vamplew","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/568836671930369-1512632639962_Q64\/Peter_Vamplew.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Federation University Australia"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-05-19","headline":"Prediction error-driven memory consolidation for continual learning. On the case of adaptive greenhouse models","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342408930_Prediction_error-driven_memory_consolidation_for_continual_learning_On_the_case_of_adaptive_greenhouse_models","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Luis Miranda","url":"https:\/\/www.researchgate.net\/profile\/Luis_Miranda40","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/772182376452097-1561114033784_Q64\/Luis_Miranda40.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Humboldt-Universit\u00e4t zu Berlin"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Uwe Schmidt","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2176835974_Uwe_Schmidt","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Guido Schillaci","url":"https:\/\/www.researchgate.net\/profile\/Guido_Schillaci","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/282267919372300-1444309324682_Q64\/Guido_Schillaci.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Scuola Superiore Sant'Anna"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-06-01","headline":"Guest Editorial Special Issue on Multidisciplinary Perspectives on Mechanisms of Language Learning","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342090428_Guest_Editorial_Special_Issue_on_Multidisciplinary_Perspectives_on_Mechanisms_of_Language_Learning","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Malte Schilling","url":"https:\/\/www.researchgate.net\/profile\/Malte_Schilling","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272734458478644-1442036371001_Q64\/Malte_Schilling.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Bielefeld University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Katharina J. Rohlfing","url":"https:\/\/www.researchgate.net\/profile\/Katharina_Rohlfing","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/278465891848193-1443402850972_Q64\/Katharina_Rohlfing.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e4t Paderborn"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Paul Vogt","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2176198330_Paul_Vogt","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Michael Spranger","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2176185437_Michael_Spranger","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"http:\/\/ieeexplore.ieee.org\/xpl\/RecentIssue.jsp?punumber=7274989","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-06-09","headline":"Teaching NICO How to Grasp: An Empirical Study on Crossmodal Social Interaction as a Key Factor for Robots...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342057541_Teaching_NICO_How_to_Grasp_An_Empirical_Study_on_Crossmodal_Social_Interaction_as_a_Key_Factor_for_Robots_Learning_From_Humans","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Theresa Pekarek Rosin","url":"https:\/\/www.researchgate.net\/profile\/Theresa_Pekarek_Rosin","image":"https:\/\/c5.rgstatic.net\/m\/4671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Erik Strahl","url":"https:\/\/www.researchgate.net\/profile\/Erik_Strahl","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/277050150670344-1443065311805_Q64\/Erik_Strahl.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Matthias Kerzel","url":"https:\/\/www.researchgate.net\/profile\/Matthias_Kerzel","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/638785058529281-1529309634390_Q64\/Matthias_Kerzel.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Stefan Wermter","url":"https:\/\/www.researchgate.net\/profile\/Stefan_Wermter","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/312714742697984-1451568412045_Q64\/Stefan_Wermter.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Frontiers","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-06-01","headline":"Hierarchical Interest-Driven Goal Babbling for Efficient Bootstrapping of Sensorimotor skills","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342048052_Hierarchical_Interest-Driven_Goal_Babbling_for_Efficient_Bootstrapping_of_Sensorimotor_skills","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Rania Rayyes","url":"https:\/\/www.researchgate.net\/profile\/Rania_Rayyes","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/436368292749312-1481049716227_Q64\/Rania_Rayyes.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Technische Universit\u00e4t Braunschweig"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Heiko Donat","url":"https:\/\/www.researchgate.net\/profile\/Heiko_Donat","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/515037208027136-1499805848030_Q64\/Heiko_Donat.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Technische Universit\u00e4t Braunschweig"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Jochen J. Steil","url":"https:\/\/www.researchgate.net\/profile\/Jochen_Steil","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/641503240155136-1529957699271_Q64\/Jochen_Steil.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Technische Universit\u00e4t Braunschweig"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-06-11","headline":"Efficient Online Interest-Driven Exploration for Developmental Robots","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342047886_Efficient_Online_Interest-Driven_Exploration_for_Developmental_Robots","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Rania Rayyes","url":"https:\/\/www.researchgate.net\/profile\/Rania_Rayyes","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/436368292749312-1481049716227_Q64\/Rania_Rayyes.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Technische Universit\u00e4t Braunschweig"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Heiko Donat","url":"https:\/\/www.researchgate.net\/profile\/Heiko_Donat","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/515037208027136-1499805848030_Q64\/Heiko_Donat.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Technische Universit\u00e4t Braunschweig"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Jochen J. Steil","url":"https:\/\/www.researchgate.net\/profile\/Jochen_Steil","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/641503240155136-1529957699271_Q64\/Jochen_Steil.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Technische Universit\u00e4t Braunschweig"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"http:\/\/ieeexplore.ieee.org\/xpl\/RecentIssue.jsp?punumber=7274989","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-06-07","headline":"Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/342008351_Intrinsic_motivation_and_episodic_memories_for_robot_exploration_of_high-dimensional_sensory_spaces","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Timoth\u00e9e Wintz","url":"https:\/\/www.researchgate.net\/profile\/Timothee_Wintz","image":"https:\/\/c5.rgstatic.net\/m\/4671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Sony Computer Science Laboratories, Inc."}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Guido Schillaci","url":"https:\/\/www.researchgate.net\/profile\/Guido_Schillaci","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/282267919372300-1444309324682_Q64\/Guido_Schillaci.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Scuola Superiore Sant'Anna"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Antonio Pico Villalpando","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2159995706_Antonio_Pico_Villalpando","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Verena Vanessa Hafner","url":"https:\/\/www.researchgate.net\/profile\/Verena_Hafner","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/871462063857664-1584784155569_Q64\/Verena_Hafner.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Humboldt-Universit\u00e4t zu Berlin"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"SAGE Publications","url":"http:\/\/adb.sagepub.com\/","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-05-26","headline":"Learning action models by curiosity driven self exploration and language guided generalization","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/341654093_Learning_action_models_by_curiosity_driven_self_exploration_and_language_guided_generalization","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Maximilian Panzner","url":"https:\/\/www.researchgate.net\/profile\/Maximilian_Panzner","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/304079211827204-1449509541980_Q64\/Maximilian_Panzner.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Bielefeld University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-05-01","headline":"Searching for Models for Psychological Science: A Possible Contribution of Simulation","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/341444281_Searching_for_Models_for_Psychological_Science_A_Possible_Contribution_of_Simulation","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Santo F. Di Nuovo","url":"https:\/\/www.researchgate.net\/profile\/Santo_Di_Nuovo","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/273665375862784-1442258318030_Q64\/Santo_Di_Nuovo.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Catania"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Springer Verlag","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-11-13","headline":"La sensibilit\u00e9 aux contingences sensorimotrices chez le b\u00e9b\u00e9 et son r\u00f4le dans le d\u00e9veloppement du...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/341426178_La_sensibilite_aux_contingences_sensorimotrices_chez_le_bebe_et_son_role_dans_le_developpement_du_savoir-faire_corporel_Approche_croisee_en_robotique_et_psychologie_du_developpement","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Lisa Jacquey","url":"https:\/\/www.researchgate.net\/profile\/Lisa_Jacquey","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/856964636606466-1581327699371_Q64\/Lisa_Jacquey.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University Paris Nanterre"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-05-13","headline":"DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/341369189_DREAM_Architecture_a_Developmental_Approach_to_Open-Ended_Learning_in_Robotics","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"St\u00e9phane Doncieux","url":"https:\/\/www.researchgate.net\/profile\/Stephane_Doncieux","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272463351250946-1441971733318_Q64\/Stephane_Doncieux.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Sorbonne Universit\u00e9"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Nicolas Bredeche","url":"https:\/\/www.researchgate.net\/profile\/Nicolas_Bredeche","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272410838564917-1441959213908_Q64\/Nicolas_Bredeche.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Sorbonne Universit\u00e9"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Leni K. Le Goff","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2122326604_Leni_K_Le_Goff","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Richard Duro","url":"https:\/\/www.researchgate.net\/profile\/Richard_Duro","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272217246269465-1441913057338_Q64\/Richard_Duro.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of A Coru\u00f1a"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-09-10","headline":"Gestures and Object Affordances for Human\u2013Robot Interaction","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/340941248_Gestures_and_Object_Affordances_for_Human-Robot_Interaction","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Giovanni Saponaro","url":"https:\/\/www.researchgate.net\/profile\/Giovanni_Saponaro","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/835789256658945-1576279095556_Q64\/Giovanni_Saponaro.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Lisbon"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-04-03","headline":"Shall I Trust You? From Child-Robot Interaction to Trusting Relationships","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/340429465_Shall_I_Trust_You_From_Child-Robot_Interaction_to_Trusting_Relationships","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Cinzia Di Dio","url":"https:\/\/www.researchgate.net\/profile\/Cinzia_Di_Dio","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/352570078973953-1461070665386_Q64\/Cinzia_Di_Dio.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e0 Cattolica del Sacro Cuore"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Federico Manzi","url":"https:\/\/www.researchgate.net\/profile\/Federico_Manzi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/664543986462723-1535451041907_Q64\/Federico_Manzi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Giulia Peretti","url":"https:\/\/www.researchgate.net\/profile\/Giulia_Peretti","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/702967627919363-1544611951687_Q64\/Giulia_Peretti.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Antonella Marchetti","url":"https:\/\/www.researchgate.net\/profile\/Antonella_Marchetti","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/326859662348290-1454940824962_Q64\/Antonella_Marchetti.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Frontiers","url":"http:\/\/www.frontiersin.org\/Psychology","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-03-23","headline":"A Developmental Neuro-Robotics Approach for Boosting the Recognition of Handwritten Digits","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/340114852_A_Developmental_Neuro-Robotics_Approach_for_Boosting_the_Recognition_of_Handwritten_Digits","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Alessandro Di Nuovo","url":"https:\/\/www.researchgate.net\/profile\/Alessandro_Di_Nuovo","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272455281410087-1441969809775_Q64\/Alessandro_Di_Nuovo.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Sheffield Hallam University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-03-12","headline":"An Experiment in Morphological Development for Learning ANN Based Controllers","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/339972044_An_Experiment_in_Morphological_Development_for_Learning_ANN_Based_Controllers","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Martin Naya","url":"https:\/\/www.researchgate.net\/profile\/Martin_Naya","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/466431264464896-1488217287095_Q64\/Martin_Naya.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of A Coru\u00f1a"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Andres Fai\u00f1a","url":"https:\/\/www.researchgate.net\/profile\/Andres_Faina","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/525483636948993-1502296470688_Q64\/Andres_Faina.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"IT University of Copenhagen"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Richard Duro","url":"https:\/\/www.researchgate.net\/profile\/Richard_Duro","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272217246269465-1441913057338_Q64\/Richard_Duro.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of A Coru\u00f1a"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-03-12","headline":"Some Experiments on the influence of Problem Hardness in Morphological Development based Learning of...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/339898577_Some_Experiments_on_the_influence_of_Problem_Hardness_in_Morphological_Development_based_Learning_of_Neural_Controllers","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Martin Naya","url":"https:\/\/www.researchgate.net\/profile\/Martin_Naya","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/466431264464896-1488217287095_Q64\/Martin_Naya.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of A Coru\u00f1a"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Andres Fai\u00f1a","url":"https:\/\/www.researchgate.net\/profile\/Andres_Faina","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/525483636948993-1502296470688_Q64\/Andres_Faina.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"IT University of Copenhagen"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Richard Duro","url":"https:\/\/www.researchgate.net\/profile\/Richard_Duro","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272217246269465-1441913057338_Q64\/Richard_Duro.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of A Coru\u00f1a"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-02-27","headline":"Come I bambini pensano alla mente di un robot. Il ruolo dell\u2019attaccamento e della Teoria della Mente...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/339528799_Come_I_bambini_pensano_alla_mente_di_un_robot_Il_ruolo_dell'attaccamento_e_della_Teoria_della_Mente_nell'attribuzione_di_stati_mentali_a_un_agente_robotico_How_children_think_about_the_robot's_mind_Th","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Cinzia Di Dio","url":"https:\/\/www.researchgate.net\/profile\/Cinzia_Di_Dio","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/352570078973953-1461070665386_Q64\/Cinzia_Di_Dio.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e0 Cattolica del Sacro Cuore"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Federico Manzi","url":"https:\/\/www.researchgate.net\/profile\/Federico_Manzi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/664543986462723-1535451041907_Q64\/Federico_Manzi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Giulia Peretti","url":"https:\/\/www.researchgate.net\/profile\/Giulia_Peretti","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/702967627919363-1544611951687_Q64\/Giulia_Peretti.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Antonella Marchetti","url":"https:\/\/www.researchgate.net\/profile\/Antonella_Marchetti","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/326859662348290-1454940824962_Q64\/Antonella_Marchetti.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Societ\u00e0 Editrice il Mulino","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-02-21","headline":"Prerequisites for an Artificial Self","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/339486753_Prerequisites_for_an_Artificial_Self","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Verena Vanessa Hafner","url":"https:\/\/www.researchgate.net\/profile\/Verena_Hafner","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/871462063857664-1584784155569_Q64\/Verena_Hafner.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Humboldt-Universit\u00e4t zu Berlin"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Pontus Loviken","url":"https:\/\/www.researchgate.net\/profile\/Pontus_Loviken","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/543559944474624-1506606197892_Q64\/Pontus_Loviken.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Schlumberger Limited"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Antonio Pico Villalpando","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2159995706_Antonio_Pico_Villalpando","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Guido Schillaci","url":"https:\/\/www.researchgate.net\/profile\/Guido_Schillaci","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/282267919372300-1444309324682_Q64\/Guido_Schillaci.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Scuola Superiore Sant'Anna"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-02-21","headline":"Prerequisites for an Artificial Self","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/339411547_Prerequisites_for_an_Artificial_Self","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Verena Vanessa Hafner","url":"https:\/\/www.researchgate.net\/profile\/Verena_Hafner","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/871462063857664-1584784155569_Q64\/Verena_Hafner.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Humboldt-Universit\u00e4t zu Berlin"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Pontus Loviken","url":"https:\/\/www.researchgate.net\/profile\/Pontus_Loviken","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/543559944474624-1506606197892_Q64\/Pontus_Loviken.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Schlumberger Limited"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Antonio Pico Villalpando","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2159995706_Antonio_Pico_Villalpando","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Guido Schillaci","url":"https:\/\/www.researchgate.net\/profile\/Guido_Schillaci","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/282267919372300-1444309324682_Q64\/Guido_Schillaci.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Scuola Superiore Sant'Anna"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Frontiers","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-02-14","headline":"Cross-Talk of Low-Level Sensory and High-Level Cognitive Processing: Development, Mechanisms, and...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/339315108_Cross-Talk_of_Low-Level_Sensory_and_High-Level_Cognitive_Processing_Development_Mechanisms_and_Relevance_for_Cross-Modal_Abilities_of_the_Brain","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Xiaxia Xu","url":"https:\/\/www.researchgate.net\/profile\/Xiaxia_Xu","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/656636414812160-1533565729002_Q64\/Xiaxia_Xu.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University Medical Center Hamburg - Eppendorf"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Ileana L Hanganu-Opatz","url":"https:\/\/www.researchgate.net\/profile\/Ileana_Hanganu-Opatz2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/459819359444993-1486640886401_Q64\/Ileana_Hanganu-Opatz2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University Medical Center Hamburg - Eppendorf"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Malte Bieler","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2071956521_Malte_Bieler","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Frontiers","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-02-08","headline":"Improved and scalable online learning of spatial concepts and language models with mapping","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/339129170_Improved_and_scalable_online_learning_of_spatial_concepts_and_language_models_with_mapping","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tetsunari Inamura","url":"https:\/\/www.researchgate.net\/profile\/Tetsunari_Inamura","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/273743087927303-1442276846418_Q64\/Tetsunari_Inamura.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"National Institute of Informatics"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Akira Taniguchi","url":"https:\/\/www.researchgate.net\/profile\/Akira_Taniguchi2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/421315027718145-1477460738439_Q64\/Akira_Taniguchi2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Yoshinobu Hagiwara","url":"https:\/\/www.researchgate.net\/profile\/Yoshinobu_Hagiwara","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/273782359195650-1442286209102_Q64\/Yoshinobu_Hagiwara.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tadahiro Taniguchi","url":"https:\/\/www.researchgate.net\/profile\/Tadahiro_Taniguchi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/277845675921468-1443254979450_Q64\/Tadahiro_Taniguchi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Springer Verlag","url":"http:\/\/www.springerlink.com\/openurl.asp?genre=journal&issn=0929-5593","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-01-31","headline":"A curious formulation robot enables the discovery of a novel protocell behavior","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/338957151_A_curious_formulation_robot_enables_the_discovery_of_a_novel_protocell_behavior","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Jonathan Grizou","url":"https:\/\/www.researchgate.net\/profile\/Jonathan_Grizou","image":"https:\/\/c5.rgstatic.net\/m\/4671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"National Institute for Research in Computer Science and Control"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Laurie Points","url":"https:\/\/www.researchgate.net\/profile\/Laurie_Points","image":"https:\/\/c5.rgstatic.net\/m\/4671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Glasgow"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Abhishek Sharma","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2146051871_Abhishek_Sharma","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Leroy Cronin","url":"https:\/\/www.researchgate.net\/scientific-contributions\/39871589_Leroy_Cronin","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"http:\/\/advances.sciencemag.org\/","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-01-17","headline":"Editorial: Intrinsically Motivated Open-Ended Learning in Autonomous Robots","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/338663463_Editorial_Intrinsically_Motivated_Open-Ended_Learning_in_Autonomous_Robots","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Pierre-Yves Oudeyer","url":"https:\/\/www.researchgate.net\/profile\/Pierre-Yves_Oudeyer","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272448205619223-1441968122611_Q64\/Pierre-Yves_Oudeyer.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"National Institute for Research in Computer Science and Control"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Andrew G. Barto","url":"https:\/\/www.researchgate.net\/scientific-contributions\/3224749_Andrew_G_Barto","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Vieri Giuliano Santucci","url":"https:\/\/www.researchgate.net\/profile\/Vieri_Santucci","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272446259462193-1441967658892_Q64\/Vieri_Santucci.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Italian National Research Council"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Gianluca Baldassarre","url":"https:\/\/www.researchgate.net\/profile\/Gianluca_Baldassarre","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/273803989221394-1442291366879_Q64\/Gianluca_Baldassarre.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Italian National Research Council"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Frontiers","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-12-20","headline":"The dynamics of motor learning through the formation of internal models","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/338092098_The_dynamics_of_motor_learning_through_the_formation_of_internal_models","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Camilla Pierella","url":"https:\/\/www.researchgate.net\/profile\/Camilla_Pierella","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/279156945375233-1443567610203_Q64\/Camilla_Pierella.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Maura Casadio","url":"https:\/\/www.researchgate.net\/scientific-contributions\/38329540_Maura_Casadio","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Ferdinando A Mussa-Ivaldi","url":"https:\/\/www.researchgate.net\/scientific-contributions\/4981705_Ferdinando_A_Mussa-Ivaldi","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Sara A Solla","url":"https:\/\/www.researchgate.net\/profile\/Sara_Solla","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272700656582668-1442028311985_Q64\/Sara_Solla.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Northwestern University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Public Library of Science","url":"http:\/\/compbiol.plosjournals.org\/","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2017-11-01","headline":"Learning of Neurobotic Visuomotor Abilities based on Interactions with the Environment","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/338067808_Learning_of_Neurobotic_Visuomotor_Abilities_based_on_Interactions_with_the_Environment","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Matthias Kerzel","url":"https:\/\/www.researchgate.net\/profile\/Matthias_Kerzel","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/638785058529281-1529309634390_Q64\/Matthias_Kerzel.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Stefan Wermter","url":"https:\/\/www.researchgate.net\/profile\/Stefan_Wermter","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/312714742697984-1451568412045_Q64\/Stefan_Wermter.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-08-01","headline":"Developing Robot Reaching Skill via Look-ahead Planning","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/337981905_Developing_Robot_Reaching_Skill_via_Look-ahead_Planning","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tianlin Liu","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2120291959_Tianlin_Liu","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Mengxi Nie","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2147912668_Mengxi_Nie","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Xihong Wu","url":"https:\/\/www.researchgate.net\/scientific-contributions\/39459825_Xihong_Wu","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Dingsheng Luo","url":"https:\/\/www.researchgate.net\/profile\/Dingsheng_Luo","image":"https:\/\/c5.rgstatic.net\/m\/4671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Peking University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-12-10","headline":"Developing the knowledge of number digits in a child-like robot","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/337876291_Developing_the_knowledge_of_number_digits_in_a_child-like_robot","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"James L. McClelland","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2167841258_James_L_McClelland","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Alessandro Di Nuovo","url":"https:\/\/www.researchgate.net\/profile\/Alessandro_Di_Nuovo","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272455281410087-1441969809775_Q64\/Alessandro_Di_Nuovo.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Sheffield Hallam University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-12-01","headline":"Symbol Emergence as an Interpersonal Multimodal Categorization","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/337860529_Symbol_Emergence_as_an_Interpersonal_Multimodal_Categorization","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Yoshinobu Hagiwara","url":"https:\/\/www.researchgate.net\/profile\/Yoshinobu_Hagiwara","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/273782359195650-1442286209102_Q64\/Yoshinobu_Hagiwara.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Hiroyoshi Kobayashi","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2139895506_Hiroyoshi_Kobayashi","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Akira Taniguchi","url":"https:\/\/www.researchgate.net\/profile\/Akira_Taniguchi2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/421315027718145-1477460738439_Q64\/Akira_Taniguchi2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tadahiro Taniguchi","url":"https:\/\/www.researchgate.net\/profile\/Tadahiro_Taniguchi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/277845675921468-1443254979450_Q64\/Tadahiro_Taniguchi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-12-04","headline":"Sensorimotor Contingencies as a Key Drive of Development: From Babies to Robots","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/337732920_Sensorimotor_Contingencies_as_a_Key_Drive_of_Development_From_Babies_to_Robots","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Lisa Jacquey","url":"https:\/\/www.researchgate.net\/profile\/Lisa_Jacquey","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/856964636606466-1581327699371_Q64\/Lisa_Jacquey.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University Paris Nanterre"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Gianluca Baldassarre","url":"https:\/\/www.researchgate.net\/profile\/Gianluca_Baldassarre","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/273803989221394-1442291366879_Q64\/Gianluca_Baldassarre.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Italian National Research Council"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Vieri Giuliano Santucci","url":"https:\/\/www.researchgate.net\/profile\/Vieri_Santucci","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272446259462193-1441967658892_Q64\/Vieri_Santucci.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Italian National Research Council"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"J. Kevin O\u2019Regan","url":"https:\/\/www.researchgate.net\/profile\/J_Oregan","image":"https:\/\/c5.rgstatic.net\/m\/4671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Paris Descartes, CPSC"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Frontiers","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-11-01","headline":"Integrated Cognitive Architecture for Robot Learning of Action and Language","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/337643066_Integrated_Cognitive_Architecture_for_Robot_Learning_of_Action_and_Language","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Kazuki Miyazawa","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2167312373_Kazuki_Miyazawa","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Takato Horii","url":"https:\/\/www.researchgate.net\/profile\/Takato_Horii","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/378619366002692-1467281299704_Q64\/Takato_Horii.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Osaka University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tatsuya Aoki","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2117794322_Tatsuya_Aoki","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Takayuki Nagai","url":"https:\/\/www.researchgate.net\/profile\/Takayuki_Nagai","image":"https:\/\/c5.rgstatic.net\/m\/4671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Electro-Communications"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-11-25","headline":"Intrinsically Motivated Active Perception for Multi-areas View Tasks","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/337490581_Intrinsically_Motivated_Active_Perception_for_Multi-areas_View_Tasks","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Dashun Pei","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2167037781_Dashun_Pei","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Linhua Jiang","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2167032254_Linhua_Jiang","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-11-01","headline":"Using AI Methods to Evaluate a Minimal Model for Perception","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/337052393_Using_AI_Methods_to_Evaluate_a_Minimal_Model_for_Perception","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Robert Prentner","url":"https:\/\/www.researchgate.net\/profile\/Robert_Prentner","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/281170844635137-1444047761279_Q64\/Robert_Prentner.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of California, Irvine"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Chris Fields","url":"https:\/\/www.researchgate.net\/profile\/Chris_Fields2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/277701085679619-1443220506609_Q64\/Chris_Fields2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"chrisfieldsresearch.com"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-10-01","headline":"Editorial: Machine Learning Methods for High-Level Cognitive Capabilities in Robotics","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336719137_Editorial_Machine_Learning_Methods_for_High-Level_Cognitive_Capabilities_in_Robotics","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Emre Ugur","url":"https:\/\/www.researchgate.net\/profile\/Emre_Ugur2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272478324916228-1441975303225_Q64\/Emre_Ugur2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Advanced Telecommunications Research Institute"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tetsuya Ogata","url":"https:\/\/www.researchgate.net\/profile\/Tetsuya_Ogata","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/687174005583874-1540846458230_Q64\/Tetsuya_Ogata.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Waseda University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Yiannis Demiris","url":"https:\/\/www.researchgate.net\/profile\/Yiannis_Demiris","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/280750613123073-1443947570259_Q64\/Yiannis_Demiris.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Imperial College London"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tadahiro Taniguchi","url":"https:\/\/www.researchgate.net\/profile\/Tadahiro_Taniguchi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/277845675921468-1443254979450_Q64\/Tadahiro_Taniguchi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Frontiers","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-10-17","headline":"Maschinenethik und Roboterethik","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336594986_Maschinenethik_und_Roboterethik","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Janina Loh","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2125144249_Janina_Loh","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-04-12","headline":"Integrating Image-based and Knowledge-based Representation Learning","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336250917_Integrating_Image-based_and_Knowledge-based_Representation_Learning","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Ruobing Xie","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2163377975_Ruobing_Xie","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Zhiyuan Liu","url":"https:\/\/www.researchgate.net\/profile\/Zhiyuan_Liu","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/647976968024066-1531501156358_Q64\/Zhiyuan_Liu.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Tsinghua University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Maosong Sun","url":"https:\/\/www.researchgate.net\/scientific-contributions\/70737274_Maosong_Sun","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Stefan Heinrich","url":"https:\/\/www.researchgate.net\/profile\/Stefan_Heinrich","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/860086712942592-1582072060346_Q64\/Stefan_Heinrich.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Tokyo"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"http:\/\/ieeexplore.ieee.org\/xpl\/RecentIssue.jsp?punumber=7274989","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-10-01","headline":"Unsupervised Phoneme and Word Discovery From Multiple Speakers Using Double Articulation Analyzer and...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336194481_Unsupervised_Phoneme_and_Word_Discovery_From_Multiple_Speakers_Using_Double_Articulation_Analyzer_and_Neural_Network_With_Parametric_Bias","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Ryo Nakashima","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2076583141_Ryo_Nakashima","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Ryo Ozaki","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2159157407_Ryo_Ozaki","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tadahiro Taniguchi","url":"https:\/\/www.researchgate.net\/profile\/Tadahiro_Taniguchi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/277845675921468-1443254979450_Q64\/Tadahiro_Taniguchi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Ritsumeikan University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-10-01","headline":"Toward evolutionary and developmental intelligence","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336182040_Toward_evolutionary_and_developmental_intelligence","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Kenji Doya","url":"https:\/\/www.researchgate.net\/profile\/Kenji_Doya","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/874025555329024-1585395339066_Q64\/Kenji_Doya.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Okinawa Institute of Science and Technology"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Tadahiro Taniguchi","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2164349806_Tadahiro_Taniguchi","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"http:\/\/www.journals.elsevier.com\/current-opinion-in-behavioral-sciences","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-08-01","headline":"Neurocognitive Shared Visuomotor Network for End-to-end Learning of Object Identification, Localization...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336156737_Neurocognitive_Shared_Visuomotor_Network_for_End-to-end_Learning_of_Object_Identification_Localization_and_Grasping_on_a_Humanoid","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Manfred Eppe","url":"https:\/\/www.researchgate.net\/profile\/Manfred_Eppe","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/439227298127872-1481731356508_Q64\/Manfred_Eppe.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Stefan Heinrich","url":"https:\/\/www.researchgate.net\/profile\/Stefan_Heinrich","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/860086712942592-1582072060346_Q64\/Stefan_Heinrich.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Tokyo"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Matthias Kerzel","url":"https:\/\/www.researchgate.net\/profile\/Matthias_Kerzel","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/638785058529281-1529309634390_Q64\/Matthias_Kerzel.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Stefan Wermter","url":"https:\/\/www.researchgate.net\/profile\/Stefan_Wermter","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/312714742697984-1451568412045_Q64\/Stefan_Wermter.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-08-01","headline":"Replication of Infant Behaviours with a Babybot: Early Pointing Gesture Comprehension","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336156652_Replication_of_Infant_Behaviours_with_a_Babybot_Early_Pointing_Gesture_Comprehension","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Baris Serhan","url":"https:\/\/www.researchgate.net\/profile\/Baris_Serhan","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/492784201236480-1494500317359_Q64\/Baris_Serhan.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Lincoln"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Angelo Cangelosi","url":"https:\/\/www.researchgate.net\/profile\/Angelo_Cangelosi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/784867574951938-1564138420404_Q64\/Angelo_Cangelosi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Manchester"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-08-01","headline":"Mindreading for Robots: Predicting Intentions via Dynamical Clustering of Human Postures","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/336156474_Mindreading_for_Robots_Predicting_Intentions_via_Dynamical_Clustering_of_Human_Postures","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Samuele Vinanzi","url":"https:\/\/www.researchgate.net\/profile\/Samuele_Vinanzi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/543561789317120-1506606637460_Q64\/Samuele_Vinanzi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Manchester"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Christian Goerick","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2164366571_Christian_Goerick","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Angelo Cangelosi","url":"https:\/\/www.researchgate.net\/profile\/Angelo_Cangelosi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/784867574951938-1564138420404_Q64\/Angelo_Cangelosi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Manchester"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-09-09","headline":"Mixed-Reality Deep Reinforcement Learning for a Reach-to-grasp Task","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/335699290_Mixed-Reality_Deep_Reinforcement_Learning_for_a_Reach-to-grasp_Task","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Hadi Beik Mohammadi","url":"https:\/\/www.researchgate.net\/profile\/Hadi_Beik_Mohammadi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/564953765642240-1511706882029_Q64\/Hadi_Beik_Mohammadi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Bosch Center for Artificial Intelligence"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Mohammad Ali Zamani","url":"https:\/\/www.researchgate.net\/profile\/Mohammad_Ali_Zamani2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/613432185339905-1523265038449_Q64\/Mohammad_Ali_Zamani2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Hamburger Informatik Technologie-Center (HITeC)"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Matthias Kerzel","url":"https:\/\/www.researchgate.net\/profile\/Matthias_Kerzel","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/638785058529281-1529309634390_Q64\/Matthias_Kerzel.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Stefan Wermter","url":"https:\/\/www.researchgate.net\/profile\/Stefan_Wermter","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/312714742697984-1451568412045_Q64\/Stefan_Wermter.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-08-09","headline":"On building a person: benchmarks for robotic personhood","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/335082599_On_building_a_person_benchmarks_for_robotic_personhood","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"John Barresi","url":"https:\/\/www.researchgate.net\/profile\/John_Barresi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272501099986955-1441980733471_Q64\/John_Barresi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Dalhousie University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Taylor & Francis","url":"http:\/\/www.informaworld.com\/openurl?genre=journal&issn=0952813X","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-08-09","headline":"Reaching development through visuo-proprioceptive-tactile integration on a humanoid robot - a deep...","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/335062725_Reaching_development_through_visuo-proprioceptive-tactile_integration_on_a_humanoid_robot_-_a_deep_learning_approach","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Phuong D. H. Nguyen","url":"https:\/\/www.researchgate.net\/profile\/Phuong_Nguyen370","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/541300690612225-1506067549564_Q64\/Phuong_Nguyen370.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Hamburg"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Matej Hoffmann","url":"https:\/\/www.researchgate.net\/profile\/Matej_Hoffmann","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/552586101129216-1508758201339_Q64\/Matej_Hoffmann.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Czech Technical University in Prague"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Ugo Pattacini","url":"https:\/\/www.researchgate.net\/scientific-contributions\/70648233_Ugo_Pattacini","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Giorgio Metta","url":"https:\/\/www.researchgate.net\/scientific-contributions\/3227665_Giorgio_Metta","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-01-01","headline":"On-Line Educational Resources on Robotics: A Review","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/334771935_On-Line_Educational_Resources_on_Robotics_A_Review","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Maria Pozzi","url":"https:\/\/www.researchgate.net\/profile\/Maria_Pozzi3","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/496118087204864-1495295177603_Q64\/Maria_Pozzi3.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e0 degli Studi di Siena"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Domenico Prattichizzo","url":"https:\/\/www.researchgate.net\/profile\/Domenico_Prattichizzo","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272304324214786-1441933818268_Q64\/Domenico_Prattichizzo.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e0 degli Studi di Siena"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Monica Malvezzi","url":"https:\/\/www.researchgate.net\/profile\/Monica_Malvezzi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/338066851024898-1457612826913_Q64\/Monica_Malvezzi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e0 degli Studi di Siena"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2020-01-01","headline":"Inclusive Robotics and AI \u2013 Some Urgent Ethical and Societal Issues","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/334771680_Inclusive_Robotics_and_AI_-_Some_Urgent_Ethical_and_Societal_Issues","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Mark Coeckelbergh","url":"https:\/\/www.researchgate.net\/profile\/Mark_Coeckelbergh","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/340894101327896-1458286895972_Q64\/Mark_Coeckelbergh.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"University of Vienna"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-07-18","headline":"The Use of Social Robots and the Uncanny Valley Phenomenon","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/334518172_The_Use_of_Social_Robots_and_the_Uncanny_Valley_Phenomenon","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Melinda Mende","url":"https:\/\/www.researchgate.net\/profile\/Melinda_Mende","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/690678216552448-1541681927105_Q64\/Melinda_Mende.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e4t Potsdam"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Katharina K\u00fchne","url":"https:\/\/www.researchgate.net\/profile\/Katharina_Kuehne2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/367518507126785-1464634648316_Q64\/Katharina_Kuehne2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Brandenburgklinik; Potsdam University"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Martin H Fischer","url":"https:\/\/www.researchgate.net\/profile\/Martin_Fischer2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272449614905356-1441968458206_Q64\/Martin_Fischer2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e4t Potsdam"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-07-18","headline":"Intimate Relationships with Humanoid Robots: Exploring Human Sexuality in the Twenty-First Century","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/334516278_Intimate_Relationships_with_Humanoid_Robots_Exploring_Human_Sexuality_in_the_Twenty-First_Century","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Yuefang Zhou","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2145220295_Yuefang_Zhou","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Martin H Fischer","url":"https:\/\/www.researchgate.net\/profile\/Martin_Fischer2","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272449614905356-1441968458206_Q64\/Martin_Fischer2.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e4t Potsdam"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-07-17","headline":"Preliminary Investigation on Visual Finger-Counting with the iCub Robot Cameras and Hands","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/334503805_Preliminary_Investigation_on_Visual_Finger-Counting_with_the_iCub_Robot_Cameras_and_Hands","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Alexandr Lucas","url":"https:\/\/www.researchgate.net\/profile\/Alexandr_Lucas","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/669671934152710-1536673639855_Q64\/Alexandr_Lucas.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"The University of Sheffield"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Carlos Ricolfe-Viala","url":"https:\/\/www.researchgate.net\/scientific-contributions\/2160034975_Carlos_Ricolfe-Viala","image":"https:\/\/c5.rgstatic.net\/m\/435982309481010\/images\/template\/default\/author\/author_default_m.jpg"},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Alessandro Di Nuovo","url":"https:\/\/www.researchgate.net\/profile\/Alessandro_Di_Nuovo","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/272455281410087-1441969809775_Q64\/Alessandro_Di_Nuovo.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Sheffield Hallam University"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}},{"@context":"https:\/\/schema.org\/","@type":"ScholarlyArticle","datePublished":"2019-07-16","headline":"Shall I trust you? From child human-robot interaction to trusting relationships","mainEntityOfPage":"https:\/\/www.researchgate.net\/publication\/334502304_Shall_I_trust_you_From_child_human-robot_interaction_to_trusting_relationships","author":[{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Cinzia Di Dio","url":"https:\/\/www.researchgate.net\/profile\/Cinzia_Di_Dio","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/352570078973953-1461070665386_Q64\/Cinzia_Di_Dio.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Universit\u00e0 Cattolica del Sacro Cuore"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Federico Manzi","url":"https:\/\/www.researchgate.net\/profile\/Federico_Manzi","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/664543986462723-1535451041907_Q64\/Federico_Manzi.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Giulia Peretti","url":"https:\/\/www.researchgate.net\/profile\/Giulia_Peretti","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/702967627919363-1544611951687_Q64\/Giulia_Peretti.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}},{"@context":"https:\/\/schema.org\/","@type":"Person","name":"Antonella Marchetti","url":"https:\/\/www.researchgate.net\/profile\/Antonella_Marchetti","image":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/326859662348290-1454940824962_Q64\/Antonella_Marchetti.jpg","memberOf":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"Catholic University of the Sacred Heart"}}],"publisher":{"@context":"https:\/\/schema.org\/","@type":"Organization","name":"unknown","url":"unknown","logo":"unknown"}}]}</script>
<script src="https://s.ntv.io/serve/load.js" async></script>
<script src="https://www.googletagservices.com/tag/js/gpt.js" async></script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">window.googletag=window.googletag||{cmd:[]}</script>
    <!-- TradeDoubler site verification 2966599 -->
<style>.nova-e-icon{width:1em;height:1em;vertical-align:-.1875rem;display:inline-block;overflow:visible}.nova-e-icon{position:relative;fill:currentColor}.nova-e-icon--size-s{width:1rem;height:1rem}.nova-e-icon--size-m{width:1.5rem;height:1.5rem}.nova-e-icon--color-green.nova-e-icon--theme-bare.nova-e-icon--luminosity-medium{color:#00a39e}.nova-e-icon--color-green.nova-e-icon--theme-bare.nova-e-icon--luminosity-low{color:#00141a}.nova-e-icon--color-grey.nova-e-icon--theme-bare.nova-e-icon--luminosity-medium{color:#888}.nova-e-icon--color-grey.nova-e-icon--theme-bare.nova-e-icon--luminosity-low{color:#333}.nova-e-icon--color-white.nova-e-icon--theme-bare.nova-e-icon--luminosity-low,.nova-e-icon--color-white.nova-e-icon--theme-bare.nova-e-icon--luminosity-medium{color:#fff}.nova-e-text{margin-top:0;color:inherit;font-weight:400;font-family:Roboto,Arial,sans-serif;font-size:.875rem;line-height:1.3;margin-bottom:1.42857em}@media screen and (min-width:0px){.nova-e-text--size-s{font-size:.75rem}.nova-e-text--size-l,.nova-e-text--size-m{font-size:.875rem}.nova-e-text--size-xl{font-size:1.1875rem}.nova-e-text--size-xxl{font-size:1.375rem}.nova-e-text--size-xxxl{font-size:1.5rem}}@media screen and (min-width:992px){.nova-e-text--size-s{font-size:.75rem}.nova-e-text--size-m{font-size:.875rem}.nova-e-text--size-l{font-size:1rem}.nova-e-text--size-xl{font-size:1.375rem}.nova-e-text--size-xxl{font-size:1.75rem}.nova-e-text--size-xxxl{font-size:2rem}}.nova-e-text--clamp{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.nova-e-text[class*=nova-e-text--clamp-]{overflow:hidden;display:block;display:-webkit-box;text-overflow:ellipsis;text-overflow:-o-ellipsis-lastline;-webkit-box-orient:vertical;text-align:left}@-moz-document url-prefix(""){.nova-e-text[class*=nova-e-text--clamp-]{padding-right:1em;position:relative}.nova-e-text[class*=nova-e-text--clamp-]:before{position:absolute;right:0;bottom:0}.nova-e-text[class*=nova-e-text--clamp-]:after{position:absolute;right:0;width:1em;background:#fff}}.nova-e-text--size-s{margin-bottom:1.25em}.nova-e-text--size-s.nova-e-text--clamp-2{max-height:2.6em;-webkit-line-clamp:2}@-moz-document url-prefix(""){.nova-e-text--size-s.nova-e-text--clamp-2:before{content:"\2026"}.nova-e-text--size-s.nova-e-text--clamp-2:after{content:"";height:1.3em}}.nova-e-text--size-s.nova-e-text--clamp-3{max-height:3.9em;-webkit-line-clamp:3}@-moz-document url-prefix(""){.nova-e-text--size-s.nova-e-text--clamp-3:before{content:"\2026"}.nova-e-text--size-s.nova-e-text--clamp-3:after{content:"";height:1.3em}}.nova-e-text--size-m.nova-e-text--clamp-2{max-height:2.6em;-webkit-line-clamp:2}@-moz-document url-prefix(""){.nova-e-text--size-m.nova-e-text--clamp-2:before{content:"\2026"}.nova-e-text--size-m.nova-e-text--clamp-2:after{content:"";height:1.3em}}.nova-e-text--size-m.nova-e-text--clamp-3{max-height:3.9em;-webkit-line-clamp:3}@-moz-document url-prefix(""){.nova-e-text--size-m.nova-e-text--clamp-3:before{content:"\2026"}.nova-e-text--size-m.nova-e-text--clamp-3:after{content:"";height:1.3em}}.nova-e-text--size-l{margin-bottom:1.25em}.nova-e-text--size-l.nova-e-text--clamp-2{max-height:2.6em;-webkit-line-clamp:2}@-moz-document url-prefix(""){.nova-e-text--size-l.nova-e-text--clamp-2:before{content:"\2026"}.nova-e-text--size-l.nova-e-text--clamp-2:after{content:"";height:1.3em}}.nova-e-text--size-l.nova-e-text--clamp-3{max-height:3.9em;-webkit-line-clamp:3}@-moz-document url-prefix(""){.nova-e-text--size-l.nova-e-text--clamp-3:before{content:"\2026"}.nova-e-text--size-l.nova-e-text--clamp-3:after{content:"";height:1.3em}}.nova-e-text--size-xl{line-height:1.2;margin-bottom:.90909em}.nova-e-text--size-xl.nova-e-text--clamp-2{max-height:2.4em;-webkit-line-clamp:2}@-moz-document url-prefix(""){.nova-e-text--size-xl.nova-e-text--clamp-2:before{content:"\2026"}.nova-e-text--size-xl.nova-e-text--clamp-2:after{content:"";height:1.2em}}.nova-e-text--size-xl.nova-e-text--clamp-3{max-height:3.6em;-webkit-line-clamp:3}@-moz-document url-prefix(""){.nova-e-text--size-xl.nova-e-text--clamp-3:before{content:"\2026"}.nova-e-text--size-xl.nova-e-text--clamp-3:after{content:"";height:1.2em}}.nova-e-text--size-xxl{line-height:1.2;margin-bottom:.89286em}.nova-e-text--size-xxl.nova-e-text--clamp-2{max-height:2.4em;-webkit-line-clamp:2}@-moz-document url-prefix(""){.nova-e-text--size-xxl.nova-e-text--clamp-2:before{content:"\2026"}.nova-e-text--size-xxl.nova-e-text--clamp-2:after{content:"";height:1.2em}}.nova-e-text--size-xxl.nova-e-text--clamp-3{max-height:3.6em;-webkit-line-clamp:3}@-moz-document url-prefix(""){.nova-e-text--size-xxl.nova-e-text--clamp-3:before{content:"\2026"}.nova-e-text--size-xxl.nova-e-text--clamp-3:after{content:"";height:1.2em}}.nova-e-text--size-xxxl{line-height:1.2;margin-bottom:.78125em;font-weight:300}.nova-e-text--size-xxxl.nova-e-text--clamp-2{max-height:2.4em;-webkit-line-clamp:2}@-moz-document url-prefix(""){.nova-e-text--size-xxxl.nova-e-text--clamp-2:before{content:"\2026"}.nova-e-text--size-xxxl.nova-e-text--clamp-2:after{content:"";height:1.2em}}.nova-e-text--size-xxxl.nova-e-text--clamp-3{max-height:3.6em;-webkit-line-clamp:3}@-moz-document url-prefix(""){.nova-e-text--size-xxxl.nova-e-text--clamp-3:before{content:"\2026"}.nova-e-text--size-xxxl.nova-e-text--clamp-3:after{content:"";height:1.2em}}.nova-e-text--spacing-none{margin-bottom:0}.nova-e-text--spacing-xxs{margin-bottom:5px}.nova-e-text--spacing-xs{margin-bottom:10px}.nova-e-text--spacing-s{margin-bottom:15px}.nova-e-text--spacing-m{margin-bottom:20px}.nova-e-text--spacing-l{margin-bottom:25px}.nova-e-text--color-white,.nova-e-text--color-white:hover{color:#fff}.nova-e-text--color-grey-100,.nova-e-text--color-grey-100:hover{color:#f4f4f4}.nova-e-text--color-grey-500,.nova-e-text--color-grey-500:hover{color:#888}.nova-e-text--color-grey-700,.nova-e-text--color-grey-700:hover{color:#555}.nova-e-text--color-grey-800,.nova-e-text--color-grey-800:hover{color:#333}.nova-e-text--color-grey-900,.nova-e-text--color-grey-900:hover{color:#111}.nova-e-text--color-inherit{color:inherit}.nova-c-button-group{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-collapse:separate}.nova-c-button-group--gutter-m{margin-left:-20px;margin-bottom:-20px}.nova-c-button-group--gutter-m>.nova-c-button-group__item{margin-left:20px;margin-bottom:20px}.nova-c-button-group__item{-ms-flex-item-align:center;align-self:center;position:relative}.nova-c-button-group__item:empty{display:none!important}.nova-c-button-group__item:hover{z-index:1}@-webkit-keyframes fadeFocus{to{-webkit-box-shadow:0 0 0 1px transparent;box-shadow:0 0 0 1px transparent}}@keyframes fadeFocus{to{-webkit-box-shadow:0 0 0 1px transparent;box-shadow:0 0 0 1px transparent}}@-webkit-keyframes fadeFocusBare{to{-webkit-box-shadow:0 2px 0 0 transparent;box-shadow:0 2px 0 0 transparent;background:transparent}}@keyframes fadeFocusBare{to{-webkit-box-shadow:0 2px 0 0 transparent;box-shadow:0 2px 0 0 transparent;background:transparent}}button::-moz-focus-inner,input[type=button]::-moz-focus-inner{border:0;padding:0;margin-top:-.4px;margin-bottom:-.4px}.nova-c-button{-webkit-box-sizing:border-box;box-sizing:border-box;font-family:Roboto,Arial,sans-serif;border:1px solid transparent;outline:none;background-color:transparent;cursor:pointer;text-decoration:none;position:relative;text-align:center;line-height:1;overflow:hidden;margin:0;vertical-align:top;display:inline-block;white-space:nowrap;-webkit-transition:.1s linear;transition:.1s linear}.nova-c-button *,.nova-c-button :after,.nova-c-button :before{-webkit-box-sizing:inherit;box-sizing:inherit}.nova-c-button__icon{font-style:normal;vertical-align:middle;pointer-events:none;display:inline-block;text-align:center;fill:currentColor;-webkit-transition:opacity .05s linear,-webkit-transform .05s linear;transition:opacity .05s linear,-webkit-transform .05s linear;transition:transform .05s linear,opacity .05s linear;transition:transform .05s linear,opacity .05s linear,-webkit-transform .05s linear;min-height:1.3em;position:relative}.nova-c-button__icon+.nova-c-button__label{margin-left:.6em}.nova-c-button__label{-webkit-transition:opacity .05s linear,-webkit-transform .05s linear;transition:opacity .05s linear,-webkit-transform .05s linear;transition:transform .05s linear,opacity .05s linear;transition:transform .05s linear,opacity .05s linear,-webkit-transform .05s linear;display:inline-block;vertical-align:middle;position:relative}.nova-c-button__label+.nova-c-button__icon{margin-left:.6em}.nova-c-button--align-left{text-align:left}.nova-c-button--align-center{text-align:center}.nova-c-button--radius-m{border-radius:2px}.nova-c-button--radius-full{border-radius:9999px}.nova-c-button--size-xs{font-size:.75rem;padding:.41667em .5em}.nova-c-button--size-xs .nova-c-button__label{line-height:1.33333}.nova-c-button--size-xs .nova-c-button__icon{min-height:1.33333em}.nova-c-button--size-s{font-size:.875rem;padding:.5em 1em}.nova-c-button--size-s .nova-c-button__label{line-height:1.28571}.nova-c-button--size-s .nova-c-button__icon{min-height:1.28571em}.nova-c-button--size-m{font-size:.875rem;padding:.78571em 2em}.nova-c-button--size-m .nova-c-button__label{line-height:1.28571}.nova-c-button--size-m .nova-c-button__icon{min-height:1.28571em}.nova-c-button--size-xs .nova-c-button__icon{width:1em;height:1em}.nova-c-button:disabled,.nova-c-button[disabled]{cursor:default;pointer-events:none}.nova-c-button:disabled .nova-c-button__label,.nova-c-button:disabled:hover,.nova-c-button[disabled] .nova-c-button__label,.nova-c-button[disabled]:hover{text-decoration:none}.nova-c-button--color-blue.nova-c-button--theme-solid{background-color:#0080ff;border-color:#0080ff;color:#fff}@supports (display:block){.nova-c-button--color-blue.nova-c-button--theme-solid:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,128,255,.4);box-shadow:0 0 0 3px rgba(0,128,255,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-blue.nova-c-button--theme-solid:active,.nova-c-button--color-blue.nova-c-button--theme-solid:hover{background-color:#034aa6;border-color:#034aa6}.nova-c-button--color-blue.nova-c-button--theme-solid[disabled]{background-color:#bbb;border-color:#bbb}.nova-c-button--color-blue.nova-c-button--theme-solid.is-selected{background-color:#034aa6;border-color:#034aa6}@supports (display:block){.nova-c-button--color-blue.nova-c-button--theme-solid.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(3,74,166,.4);box-shadow:0 0 0 3px rgba(3,74,166,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-blue.nova-c-button--theme-ghost{border-color:#0080ff;color:#0080ff}@supports (display:block){.nova-c-button--color-blue.nova-c-button--theme-ghost:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,128,255,.4);box-shadow:0 0 0 3px rgba(0,128,255,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-blue.nova-c-button--theme-ghost:active,.nova-c-button--color-blue.nova-c-button--theme-ghost:hover{background-color:#0080ff;border-color:#0080ff;color:#fff}.nova-c-button--color-blue.nova-c-button--theme-ghost[disabled]{border-color:#bbb;color:#bbb;background-color:transparent}.nova-c-button--color-blue.nova-c-button--theme-ghost.is-selected{background-color:#0080ff;border-color:#0080ff;color:#fff}@supports (display:block){.nova-c-button--color-blue.nova-c-button--theme-ghost.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,128,255,.4);box-shadow:0 0 0 3px rgba(0,128,255,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-blue.nova-c-button--theme-bare{color:#0080ff}@supports (display:block){.nova-c-button--color-blue.nova-c-button--theme-bare:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,128,255,.1);box-shadow:0 0 0 3px rgba(0,128,255,.1);background:rgba(0,128,255,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-blue.nova-c-button--theme-bare:active,.nova-c-button--color-blue.nova-c-button--theme-bare:hover{color:#034aa6}.nova-c-button--color-blue.nova-c-button--theme-bare:active .nova-c-button__label,.nova-c-button--color-blue.nova-c-button--theme-bare:hover .nova-c-button__label{text-decoration:underline}.nova-c-button--color-blue.nova-c-button--theme-bare[disabled]{color:#bbb}.nova-c-button--color-blue.nova-c-button--theme-bare[disabled] .nova-c-button__label{text-decoration:none}.nova-c-button--color-blue.nova-c-button--theme-bare.is-selected{color:#034aa6}@supports (display:block){.nova-c-button--color-blue.nova-c-button--theme-bare.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(3,74,166,.1);box-shadow:0 0 0 3px rgba(3,74,166,.1);background:rgba(3,74,166,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-blue.nova-c-button--theme-bare.is-selected .nova-c-button__label{text-decoration:underline}.nova-c-button--color-grey.nova-c-button--theme-solid{background-color:#888;border-color:#888;color:#fff}@supports (display:block){.nova-c-button--color-grey.nova-c-button--theme-solid:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,53.3%,.4);box-shadow:0 0 0 3px hsla(0,0%,53.3%,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-grey.nova-c-button--theme-solid:active,.nova-c-button--color-grey.nova-c-button--theme-solid:hover{background-color:#555;border-color:#555}.nova-c-button--color-grey.nova-c-button--theme-solid[disabled]{background-color:#bbb;border-color:#bbb}.nova-c-button--color-grey.nova-c-button--theme-solid.is-selected{background-color:#555;border-color:#555}@supports (display:block){.nova-c-button--color-grey.nova-c-button--theme-solid.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(85,85,85,.4);box-shadow:0 0 0 3px rgba(85,85,85,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-grey.nova-c-button--theme-ghost{border-color:#888;color:#888}@supports (display:block){.nova-c-button--color-grey.nova-c-button--theme-ghost:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,53.3%,.4);box-shadow:0 0 0 3px hsla(0,0%,53.3%,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-grey.nova-c-button--theme-ghost:active,.nova-c-button--color-grey.nova-c-button--theme-ghost:hover{background-color:#888;border-color:#888;color:#fff}.nova-c-button--color-grey.nova-c-button--theme-ghost[disabled]{border-color:#bbb;color:#bbb;background-color:transparent}.nova-c-button--color-grey.nova-c-button--theme-ghost.is-selected{background-color:#888;border-color:#888;color:#fff}@supports (display:block){.nova-c-button--color-grey.nova-c-button--theme-ghost.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,53.3%,.4);box-shadow:0 0 0 3px hsla(0,0%,53.3%,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-grey.nova-c-button--theme-bare{color:#888}@supports (display:block){.nova-c-button--color-grey.nova-c-button--theme-bare:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,53.3%,.1);box-shadow:0 0 0 3px hsla(0,0%,53.3%,.1);background:hsla(0,0%,53.3%,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-grey.nova-c-button--theme-bare:active,.nova-c-button--color-grey.nova-c-button--theme-bare:hover{color:#555}.nova-c-button--color-grey.nova-c-button--theme-bare:active .nova-c-button__label,.nova-c-button--color-grey.nova-c-button--theme-bare:hover .nova-c-button__label{text-decoration:underline}.nova-c-button--color-grey.nova-c-button--theme-bare[disabled]{color:#bbb}.nova-c-button--color-grey.nova-c-button--theme-bare[disabled] .nova-c-button__label{text-decoration:none}.nova-c-button--color-grey.nova-c-button--theme-bare.is-selected{color:#555}@supports (display:block){.nova-c-button--color-grey.nova-c-button--theme-bare.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(85,85,85,.1);box-shadow:0 0 0 3px rgba(85,85,85,.1);background:rgba(85,85,85,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-grey.nova-c-button--theme-bare.is-selected .nova-c-button__label{text-decoration:underline}.nova-c-button--color-green.nova-c-button--theme-solid{background-color:#00a39e;border-color:#00a39e;color:#fff}@supports (display:block){.nova-c-button--color-green.nova-c-button--theme-solid:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,163,158,.4);box-shadow:0 0 0 3px rgba(0,163,158,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-green.nova-c-button--theme-solid:active,.nova-c-button--color-green.nova-c-button--theme-solid:hover{background-color:#00444d;border-color:#00444d}.nova-c-button--color-green.nova-c-button--theme-solid[disabled]{background-color:#bbb;border-color:#bbb}.nova-c-button--color-green.nova-c-button--theme-solid.is-selected{background-color:#00444d;border-color:#00444d}@supports (display:block){.nova-c-button--color-green.nova-c-button--theme-solid.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,68,77,.4);box-shadow:0 0 0 3px rgba(0,68,77,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-green.nova-c-button--theme-ghost{border-color:#00a39e;color:#00a39e}@supports (display:block){.nova-c-button--color-green.nova-c-button--theme-ghost:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,163,158,.4);box-shadow:0 0 0 3px rgba(0,163,158,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-green.nova-c-button--theme-ghost:active,.nova-c-button--color-green.nova-c-button--theme-ghost:hover{background-color:#00a39e;border-color:#00a39e;color:#fff}.nova-c-button--color-green.nova-c-button--theme-ghost[disabled]{border-color:#bbb;color:#bbb;background-color:transparent}.nova-c-button--color-green.nova-c-button--theme-ghost.is-selected{background-color:#00a39e;border-color:#00a39e;color:#fff}@supports (display:block){.nova-c-button--color-green.nova-c-button--theme-ghost.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,163,158,.4);box-shadow:0 0 0 3px rgba(0,163,158,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-green.nova-c-button--theme-bare{color:#00a39e}@supports (display:block){.nova-c-button--color-green.nova-c-button--theme-bare:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,163,158,.1);box-shadow:0 0 0 3px rgba(0,163,158,.1);background:rgba(0,163,158,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-green.nova-c-button--theme-bare:active,.nova-c-button--color-green.nova-c-button--theme-bare:hover{color:#00444d}.nova-c-button--color-green.nova-c-button--theme-bare:active .nova-c-button__label,.nova-c-button--color-green.nova-c-button--theme-bare:hover .nova-c-button__label{text-decoration:underline}.nova-c-button--color-green.nova-c-button--theme-bare[disabled]{color:#bbb}.nova-c-button--color-green.nova-c-button--theme-bare[disabled] .nova-c-button__label{text-decoration:none}.nova-c-button--color-green.nova-c-button--theme-bare.is-selected{color:#00444d}@supports (display:block){.nova-c-button--color-green.nova-c-button--theme-bare.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px rgba(0,68,77,.1);box-shadow:0 0 0 3px rgba(0,68,77,.1);background:rgba(0,68,77,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-green.nova-c-button--theme-bare.is-selected .nova-c-button__label{text-decoration:underline}.nova-c-button--color-white.nova-c-button--theme-solid{background-color:#fff;border-color:#fff;color:#111}@supports (display:block){.nova-c-button--color-white.nova-c-button--theme-solid:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,100%,.4);box-shadow:0 0 0 3px hsla(0,0%,100%,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-white.nova-c-button--theme-solid:active,.nova-c-button--color-white.nova-c-button--theme-solid:hover{background-color:#fff;border-color:#fff}.nova-c-button--color-white.nova-c-button--theme-solid[disabled]{background-color:#bbb;border-color:#bbb}.nova-c-button--color-white.nova-c-button--theme-solid.is-selected{background-color:#fff;border-color:#fff}@supports (display:block){.nova-c-button--color-white.nova-c-button--theme-solid.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,100%,.4);box-shadow:0 0 0 3px hsla(0,0%,100%,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-white.nova-c-button--theme-ghost{border-color:#fff;color:#fff}@supports (display:block){.nova-c-button--color-white.nova-c-button--theme-ghost:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,100%,.4);box-shadow:0 0 0 3px hsla(0,0%,100%,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-white.nova-c-button--theme-ghost:active,.nova-c-button--color-white.nova-c-button--theme-ghost:hover{background-color:#fff;border-color:#fff;color:#111}.nova-c-button--color-white.nova-c-button--theme-ghost[disabled]{border-color:#bbb;color:#bbb;background-color:transparent}.nova-c-button--color-white.nova-c-button--theme-ghost.is-selected{background-color:#fff;border-color:#fff;color:#fff}@supports (display:block){.nova-c-button--color-white.nova-c-button--theme-ghost.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,100%,.4);box-shadow:0 0 0 3px hsla(0,0%,100%,.4);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocus;animation-name:fadeFocus;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-white.nova-c-button--theme-bare{color:#fff}@supports (display:block){.nova-c-button--color-white.nova-c-button--theme-bare:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,100%,.1);box-shadow:0 0 0 3px hsla(0,0%,100%,.1);background:hsla(0,0%,100%,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-white.nova-c-button--theme-bare:active,.nova-c-button--color-white.nova-c-button--theme-bare:hover{color:#fff}.nova-c-button--color-white.nova-c-button--theme-bare:active .nova-c-button__label,.nova-c-button--color-white.nova-c-button--theme-bare:hover .nova-c-button__label{text-decoration:underline}.nova-c-button--color-white.nova-c-button--theme-bare[disabled]{color:#bbb}.nova-c-button--color-white.nova-c-button--theme-bare[disabled] .nova-c-button__label{text-decoration:none}.nova-c-button--color-white.nova-c-button--theme-bare.is-selected{color:#fff}@supports (display:block){.nova-c-button--color-white.nova-c-button--theme-bare.is-selected:focus-visible{-webkit-box-shadow:0 0 0 3px hsla(0,0%,100%,.1);box-shadow:0 0 0 3px hsla(0,0%,100%,.1);background:hsla(0,0%,100%,.11);-webkit-transition:none;transition:none;-webkit-animation-name:fadeFocusBare;animation-name:fadeFocusBare;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-timing-function:cubic-bezier(.25,.46,.45,.94);animation-timing-function:cubic-bezier(.25,.46,.45,.94);-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-delay:.6s;animation-delay:.6s}}.nova-c-button--color-white.nova-c-button--theme-bare.is-selected .nova-c-button__label{text-decoration:underline}.nova-c-button--theme-solid:hover .nova-c-button__label{text-decoration:none}.nova-c-button--theme-bare{padding:0;border-radius:0}.nova-c-button--theme-bare .nova-c-button__label{margin-top:0;margin-bottom:0}.nova-c-button--theme-bare .nova-c-button__icon{min-height:0}.nova-c-button--width-square{padding-left:0;padding-right:0;-webkit-box-sizing:content-box;box-sizing:content-box}.nova-c-button--width-square.nova-c-button--size-xs{width:2.16667em}.nova-c-button--width-square.nova-c-button--size-s{width:2.28571em}.nova-c-button--width-square.nova-c-button--size-m{width:2.85714em}.nova-c-button--width-square.nova-c-button--theme-bare{width:auto}.nova-c-button--width-full{width:100%}@-webkit-keyframes nova--spinner-rotate{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(-1turn);transform:rotate(-1turn)}}@keyframes nova--spinner-rotate{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(-1turn);transform:rotate(-1turn)}}@-webkit-keyframes nova--spinner-stroke{0%{stroke-dashoffset:44}50%{stroke-dashoffset:0}to{stroke-dashoffset:-44}}@keyframes nova--spinner-stroke{0%{stroke-dashoffset:44}50%{stroke-dashoffset:0}to{stroke-dashoffset:-44}}.nova-e-spinner{display:inline-block;width:1.1428571429em;height:1.1428571429em;vertical-align:top;line-height:1}.nova-e-spinner--size-m{width:1.5rem;height:1.5rem}.nova-e-spinner__asset{display:block;width:100%;height:100%;fill:none}.nova-e-spinner__circle{stroke:currentColor;opacity:.4}.nova-e-spinner__stroke{stroke-dasharray:44;stroke-dashoffset:33;-webkit-animation:nova--spinner-stroke 2s cubic-bezier(.455,.03,.515,.955) infinite both,nova--spinner-rotate 1s linear infinite both;animation:nova--spinner-stroke 2s cubic-bezier(.455,.03,.515,.955) infinite both,nova--spinner-rotate 1s linear infinite both;-webkit-transform-origin:center;transform-origin:center;stroke:currentColor}.nova-e-list{padding-left:0;list-style-position:outside;font-weight:400;color:inherit;font-family:Roboto,Arial,sans-serif;font-size:.875rem;margin:0 0 1.42857em 1.2em;line-height:1}.nova-e-list__item{line-height:1.3}.nova-e-list__item:empty{display:none!important}.nova-e-list__item:not(:last-child){margin-bottom:.5em}@media screen and (min-width:0px){.nova-e-list--size-s{font-size:.75rem}.nova-e-list--size-m{font-size:.875rem}}@media screen and (min-width:992px){.nova-e-list--size-s{font-size:.75rem}.nova-e-list--size-m{font-size:.875rem}}.nova-e-list--type-unordered,ul.nova-e-list{list-style-type:disc}ol.nova-e-list{list-style-type:decimal}.nova-e-list--type-bare,.nova-e-list--type-inline,ol.nova-e-list--type-bare,ol.nova-e-list--type-inline,ul.nova-e-list--type-bare,ul.nova-e-list--type-inline{list-style:none;margin-left:0}.nova-e-list--type-inline>.nova-e-list__item,ol.nova-e-list--type-inline>.nova-e-list__item,ul.nova-e-list--type-inline>.nova-e-list__item{display:inline}.nova-e-list--type-inline>.nova-e-list__item:not(:empty)~.nova-e-list__item:not(:empty):before,ol.nova-e-list--type-inline>.nova-e-list__item:not(:empty)~.nova-e-list__item:not(:empty):before,ul.nova-e-list--type-inline>.nova-e-list__item:not(:empty)~.nova-e-list__item:not(:empty):before{content:"\00A0\00B7\0020";vertical-align:middle}.nova-e-list--size-s{margin-bottom:1.25em}.nova-e-list--spacing-none{margin-bottom:0}.nova-e-list--spacing-s{margin-bottom:15px}.lite-page-ad-fallback{vertical-align:middle;padding:20px;border:1px solid #ddd}.lite-page-ad--has-label.lite-page-ad--rendered .rg-ad>div{position:relative}.lite-page-ad--has-label.lite-page-ad--rendered .rg-ad>div:before{position:absolute;text-align:center;top:-12px;width:100%;left:0;content:"Advertisement";color:#777;font-size:.6875rem;line-height:.6875rem}.lite-page-ad__inner{position:relative}.lite-page-ad__close{display:none}.lite-page-ad__fallback{text-align:left;display:none}.lite-page-ad--vertical-spacing-xs{margin-top:10px;margin-bottom:10px}.lite-page-ad--vertical-spacing-xl{margin-top:30px;margin-bottom:30px}.lite-page-ad--sticky{position:-webkit-sticky;position:sticky;top:25px}.lite-page-ad--has-label .lite-page-ad__inner{padding-top:10px}.lite-page-ad--layout-break-sidebar{overflow:visible;position:relative;height:0}.lite-page-ad--layout-break-sidebar .lite-page-ad__inner{position:absolute;right:-335px}.lite-page-ad__header-close{position:absolute;top:10px;right:10px;z-index:2}@media (max-width:819px){.lite-page-ad__header-close{display:none}}.lite-page-ad--layout-header{margin:0;background-color:inherit}@media (min-width:820px){.lite-page-ad--layout-header{height:122px}}.lite-page-ad--layout-header .lite-page-ad__inner{padding:20px 0 10px;background-color:inherit}@media (min-width:820px){.lite-page-ad--layout-header .lite-page-ad__inner{min-height:91px}.lite-page-ad--layout-header .is-sticky{position:fixed;width:100%;z-index:91;top:0;left:0;border-bottom:1px solid #ddd}}.lite-page-ad--empty{padding:0!important;margin:0!important}.lite-page-ad--empty .lite-page-ad__fallback{display:block}.lite-page-ad--empty .lite-page-ad__inner{padding:0!important;margin:0!important}@media (min-width:820px){.lite-page-ad--empty .lite-page-ad__inner{min-height:122px}}.lite-page-ad--empty .nova-c-card{box-shadow:none!important;background:transparent!important}.lite-page-ad--empty .nova-c-card,.lite-page-ad--empty .nova-c-card__body,.lite-page-ad--empty .nova-c-card__header{padding:0!important;margin:0!important}.lite-page-ad--catfish,.lite-page-ad--empty .nova-c-card__header{display:none}@media screen and (max-width:767px){.lite-page-ad--catfish{display:block;position:fixed;top:auto;bottom:0;left:0;right:0;z-index:1}}.lite-page--wide .lite-page-ad--layout-break-sidebar .lite-page-ad__inner{right:-331px}.lite-page-ad--catfish.lite-page-ad--rendered .lite-page-ad__inner{padding-top:0;width:100%;background-color:#fff;border-top:1px solid #ddd}.lite-page-ad--catfish.lite-page-ad--rendered .lite-page-ad__close{z-index:2;position:absolute;top:-10px;right:0;display:block}.publication .lite-page-ad--sticky{top:190px}.publication .lite-page-ad--layout-header:not(.lite-page-ad--empty){min-height:80px}@media (min-width:820px){.publication .lite-page-ad--layout-header:not(.lite-page-ad--empty){min-height:120px}}@media (min-width:820px){.publication .lite-page-ad .is-sticky{position:fixed;top:0;left:0;background-color:#fff}}.publication.lite-page--header-ad-collapsed .lite-page__main{padding-top:20px}.publication.lite-page--header-ad-collapsed .lite-page-ad--sticky{top:70px}@media screen and (max-width:767px){.publication .lite-page-ad--catfish{z-index:90}}.nova-c-card{-webkit-box-shadow:0 0 2px rgba(0,0,0,.18),0 1px 3px rgba(0,0,0,.12);box-shadow:0 0 2px rgba(0,0,0,.18),0 1px 3px rgba(0,0,0,.12);-webkit-box-sizing:border-box;box-sizing:border-box;background:#fff;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;max-width:100%;width:100%}@supports (display:flex){.nova-c-card{width:auto}}.nova-c-card--elevation-none{-webkit-box-shadow:0 0 0 0 transparent;box-shadow:0 0 0 0 transparent;border:1px solid #ddd;border-radius:.125rem}@media screen and (min-width:0px){.nova-c-card--spacing-xs>.nova-c-card__header{padding:20px 10px}.nova-c-card--spacing-xs>.nova-c-card__body{padding:10px}}@media screen and (min-width:0px){.nova-c-card--spacing-m>.nova-c-card__header{padding:20px 15px}.nova-c-card--spacing-m>.nova-c-card__body{padding:15px}}@media screen and (min-width:992px){.nova-c-card--spacing-m>.nova-c-card__body,.nova-c-card--spacing-m>.nova-c-card__header{padding:20px}}.nova-c-card__header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;border-bottom:1px solid #ddd}.nova-c-card__body{padding:30px;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.lite-page__header-search{height:62px}.lite-page__header-search-input-wrapper{position:relative}.lite-page__header-search-input{margin:0;transition:none;border:0;width:390px;max-width:500px;height:34px;line-height:26px;border-radius:.125rem;background-color:#a0ded9;position:relative;padding:0 30px 0 10px;font-size:.875rem}.lite-page__header-search-input::-moz-placeholder{color:#00a39e}.lite-page__header-search-input:-ms-input-placeholder{color:#00a39e}.lite-page__header-search-input::-ms-input-placeholder{color:#00a39e}.lite-page__header-search-input::placeholder{color:#00a39e}.lite-page__header-search-input:focus{background-color:#fff}.lite-page__header-search-or{vertical-align:text-bottom}.lite-page__header-search .lite-page__header-search-button{position:absolute;height:100%;width:30px!important;right:0;top:0}.lite-page__header-search :-ms-input-placeholder,.lite-page__header-search ::-webkit-input-placeholder{color:#ddf0ee}@media (max-width:1199px){.lite-page__header-search-input{width:46px}}@media (max-width:991px){.lite-page .lite-page__header-search>.item{display:none}.lite-page .lite-page__header-search>.item:first-child{display:table-cell}.lite-page .lite-page__header-search .lite-page__header-search-button{position:relative;background-color:#00a39e;color:#fff;height:30px}.lite-page .lite-page__header-search .lite-page__header-search-button:hover{background-color:#00444d;color:#fff}.lite-page__header-search{height:40px}.lite-page__header-search-input{display:none}}.nova-o-pack{display:table;width:100%;table-layout:fixed;border-spacing:0}.nova-o-pack>.nova-o-pack__item{display:table-cell;vertical-align:top}.nova-o-pack>.nova-o-pack__item:empty{display:none!important}.nova-o-pack--gutter-m{margin-left:-10px;margin-right:-10px}.nova-o-pack--gutter-m>.nova-o-pack__item{padding-left:10px;padding-right:10px}.nova-o-pack--gutter-l{margin-left:-12.5px;margin-right:-12.5px}.nova-o-pack--gutter-l>.nova-o-pack__item{padding-left:12.5px;padding-right:12.5px}.nova-o-pack--gutter-xl{margin-left:-15px;margin-right:-15px}.nova-o-pack--gutter-xl>.nova-o-pack__item{padding-left:15px;padding-right:15px}.nova-o-pack--width-auto{width:auto}.nova-o-pack--vertical-align-middle>.nova-o-pack__item{vertical-align:middle}.lite-page-tooltip{display:inline-block;position:relative}.lite-page-tooltip:hover .lite-page-tooltip__content{display:inline-block;z-index:7000}.lite-page-tooltip__content{box-shadow:0 1px 6px rgba(0,0,0,.13),0 3px 6px rgba(0,0,0,.09);display:none;position:absolute;background:#fff;padding:10px;min-width:250px}.lite-page-tooltip__content--above{top:-100%;transform:translate(-50%,-100%);left:50%}.lite-page-tooltip__arrow{position:absolute;overflow:hidden;width:3em;height:1.5em}.lite-page-tooltip__arrow--above{transform:translate(-50%,10px);left:50%}.lite-page-tooltip__arrow-tip{background:#fff;width:1.5em;height:1.5em;transform:translate(-50%,-50%) rotate(45deg);left:50%}.lite-page-tooltip__arrow-tip,.lite-page__trusted-device-promo{box-shadow:0 1px 6px rgba(0,0,0,.13),0 3px 6px rgba(0,0,0,.09);position:absolute}.lite-page__trusted-device-promo{top:120%;right:25px;min-width:320px;z-index:1000;background-color:#fff;padding:20px;text-align:left}.lite-page__trusted-device-promo:after,.lite-page__trusted-device-promo:before{content:"";position:absolute;right:18px;top:-12px;display:block;width:0;height:0;border-left:12px solid transparent;border-right:12px solid transparent;border-bottom:12px solid #f4f4f4}.lite-page__trusted-device-promo:after{z-index:1001;right:20px;top:-10px;border-width:0 10px 10px;border-bottom-color:#fff}.lite-page__trusted-device-promo-counter{position:absolute;top:20px;left:55px;background-color:#f50}.lite-page__trusted-device-promo-avatar{padding-right:20px}@media (max-width:991px){.lite-page__trusted-device-promo{right:10px}}@media screen and (max-width:399px){.lite-page__trusted-device-promo{max-width:250px;min-width:auto}}.nova-e-badge{outline:none;border:0;font-size:.875em;font-family:Roboto,Arial,sans-serif;font-weight:400;line-height:1.3;min-height:1em;color:#fff;display:inline-block;padding:.16em .5714em;margin:-.16em 0;border-radius:624.9375rem;white-space:nowrap;min-width:1em;text-align:center;text-decoration:none;vertical-align:.125rem}a.nova-e-badge:hover,button.nova-e-badge:hover{text-decoration:underline;cursor:pointer}.nova-e-badge--display-block{display:block}.nova-e-badge--color-green.nova-e-badge--theme-solid.nova-e-badge--luminosity-medium{background-color:#00a39e;color:#fff}.nova-e-badge--color-green.nova-e-badge--theme-solid.nova-e-badge--luminosity-high{background-color:#c5e8e5;color:#007478}.nova-e-badge--color-green.nova-e-badge--theme-ghost.nova-e-badge--luminosity-medium{background-color:transparent;color:#00a39e;-webkit-box-shadow:inset 0 0 0 1px #00a39e;box-shadow:inset 0 0 0 1px #00a39e}.nova-e-badge--color-green.nova-e-badge--theme-ghost.nova-e-badge--luminosity-high{background-color:transparent;color:#a0ded9;-webkit-box-shadow:inset 0 0 0 1px #a0ded9;box-shadow:inset 0 0 0 1px #a0ded9}.nova-e-badge--radius-m{border-radius:2px}.nova-e-badge--size-l.nova-e-badge--theme-ghost,.nova-e-badge--size-l.nova-e-badge--theme-solid{font-size:.875rem;padding-top:.286em;padding-bottom:.286em;margin-top:-.286em;margin-bottom:-.286em}.nova-e-badge--size-l.nova-e-badge--display-block{margin-top:0;margin-bottom:0}.lite-page-modal{position:relative;z-index:7001}.lite-page-modal--hidden{display:none}.lite-page-modal__close{margin:10px}.lite-page-modal__close--top-right{position:absolute;top:0;right:0}.lite-page-modal__login{text-align:left}.lite-page-modal__forgot{position:relative}.lite-page-modal__forgot-link{position:absolute;right:0}.lite-page .lite-page-modal .lite-page-modal__header{margin-bottom:0}.nova-c-modal{position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,95.7%,.93);z-index:9000;overflow:auto}@supports (display:flex){.nova-c-modal{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}}@media screen and (min-width:768px){.nova-c-modal--position-center{-webkit-box-align:center;-ms-flex-align:center;align-items:center}}.nova-c-modal--width-s>.nova-c-modal__container>.nova-c-modal__window{max-width:34.375rem}.nova-c-modal--color-green .nova-c-modal__header{background:#00a39e;color:#fff}@media screen and (min-width:0px){.nova-c-modal--spacing-xl .nova-c-modal__window{padding:20px}.nova-c-modal--spacing-xl .nova-c-modal__header{padding:20px;margin:-20px -20px 20px}.nova-c-modal--spacing-xl .nova-c-modal__body{padding:20px;margin:-20px}.nova-c-modal--spacing-xl .nova-c-modal__footer{padding:10px 20px;margin:20px -20px -20px}}@media screen and (min-width:768px){.nova-c-modal--spacing-xl .nova-c-modal__window{padding:25px}.nova-c-modal--spacing-xl .nova-c-modal__header{padding:25px;margin:-25px -25px 25px}.nova-c-modal--spacing-xl .nova-c-modal__body{padding:25px;margin:-25px}.nova-c-modal--spacing-xl .nova-c-modal__footer{padding:10px 25px;margin:25px -25px -25px}}@media screen and (min-width:992px){.nova-c-modal--spacing-xl .nova-c-modal__window{padding:30px}.nova-c-modal--spacing-xl .nova-c-modal__header{padding:30px;margin:-30px -30px 30px}.nova-c-modal--spacing-xl .nova-c-modal__body{padding:30px;margin:-30px}.nova-c-modal--spacing-xl .nova-c-modal__footer{padding:10px 30px;margin:30px -30px -30px}}@media screen and (min-width:0px){.nova-c-modal--spacing-xxl .nova-c-modal__window{padding:25px}.nova-c-modal--spacing-xxl .nova-c-modal__header{padding:25px;margin:-25px -25px 25px}.nova-c-modal--spacing-xxl .nova-c-modal__body{padding:25px;margin:-25px}.nova-c-modal--spacing-xxl .nova-c-modal__footer{padding:10px 25px;margin:25px -25px -25px}}@media screen and (min-width:768px){.nova-c-modal--spacing-xxl .nova-c-modal__window{padding:30px}.nova-c-modal--spacing-xxl .nova-c-modal__header{padding:30px;margin:-30px -30px 30px}.nova-c-modal--spacing-xxl .nova-c-modal__body{padding:30px;margin:-30px}.nova-c-modal--spacing-xxl .nova-c-modal__footer{padding:10px 30px;margin:30px -30px -30px}}@media screen and (min-width:992px){.nova-c-modal--spacing-xxl .nova-c-modal__window{padding:40px}.nova-c-modal--spacing-xxl .nova-c-modal__header{padding:40px;margin:-40px -40px 40px}.nova-c-modal--spacing-xxl .nova-c-modal__body{padding:40px;margin:-40px}.nova-c-modal--spacing-xxl .nova-c-modal__footer{padding:10px 40px;margin:40px -40px -40px}}@media screen and (min-width:0px){.nova-c-modal--spacing-xxxl .nova-c-modal__window{padding:30px}.nova-c-modal--spacing-xxxl .nova-c-modal__header{padding:30px;margin:-30px -30px 30px}.nova-c-modal--spacing-xxxl .nova-c-modal__body{padding:30px;margin:-30px}.nova-c-modal--spacing-xxxl .nova-c-modal__footer{padding:10px 30px;margin:30px -30px -30px}}@media screen and (min-width:768px){.nova-c-modal--spacing-xxxl .nova-c-modal__window{padding:40px}.nova-c-modal--spacing-xxxl .nova-c-modal__header{padding:40px;margin:-40px -40px 40px}.nova-c-modal--spacing-xxxl .nova-c-modal__body{padding:40px;margin:-40px}.nova-c-modal--spacing-xxxl .nova-c-modal__footer{padding:10px 40px;margin:40px -40px -40px}}@media screen and (min-width:992px){.nova-c-modal--spacing-xxxl .nova-c-modal__window{padding:60px}.nova-c-modal--spacing-xxxl .nova-c-modal__header{padding:60px;margin:-60px -60px 60px}.nova-c-modal--spacing-xxxl .nova-c-modal__body{padding:60px;margin:-60px}.nova-c-modal--spacing-xxxl .nova-c-modal__footer{padding:10px 60px;margin:60px -60px -60px}}.nova-c-modal__container{max-height:100%;padding:0 10px}.nova-c-modal__container,.nova-c-modal__window{position:relative;width:100%;-webkit-box-sizing:border-box;box-sizing:border-box}.nova-c-modal__window{-webkit-box-shadow:0 3px 10px rgba(0,0,0,.15),0 14px 28px rgba(0,0,0,.15);box-shadow:0 3px 10px rgba(0,0,0,.15),0 14px 28px rgba(0,0,0,.15);background:#fff;margin:10px auto;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}@media screen and (min-width:768px){.nova-c-modal__window{margin:60px auto}}.nova-c-modal__window:focus{outline:none}.nova-c-modal__spinner{position:absolute;top:50%;left:50%;opacity:0;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);color:#777}.nova-c-modal__header,.nova-c-modal__spinner{-webkit-box-sizing:border-box;box-sizing:border-box}.nova-c-modal__header{background-color:#00a39e;color:#fff;position:relative;padding:30px}.nova-c-modal__header-content{position:relative;z-index:2;width:100%}.nova-c-modal__header-image{position:absolute;top:0;right:0;bottom:0;left:0;z-index:1;background-size:cover;background-position:50% 50%;opacity:.2;-webkit-transition:opacity 1s linear;transition:opacity 1s linear}@media screen and (min-width:0px){.nova-c-modal .nova-c-modal__header--spacing-none{padding:0}.nova-c-modal .nova-c-modal__header--spacing-none:before{margin-top:0;margin-right:0}}.nova-c-modal__body{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-webkit-box-sizing:border-box;box-sizing:border-box}.nova-c-modal__body-content{position:relative}.nova-c-modal__footer{min-height:4rem;-webkit-box-sizing:border-box;box-sizing:border-box;background:#fafafa;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-top:1px solid #ddd}.nova-l-flex{display:-webkit-box;display:-ms-flexbox;display:flex}.nova-l-flex--gutter-none{margin:0}.nova-l-flex--gutter-none>.nova-l-flex__item{padding:0}.nova-l-flex--gutter-xxs{margin:-2.5px}.nova-l-flex--gutter-xxs>.nova-l-flex__item{padding:2.5px}.nova-l-flex--gutter-xs{margin:-5px}.nova-l-flex--gutter-xs>.nova-l-flex__item{padding:5px}.nova-l-flex--gutter-s{margin:-7.5px}.nova-l-flex--gutter-s>.nova-l-flex__item{padding:7.5px}.nova-l-flex--gutter-m{margin:-10px}.nova-l-flex--gutter-m>.nova-l-flex__item{padding:10px}.nova-l-flex--gutter-xxxxl{margin:-50px}.nova-l-flex--gutter-xxxxl>.nova-l-flex__item{padding:50px}@media screen and (min-width:0px){.nova-l-flex--direction-column\@s-up{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.nova-l-flex--direction-row\@s-up{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row}}@media screen and (min-width:0px){.nova-l-flex--align-items-center\@s-up{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.nova-l-flex--align-items-stretch\@s-up{-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}}@media screen and (min-width:0px){.nova-l-flex--justify-content-center\@s-up{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center}.nova-l-flex--justify-content-flex-start\@s-up{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start}}@media screen and (min-width:0px){.nova-l-flex--wrap-nowrap\@s-up{-ms-flex-wrap:nowrap;flex-wrap:nowrap}.nova-l-flex--wrap-wrap\@s-up{-ms-flex-wrap:wrap;flex-wrap:wrap}}.nova-l-flex__item{-ms-flex-preferred-size:auto;flex-basis:auto}.nova-l-flex__item--grow{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;min-width:0}.nova-l-flex__item--shrink{-ms-flex-negative:1;flex-shrink:1}@media screen and (min-width:0px){.nova-l-flex__item--align-self-center\@s-up{-ms-flex-item-align:center;align-self:center}}.nova-l-form-group{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin-left:-15px;margin-bottom:-15px}.nova-l-form-group__item{-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:15px;margin-bottom:15px}.nova-l-form-group--layout-stack{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-ms-flex-wrap:nowrap;flex-wrap:nowrap}.nova-l-form-group--gutter-s{margin-left:-15px;margin-bottom:-15px}.nova-l-form-group--gutter-s>.nova-l-form-group__item{padding-left:15px;margin-bottom:15px}.nova-e-label{vertical-align:middle;display:inline-block}.nova-e-label__text{font-weight:700}.nova-c-tooltip{-webkit-box-shadow:0 1px 6px rgba(0,0,0,.13),0 3px 6px rgba(0,0,0,.09);box-shadow:0 1px 6px rgba(0,0,0,.13),0 3px 6px rgba(0,0,0,.09);-webkit-box-sizing:border-box;box-sizing:border-box;position:relative;z-index:7000;max-width:350px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.nova-c-tooltip--spacing-m .nova-c-tooltip__body{padding:20px}.nova-c-tooltip--position-above-left .nova-c-tooltip__arrow{top:100%}.nova-c-tooltip--position-above-left .nova-c-tooltip__arrow-tip{top:0;-webkit-box-shadow:2px 2px 5px rgba(0,0,0,.15);box-shadow:2px 2px 5px rgba(0,0,0,.15)}.nova-c-tooltip--position-above-left .nova-c-tooltip__arrow{left:0}.nova-c-tooltip--color-white.nova-c-tooltip--elevation-none{-webkit-box-shadow:0 0 0 0 transparent;box-shadow:0 0 0 0 transparent;border:1px solid #ddd;border-radius:.125rem}.nova-c-tooltip--color-white.nova-c-tooltip--elevation-none .nova-c-tooltip__arrow-tip{-webkit-box-shadow:0 0 0 0 transparent;box-shadow:0 0 0 0 transparent;border:1px solid #ddd}.nova-c-tooltip--elevation-none,.nova-c-tooltip--elevation-none .nova-c-tooltip__arrow-tip{-webkit-box-shadow:0 0 0 0 transparent;box-shadow:0 0 0 0 transparent}.nova-c-tooltip--color-white.nova-c-tooltip--luminosity-medium{color:#111}.nova-c-tooltip--color-white.nova-c-tooltip--luminosity-medium .nova-c-tooltip__body{background-color:#fff}.nova-c-tooltip--color-white.nova-c-tooltip--luminosity-medium .nova-c-tooltip__arrow{color:#fff}.nova-c-tooltip__arrow{position:absolute;width:48.3px;height:48.3px;overflow:hidden;-webkit-transform-origin:center center;transform-origin:center center;pointer-events:none;display:block}.nova-c-tooltip__arrow-tip{-webkit-box-shadow:2px 2px 14px rgba(0,0,0,.3);box-shadow:2px 2px 14px rgba(0,0,0,.3);position:absolute;left:50%;top:50%;width:13px;height:13px;-webkit-transform:translate(-50%,-50%) rotate(45deg);transform:translate(-50%,-50%) rotate(45deg);background:currentColor;overflow:hidden;-webkit-transform-origin:center center;transform-origin:center center}.nova-c-tooltip__arrow-tip img{position:absolute;width:368.3px;height:auto;display:block;-webkit-transform:rotate(-45deg);transform:rotate(-45deg);-webkit-transform-origin:center center;transform-origin:center center}.nova-c-tooltip__body{padding:10px;-webkit-box-sizing:border-box;box-sizing:border-box;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1}.nova-e-input__ambient{position:absolute;top:0;left:0;right:0;bottom:0;z-index:0;border:1px solid #bbb;border-radius:.125rem;background:#fafafa}.nova-e-input__field.nova-e-input__ambient{z-index:auto;position:static}.nova-e-input__field+.nova-e-input__ambient,.nova-e-input__field.nova-e-input__ambient{border:1px solid #bbb;background:#fafafa}.nova-e-input__field:focus+.nova-e-input__ambient,.nova-e-input__field:focus.nova-e-input__ambient{background:#fff;border:1px solid #0080ff;-webkit-box-shadow:inset 0 0 0 1px #0080ff;box-shadow:inset 0 0 0 1px #0080ff}.nova-e-input__field[disabled]+.nova-e-input__ambient,.nova-e-input__field[disabled].nova-e-input__ambient{border:1px solid #ddd;cursor:not-allowed}.nova-e-input__field[readonly]+.nova-e-input__ambient,.nova-e-input__field[readonly].nova-e-input__ambient{background:#fafafa;border:1px solid #bbb;-webkit-box-shadow:none;box-shadow:none}.nova-e-input__field[maxlength]:focus+.nova-e-input__ambient,.nova-e-input__field[maxlength]:focus.nova-e-input__ambient{padding-right:55px}.nova-e-input__field{position:relative;z-index:1}.nova-e-input__field{font-family:Roboto,Arial,sans-serif;font-weight:400;-webkit-appearance:none;-moz-appearance:none;appearance:none;outline:none;border:0;background:transparent;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0;width:100%}.nova-e-input__field[disabled].nova-e-input__field::-webkit-input-placeholder{color:#bbb}.nova-e-input__field[disabled].nova-e-input__field::-moz-placeholder{color:#bbb}.nova-e-input__field[disabled].nova-e-input__field:-ms-input-placeholder{color:#bbb}.nova-e-input__field[readonly]{color:#333}.nova-e-input__field--size-m{font-size:1rem;line-height:1.25;padding:.625em .5em;height:2.625em}@media screen and (min-width:768px){.nova-e-input__field--size-m{font-size:.875rem;line-height:1.28571;padding:.625em .5em;height:3em}}@-webkit-keyframes LinkFadeFocusBare{to{-webkit-box-shadow:0 2px 0 0 transparent;box-shadow:0 2px 0 0 transparent;background:transparent}}@keyframes LinkFadeFocusBare{to{-webkit-box-shadow:0 2px 0 0 transparent;box-shadow:0 2px 0 0 transparent;background:transparent}}.nova-e-link{color:inherit;outline:none;padding:0;margin:0;font-size:inherit;font-family:inherit;font-weight:inherit;border:0;background:none;text-align:inherit;display:inline}.nova-e-link,.nova-e-link:hover{text-decoration:underline;cursor:pointer}@supports (display:block){.nova-e-link:focus-visible{background:rgba(0,128,255,.11)}}.nova-e-link--theme-decorated,.nova-e-link--theme-decorated:hover{text-decoration:underline}.nova-e-link--theme-bare{text-decoration:none}.nova-e-link--theme-bare:hover{text-decoration:underline}.nova-e-link--theme-silent,.nova-e-link--theme-silent:hover{text-decoration:none}.nova-e-link--color-blue,.nova-e-link--color-blue:hover{color:#0080ff}.nova-e-checkbox{position:relative;font-family:Roboto,Arial,sans-serif;font-size:.875rem;font-weight:400;line-height:1.3;border:1px solid transparent;display:inline-block;padding:0}.nova-e-checkbox[disabled] .nova-e-checkbox__label{color:#bbb;cursor:not-allowed}.nova-e-checkbox[disabled] .nova-e-checkbox__label:before{background-color:#fafafa;border:1px solid #ddd}.nova-e-checkbox__input{opacity:0;z-index:-1;position:absolute}.nova-e-checkbox__input:focus+.nova-e-checkbox__label:before{background-color:#fafafa;border:1px solid #0080ff}.nova-e-checkbox__input:checked+.nova-e-checkbox__label:before,.nova-e-checkbox__input:checked:focus+.nova-e-checkbox__label:before{background-color:#0080ff;background-image:url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTIiIGhlaWdodD0iOSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNMTAuMDM3IDBMNC4yMjkgNi44MDcgMS44ODMgMy40NjEgMSA0LjM0NWwzLjIzIDQuMjNMMTAuOTIuODgzeiIgc3Ryb2tlPSIjRkZGIiBmaWxsPSIjRkZGIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiLz48L3N2Zz4=");background-position:50% 50%;background-repeat:no-repeat;border:1px solid #0080ff}.nova-e-checkbox__input:checked:focus+.nova-e-checkbox__label:before{-webkit-box-shadow:0 0 0 3px #d7e8f5;box-shadow:0 0 0 3px #d7e8f5}.nova-e-checkbox__input:indeterminate:checked+.nova-e-checkbox__label:before,.nova-e-checkbox__input[indeterminate]:checked+.nova-e-checkbox__label:before{background-color:#0080ff;background-image:url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZD0iTTMgN2gxMHYySDN6IiBmaWxsPSIjRkZGIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiLz48L3N2Zz4=");background-position:50% 50%;background-repeat:no-repeat;border:1px solid #0080ff}.nova-e-checkbox__label{cursor:pointer;position:relative;color:#111;-webkit-box-sizing:border-box;box-sizing:border-box;display:inline-block;min-height:.75rem}.nova-e-checkbox__label:before{content:"";display:inline-block;width:14px;height:14px;vertical-align:-.1875rem;background-color:#fafafa;border:1px solid #bbb;border-radius:.125rem}.lite-page__header{background-color:#0cb;position:absolute;top:0;width:100%;height:62px}.lite-page__header-wrapper{height:100%}.lite-page__header-logo-link{display:block;height:100%;line-height:62px;vertical-align:middle;padding:0 20px 0 30px;margin-right:10px}.lite-page__header-logo-link .brand{vertical-align:text-bottom}.lite-page__header-logo-link .brand img{margin-top:-5px}.lite-page__header-links{text-align:right;height:100%}.lite-page__header-links li{display:inline-block;height:100%}.lite-page__header-link{display:block;padding:0 30px;height:100%;line-height:62px;vertical-align:middle;font-weight:700;background-color:#0cb;color:#fff;border-left:1px solid #00a39e;text-decoration:none;font-size:.875rem;cursor:pointer}.lite-page__header-link:hover{background-color:#00a39e}.lite-page__header-login{position:relative;height:100%}.lite-page__header-login-container{position:absolute;right:0;width:100vw;max-width:310px;max-height:580px;z-index:1000;padding:30px;background-color:#fff;box-shadow:0 0 2px rgba(0,0,0,.18),0 1px 3px rgba(0,0,0,.12);box-sizing:border-box;text-align:left}.lite-page__header-login-item{margin-bottom:10px;position:relative}.lite-page__header-login-checkbox{padding:5px 0 10px;font-size:.75rem}.lite-page__header-login-label{color:#555;font-size:.875rem}.lite-page__header-login-forgot{position:absolute;right:0;top:2px}@media (max-width:1398px){.lite-page__header-link{padding:0 15px}}@media (max-width:1199px){.lite-page__header-logo-link{padding:0 20px}}@media (max-width:991px){.lite-page__header{height:40px}.lite-page__header-links{position:absolute;right:0;top:0}.lite-page__header-link{border:0;padding:0 15px;line-height:40px;font-weight:400}.lite-page__header-logo-link{line-height:40px;padding:0 10px 0 20px}.lite-page__header-logo-link .brand{width:100px}}@media (max-width:767px){.lite-page__header-link,.lite-page__header-logo-link{padding:0 10px}}@media screen and (max-width:767px){.lite-page__footer{padding:40px 10px 0}}@media screen and (min-width:768px) and (max-width:991px){.lite-page__footer{padding:40px 20px 0}}@media screen and (min-width:992px){.lite-page__footer{padding-top:40px;margin:0 auto;max-width:980px}}.lite-page__footer-index{border-top:1px solid #ddd;padding:40px 0}.lite-page__footer-index-title{font-weight:700;margin-top:10px;margin-bottom:15px;color:#111}.lite-page__footer-index-column{color:#555;font-size:.875rem}.lite-page__footer-native-app{margin-bottom:20px}.lite-page__footer-ad-slot{min-width:308px}@media screen and (max-width:767px){.lite-page__footer-ad-slot{display:none}}.lite-page__footer-base-right{text-align:right}.lite-page__footer .constrained-box-sizing{box-sizing:border-box;max-width:100%}@media screen and (min-width:992px){.lite-page--wide .lite-page__footer{max-width:1346px}}.nova-o-stack{list-style:none;margin:0;padding:0}.nova-o-stack__item{display:block;margin:0;padding:0}.nova-o-stack__item:empty{display:none!important;display:none}.nova-o-stack--show-divider>.nova-o-stack__item:not(:last-child){border-bottom:1px solid hsla(0,0%,86.7%,.5)}@media screen and (min-width:0px){.nova-o-stack--spacing-none>.nova-o-stack__item{padding-left:0;padding-right:0}}@media screen and (min-width:0px){.nova-o-stack--gutter-xxs>.nova-o-stack__item,.nova-o-stack--gutter-xxs>.nova-o-stack__item:not(:empty)~.nova-o-stack__item{padding-top:2.5px}.nova-o-stack--gutter-xxs>.nova-o-stack__item{padding-bottom:2.5px}}@media screen and (min-width:0px){.nova-o-stack--gutter-xs>.nova-o-stack__item,.nova-o-stack--gutter-xs>.nova-o-stack__item:not(:empty)~.nova-o-stack__item{padding-top:5px}.nova-o-stack--gutter-xs>.nova-o-stack__item{padding-bottom:5px}}@media screen and (min-width:0px){.nova-o-stack--gutter-s>.nova-o-stack__item,.nova-o-stack--gutter-s>.nova-o-stack__item:not(:empty)~.nova-o-stack__item{padding-top:7.5px}.nova-o-stack--gutter-s>.nova-o-stack__item{padding-bottom:7.5px}}@media screen and (min-width:0px){.nova-o-stack--gutter-m>.nova-o-stack__item,.nova-o-stack--gutter-m>.nova-o-stack__item:not(:empty)~.nova-o-stack__item{padding-top:7.5px}.nova-o-stack--gutter-m>.nova-o-stack__item{padding-bottom:7.5px}}@media screen and (min-width:992px){.nova-o-stack--gutter-m>.nova-o-stack__item,.nova-o-stack--gutter-m>.nova-o-stack__item:not(:empty)~.nova-o-stack__item{padding-top:10px}.nova-o-stack--gutter-m>.nova-o-stack__item{padding-bottom:10px}}.nova-o-stack--no-gutter-outside>.nova-o-stack__item:first-child,.nova-o-stack--no-gutter-outside>.nova-o-stack__item:not(:empty){padding-top:0}.nova-o-stack--no-gutter-outside>.nova-o-stack__item:last-child{padding-bottom:0}.lite-cookie-consent{display:block;text-align:center;min-height:38px;background:#333;line-height:24px;padding:5px 20px;box-sizing:border-box}.lite-cookie-consent__description{margin-right:20px;display:inline-block;max-width:80%}.lite-cookie-consent--hidden{display:none}.lite-cookie-consent-modal__header{position:absolute;top:-40px;left:-40px;right:-40px;background:#00a39e;color:#fff;padding:30px 40px}.lite-cookie-consent-modal__copy{padding-top:80px}@media (max-width:991px){.lite-cookie-consent-modal__header{top:-30px;left:-30px;right:-30px;padding:20px 30px}.lite-cookie-consent-modal__copy{padding-top:60px}}@media (max-width:767px){.lite-cookie-consent-modal__header{top:-25px;left:-25px;right:-25px;padding:20px 30px;font-size:1rem}.lite-cookie-consent-modal__copy{padding-top:50px}}:focus{outline:0}abbr[title]{border-bottom:1px dotted}ol,ul{list-style:none;margin:0}fieldset,ol,td,th,ul{padding:0}fieldset{margin:0;border:0}img{vertical-align:middle;border:0;-ms-interpolation-mode:bicubic}html{font-size:100%;-webkit-text-size-adjust:100%;-moz-text-size-adjust:100%;-ms-text-size-adjust:100%;text-size-adjust:100%}a:active,a:focus,a:hover{outline:0}sub,sup{position:relative;font-size:75%;line-height:0;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}button,input,select,textarea{margin:0;font-size:100%;vertical-align:middle;outline:0}button,input{line-height:normal}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;-moz-appearance:button;appearance:button;cursor:pointer}button,input[type=button],input[type=checkbox],input[type=radio],input[type=reset],input[type=submit],label,select{cursor:pointer}input[type=search]{box-sizing:content-box;-webkit-appearance:textfield}input[type=search]::-webkit-search-cancel-button,input[type=search]::-webkit-search-decoration{-webkit-appearance:none;appearance:none}textarea{overflow:auto;vertical-align:top}blockquote{quotes:none}blockquote:after,blockquote:before{content:none}q{quotes:"“" "”" "‘" "’"}q:before{content:open-quote}q:after{content:close-quote}input,textarea{box-shadow:none}body,html{height:100%}body{position:relative;margin:0;font-size:.75rem;line-height:1.5;font-family:Roboto,Arial,sans-serif;color:#111;text-align:left;word-wrap:break-word}.lite-page{min-height:100%}@-webkit-keyframes loading{0%{opacity:.4}50%{opacity:1}to{opacity:.4}}@keyframes loading{0%{opacity:.4}50%{opacity:1}to{opacity:.4}}.lite-page{position:relative}.lite-page--lock-view{overflow:hidden!important}.lite-page:before{content:"";display:block;height:62px;background-color:#0cb}@media (max-width:991px){.lite-page:before{height:40px}}.lite-page__main{display:flex;justify-content:center;padding:0 20px}.lite-page__above{margin:20px 0}.lite-page__content{width:100%;flex-grow:1;max-width:585px;margin-right:395px}.lite-page__side{width:365px;flex-shrink:0;margin-left:-365px}.lite-page--with-header-navigation .lite-page__content,.lite-page--with-header-navigation .lite-page__side{margin-top:20px}.lite-page--with-header-navigation .lite-page-ad--sticky{top:202px}.lite-page--with-header-navigation.lite-page--header-ad-collapsed .lite-page-ad--sticky{top:80px}.lite-page--wide .lite-page__content{max-width:975px;margin-right:371px}.lite-page--wide .lite-page__side{width:351px;margin-left:-351px}.lite-page .lite-page-center{text-align:center}.lite-page .lite-page-hidden{display:none}.lite-page .lite-page-visible{display:block}@media screen and (max-width:1199px){.lite-page .hide-l{display:none}.lite-page .lite-page__above{margin-top:0}.lite-page .lite-page__content{margin-right:0}.lite-page .lite-page__main{flex-wrap:wrap;padding:0 5px;margin-top:10px}.lite-page .lite-page__side{max-width:585px;width:100%;margin-left:0;margin-top:20px}.lite-page--with-header-navigation .lite-page__content,.lite-page--with-header-navigation .lite-page__side{margin-top:10px}}@media screen and (max-width:767px){.lite-page .hide-m{display:none}.lite-page .lite-page__below{padding:0 10px}}@media (-ms-high-contrast:none),screen and (-ms-high-contrast:active){.lite-page__content{flex:1 0 100%}}.lite-page .lite-dropdown{position:relative;display:inline-block}.lite-page .lite-dropdown__dropdown-wrapper{top:100%;position:absolute;right:0;margin-top:15px;z-index:7000}.lite-page .lite-dropdown__dropdown-close{display:none;text-align:right}@media screen and (max-width:767px){.lite-page .lite-dropdown__dropdown-wrapper{position:fixed;top:50%;margin-top:-200px;right:20px;left:20px}.lite-page .lite-dropdown__dropdown{width:100%}.lite-page .lite-dropdown__dropdown-close{display:block}}.nova-c-dropdown{-webkit-box-shadow:0 1px 6px rgba(0,0,0,.13),0 3px 6px rgba(0,0,0,.09);box-shadow:0 1px 6px rgba(0,0,0,.13),0 3px 6px rgba(0,0,0,.09);z-index:7000;background:#fff;display:inline-block;min-width:230px;overflow-y:auto;max-height:400px}.nova-c-dropdown__body{position:relative;padding-top:5px;padding-bottom:5px;background:#fff}.nova-c-dropdown__divider{display:block;height:1px;background:#ddd;margin-top:5px;margin-bottom:5px}.nova-c-dropdown__action{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-negative:0;flex-shrink:0;width:100%;-webkit-box-sizing:border-box;box-sizing:border-box;padding:.9375rem 1.25rem;border:0;outline:none;cursor:pointer;background:#fff;color:#555;overflow:hidden;text-align:left;text-decoration:none}.nova-c-dropdown__action:before{content:"";position:absolute;background:#bbb;-webkit-transform:translate(0);transform:translate(0);-webkit-transition:.15s cubic-bezier(.455,.03,.515,.955);transition:.15s cubic-bezier(.455,.03,.515,.955);right:100%;top:0;bottom:0;width:2px}.nova-c-dropdown__action:focus,.nova-c-dropdown__action:hover{text-decoration:none}.nova-c-dropdown__action:focus:before,.nova-c-dropdown__action:hover:before{-webkit-transform:translate(100%);transform:translate(100%)}.nova-c-dropdown__action[disabled]{cursor:default;pointer-events:none;color:#bbb}.nova-c-dropdown__label-container{max-width:100%}.nova-e-radio{position:relative;border:1px solid transparent;display:inline-block;padding:0}.nova-e-radio[disabled] .nova-e-radio__label{color:#bbb;cursor:not-allowed}.nova-e-radio[disabled] .nova-e-radio__label:before{background-color:#fafafa;border:1px solid #ddd}.nova-e-radio__input{opacity:0;z-index:-1;position:absolute}.nova-e-radio__input:checked+.nova-e-radio__label:before,.nova-e-radio__input:focus+.nova-e-radio__label:before{border:1px solid #0080ff}.nova-e-radio__input:checked+.nova-e-radio__label:after{display:inline-block}.nova-e-radio__input:checked:focus+.nova-e-radio__label:before{border:1px solid #0080ff;-webkit-box-shadow:0 0 0 3px #d7e8f5;box-shadow:0 0 0 3px #d7e8f5}.nova-e-radio__input:checked:focus+.nova-e-radio__label:after{display:inline-block;background-color:#0080ff}.nova-e-radio__label{font-family:Roboto,Arial,sans-serif;font-size:.875rem;font-weight:400;line-height:1.3;cursor:pointer;position:relative;-webkit-box-sizing:border-box;box-sizing:border-box;color:#111;display:inline-block;min-height:.75rem}.nova-e-radio__label:before{content:"";display:inline-block;width:14px;height:14px;vertical-align:top;background-color:#fafafa;border:1px solid #bbb;border-radius:624.9375rem}.nova-e-radio__label:after{content:"";display:none;width:8px;height:8px;position:absolute;left:0;top:0;background-color:#0080ff;border-radius:624.9375rem;margin:4px}.publication-resources-summary{position:relative;height:46px}.publication-resources-summary:before{content:"";display:block}.publication-resources-summary__to-top{display:none}.publication-resources-summary__inner{position:relative;left:0;top:0;right:0;z-index:90;box-sizing:border-box;padding:0 20px;background:hsla(0,0%,100%,.95);transform:translateZ(0)}.publication-resources-summary__inner.is-sticky{position:fixed;padding-top:5px;top:122px;box-shadow:0 0 2px rgba(0,0,0,.18),0 1px 3px rgba(0,0,0,.12)}@media (max-width:819px){.publication-resources-summary__inner.is-sticky{top:0}}.publication-resources-summary__content{max-width:1346px;height:100%;margin:0 auto}.publication-resources-summary__cta-box{text-align:right}.publication-resources-summary__cta-box-inner{display:inline-block}.publication-resources-summary .publication-resources-summary__see-all:hover .publication-resources-summary__see-all-label{text-decoration:none}.publication-resources-summary .publication-resources-summary__see-all:hover .publication-resources-summary__see-all-label .publication-resources-summary__see-all-count{text-decoration:underline}.publication-resources-summary .lite-dropdown__dropdown-wrapper{margin-top:0;top:70%}@media screen and (max-width:767px){.publication-resources-summary__inner.is-sticky .publication-resources-summary__to-top{display:block}}.lite-page--header-ad-collapsed .publication-resources-summary .is-sticky{top:0}.publication-meta{margin-bottom:20px}.lite-page-avatar{position:relative;overflow:hidden;display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;align-items:center}.lite-page-avatar--size-xs{width:16px;height:16px}.lite-page-avatar--size-m{width:48px;height:48px}.lite-page-avatar--size-l{width:64px;height:64px}.lite-page-avatar--radius-full,.lite-page-avatar--radius-full .lite-image{border-radius:624.9375rem}.lite-page-avatar--radius-none,.lite-page-avatar--radius-none .lite-image{border-radius:0}.lite-page-avatar .lite-image{width:100%;height:100%}.nova-v-person-list-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;position:relative}.nova-v-person-list-item__body{overflow:hidden}.nova-v-person-list-item__footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-wrap:wrap-reverse;flex-wrap:wrap-reverse;margin:0 -15px}.nova-v-person-list-item__footer-metrics{margin:10px 15px 0}.nova-v-person-list-item__footer-metrics:empty{display:none!important}.nova-v-person-list-item__align{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.nova-v-person-list-item__align:before{content:"";width:1px;height:48px}.nova-v-person-list-item__align-content{overflow:hidden}.nova-v-person-list-item__meta{color:#777;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.nova-v-person-list-item__title{font-weight:700;color:#111}.nova-v-person-list-item__stack-item:empty{display:none!important}.nova-v-person-list-item__stack--gutter-s .nova-v-person-list-item__stack-item:not(:empty)~.nova-v-person-list-item__stack-item:not(:empty){margin-top:5px}.publication-author-list{display:flex;flex-flow:wrap}@media screen and (min-width:768px){.publication-author-list__item{box-sizing:border-box;width:50%}}@media screen and (max-width:767px){.publication-author-list__item{width:100%}}@media screen and (min-width:768px){.publication-author-list__item:nth-child(2n){padding-left:10px}}.show-more-less-authors__button--hidden{display:none}.signup-promo{margin-top:40px;width:410px;padding:25px 35px;background-color:#ddf0ee;color:#00141a;position:relative}.signup-promo__image{position:absolute;top:20px;right:20px}.signup-promo__cta{font-weight:300;padding:.6rem 2rem}@media (max-width:767px){.signup-promo{width:auto}}@media (max-width:479px){.signup-promo__image{display:none}}.nova-o-grid{margin:0;padding:0;list-style:none;letter-spacing:-.31em}.nova-o-grid__column{display:inline-block;width:100%;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:0;vertical-align:top;letter-spacing:normal;text-align:left}.nova-o-grid--vertical-align-top>.nova-o-grid__column{vertical-align:top}.nova-o-grid--horizontal-align-left{text-align:left}.nova-o-grid--gutter-m{margin-left:-10px;margin-right:-10px}.nova-o-grid--gutter-m>.nova-o-grid__column{padding-left:10px;padding-right:10px}@media screen and (min-width:0px){.nova-o-grid__column--width-12\/12\@s-up{width:100%}}@media screen and (min-width:768px){.nova-o-grid__column--width-6\/12\@m-up{width:50%}}.nova-v-citation-item__context{max-width:none;z-index:auto}.nova-e-text-highlight{line-height:inherit;padding:0 .1875rem;color:#000}.nova-e-text-highlight--color-yellow{background-color:#fce3ac}.lite-person-inline-item__image{display:inline-block;line-height:inherit;width:1.1428571429em;height:1.1428571429em;vertical-align:middle;margin-top:-2px}.nova-v-person-inline-item{text-decoration:none;outline:none;border:0;background:none;padding:0;margin:0;font-size:inherit;font-family:inherit;font-weight:inherit;color:inherit}a.nova-v-person-inline-item:hover,button.nova-v-person-inline-item:hover{text-decoration:underline;cursor:pointer}.nova-v-person-inline-item__fullname{display:inline}.nova-v-person-inline-item__fullname:not(:first-child){margin-left:.357em}.nova-v-publication-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1}.nova-v-publication-item__body{overflow:hidden}.nova-v-publication-item__footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-wrap:wrap-reverse;flex-wrap:wrap-reverse;margin:0 -15px}.nova-v-publication-item__footer-actions{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;margin:10px 15px 0}.nova-v-publication-item__footer-actions:empty{display:none!important}.nova-v-publication-item__footer-button-group{overflow:hidden}.nova-v-publication-item__meta{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-bottom:-5px}.nova-v-publication-item__meta:empty{display:none!important}.nova-v-publication-item__meta-left{margin-bottom:5px;margin-right:5px}.nova-v-publication-item__meta-left:empty{display:none!important}.nova-v-publication-item__meta-right{margin-bottom:5px;-webkit-box-flex:1;-ms-flex:1 0 0px;flex:1 0 0;overflow:hidden;min-width:100%}.nova-v-publication-item__meta-right:empty{display:none!important}@media screen and (min-width:768px){.nova-v-publication-item__meta-right{min-width:auto}.nova-v-publication-item__meta-left:nth-child(n+3)~.nova-v-publication-item__meta-right{min-width:100%}}.nova-v-publication-item__title{font-weight:700;color:#111}.nova-v-publication-item__description{color:#111;display:-webkit-box}.nova-v-publication-item__meta-data{color:#777;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.nova-v-publication-item__stack-item:empty{display:none!important}.nova-v-publication-item__stack--gutter-m .nova-v-publication-item__stack-item:not(:empty)~.nova-v-publication-item__stack-item:not(:empty){margin-top:5px}@media screen and (min-width:768px){.nova-v-publication-item__stack--gutter-m .nova-v-publication-item__stack-item:not(:empty)~.nova-v-publication-item__stack-item:not(:empty){margin-top:10px}}.nova-v-publication-item__person-list{color:#111}@-webkit-keyframes flash{0%,50%,to{opacity:.4}25%,75%{opacity:1}}@keyframes flash{0%,50%,to{opacity:.4}25%,75%{opacity:1}}.lite-tabs__tab{display:none}.nova-c-nav{background:#fff;height:100%;width:100%;position:relative;overflow:hidden}.nova-c-nav--direction-horizontal{height:3.875rem}.nova-c-nav--direction-horizontal .nova-c-nav__items{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;max-height:7.75rem;height:auto}.nova-c-nav--direction-horizontal .nova-c-nav__item{height:3.875rem}.nova-c-nav--direction-horizontal .nova-c-nav__item:after{top:100%;left:.625rem;right:.625rem;height:2px}.nova-c-nav--direction-horizontal .nova-c-nav__item.is-selected:after,.nova-c-nav--direction-horizontal .nova-c-nav__item:focus:after,.nova-c-nav--direction-horizontal .nova-c-nav__item:hover:after{-webkit-transform:translateY(-100%);transform:translateY(-100%)}.nova-c-nav--theme-tabs{background:#fafafa}.nova-c-nav--theme-tabs:before{content:"";background:#ddd;position:absolute}.nova-c-nav--theme-tabs .nova-c-nav__item{position:relative;border-color:hsla(0,0%,86.7%,.25);border-style:solid}.nova-c-nav--theme-tabs .nova-c-nav__item:after{display:none}.nova-c-nav--theme-tabs .nova-c-nav__item:focus{background:#f4f4f4}.nova-c-nav--theme-tabs .nova-c-nav__item.is-selected{background:#fff;border-color:#ddd}.nova-c-nav--theme-tabs .nova-c-nav__plus-overflow-trigger:focus{background:#f4f4f4}.nova-c-nav--theme-tabs.nova-c-nav--direction-horizontal:before{height:.0625rem;width:100%;bottom:0;left:0}.nova-c-nav--theme-tabs.nova-c-nav--direction-horizontal .nova-c-nav__item{border-left-width:1px;border-right-width:1px;border-bottom:1px solid #ddd}.nova-c-nav--theme-tabs.nova-c-nav--direction-horizontal .nova-c-nav__item.is-selected{border-bottom:1px solid #fff}.nova-c-nav--theme-tabs.nova-c-nav--direction-horizontal .nova-c-nav__item:not(:last-child){margin-right:-1px}.nova-c-nav__wrapper{overflow:hidden}.nova-c-nav__items,.nova-c-nav__wrapper{display:-webkit-box;display:-ms-flexbox;display:flex}.nova-c-nav__items{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-wrap:wrap;flex-wrap:wrap}.nova-c-nav--theme-tabs .nova-c-nav__items:after{background:-webkit-gradient(linear,left top,right top,from(hsla(0,0%,98%,0)),color-stop(50%,#fafafa));background:linear-gradient(90deg,hsla(0,0%,98%,0),#fafafa 50%)}@media (pointer:coarse){.nova-c-nav__items{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;overflow:scroll}.nova-c-nav__items:after{content:"";position:relative;width:3.75rem;z-index:2;-ms-flex-negative:0;flex-shrink:0;margin:.0625rem 0 .125rem auto}}.nova-c-nav__scroll-overflow{position:absolute;top:0;right:0;bottom:.125rem;width:3.75rem;-webkit-box-sizing:border-box;box-sizing:border-box;color:#111;display:none;z-index:1}.nova-c-nav--theme-tabs .nova-c-nav__scroll-overflow{background:-webkit-gradient(linear,left top,right top,from(hsla(0,0%,98%,0)),color-stop(50%,#fafafa));background:linear-gradient(90deg,hsla(0,0%,98%,0),#fafafa 50%)}@media (pointer:coarse){.nova-c-nav__scroll-overflow{display:-webkit-box;display:-ms-flexbox;display:flex}}.nova-c-nav__scroll-overflow-trigger{color:#555;white-space:nowrap;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;overflow:hidden;background:none;border:0;text-decoration:none;cursor:pointer;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;padding:0 0 0 1.875rem}.nova-c-nav__scroll-overflow-trigger:hover{color:#111}.nova-c-nav__plus-overflow-mount{position:absolute;bottom:0;right:0}.nova-c-nav__plus-overflow{display:-webkit-box;display:-ms-flexbox;display:flex;width:3.875rem;height:3.875rem;-ms-flex-negative:0;flex-shrink:0;-ms-flex-item-align:end;align-self:flex-end;-webkit-transform:translateY(-100%);transform:translateY(-100%)}@media (pointer:coarse){.nova-c-nav__plus-overflow{display:none}}.nova-c-nav__plus-overflow-trigger{color:#555;white-space:nowrap;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;overflow:hidden;background:none;border:0;text-decoration:none;cursor:pointer;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;padding:0;outline:none}.nova-c-nav--theme-tabs .nova-c-nav__plus-overflow-trigger{border-bottom:1px solid #ddd}.nova-c-nav__plus-overflow-trigger:hover{color:#111}.nova-c-nav__item{padding:.9375rem 1.25rem;-webkit-box-sizing:border-box;box-sizing:border-box;color:#555;white-space:nowrap;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;overflow:hidden;text-align:left;background:none;border:0;-ms-flex-negative:0;flex-shrink:0;margin:0;text-decoration:none;cursor:pointer;outline:none}.nova-c-nav__item:after{content:"";position:absolute;background:#bbb;-webkit-transform:translate(0);transform:translate(0);-webkit-transition:.15s cubic-bezier(.455,.03,.515,.955);transition:.15s cubic-bezier(.455,.03,.515,.955)}.nova-c-nav__item.is-selected,.nova-c-nav__item:hover{color:#111;text-decoration:none;background:none}.nova-c-nav__item:disabled,.nova-c-nav__item[disabled]{cursor:default;pointer-events:none;color:#bbb}.nova-c-nav--direction-horizontal .nova-c-nav__item:last-child{margin-right:-3.875rem}@media (pointer:coarse){.nova-c-nav--direction-horizontal .nova-c-nav__item:last-child{margin-right:-.625rem}}.nova-c-nav--theme-tabs .nova-c-nav__item-label{font-size:1rem}.publication-citations__item{border-top:1px solid #ddd;margin-top:15px;padding-top:15px;overflow:hidden}.publication-citations__item:first-child{border-top:0;margin-top:0}.publication-citations__item--load-more{border-top:0}@media screen and (min-width:768px){.publication-citations__item{margin-top:25px;padding-top:25px}}.publication-citations__item .publication-citations__item:first-child{padding-top:0}.publication-citations__empty{padding-top:20px;color:#555}.publication-citations__more{text-align:right}.publication-citations__menubar{margin-left:-1px}@-webkit-keyframes progressValueIdle{0%,to{background-color:#83b7e6}50%{background-color:#0080ff}}@keyframes progressValueIdle{0%,to{background-color:#83b7e6}50%{background-color:#0080ff}}@-webkit-keyframes progressValueIndeterminate{0%{-webkit-transform:translateX(-100%);transform:translateX(-100%)}to{-webkit-transform:translateX(303.0303%);transform:translateX(303.0303%)}}@keyframes progressValueIndeterminate{0%{-webkit-transform:translateX(-100%);transform:translateX(-100%)}to{-webkit-transform:translateX(303.0303%);transform:translateX(303.0303%)}}::-moz-selection{background:rgba(127,255,255,.4)}::selection{background:rgba(127,255,255,.4)}.publication-details__section{margin-bottom:60px}@media screen and (max-width:768px){.publication-details__title{font-size:1.375rem;font-weight:400}}@-webkit-keyframes bump{0%{-webkit-transform:scale(1);transform:scale(1)}35%{-webkit-transform:scale(1.2);transform:scale(1.2)}to{-webkit-transform:scale(1);transform:scale(1)}}@keyframes bump{0%{-webkit-transform:scale(1);transform:scale(1)}35%{-webkit-transform:scale(1.2);transform:scale(1.2)}to{-webkit-transform:scale(1);transform:scale(1)}}.publication-bottom{width:100%}.publication-recommendation-item{position:relative;padding:20px 20px 40px;min-height:270px;background-color:#fff;box-shadow:1px 1px 1px 0 rgba(0,0,0,.075)}.publication-recommendation-item__item{margin-bottom:20px}.publication-recommendation-item__item-badges{margin-bottom:10px}.publication-recommendation-item__more{position:absolute;left:20px;bottom:20px}.publication-recommendation-item__abstract,.publication-recommendation-item__title{word-break:break-word}.nova-v-project-item__person-list{color:#111}.project-recommendation-item{position:relative;padding:20px 20px 40px;min-height:270px;background-color:#fff;box-shadow:1px 1px 1px 0 rgba(0,0,0,.075)}.project-recommendation-item__item{margin-bottom:20px}.project-recommendation-item__item-badges{margin-bottom:10px}.project-recommendation-item__researcher{margin-bottom:30px}.project-recommendation-item__more{position:absolute;left:20px;bottom:20px}.project-recommendation-item__description,.project-recommendation-item__title{word-break:break-word}.publication-recommendations{background-color:#fafafa;margin-bottom:20px;padding:0 20px}.publication-recommendations__inner{max-width:955px;padding:40px 0}.publication-recommendations__outer{max-width:1326px;margin:0 auto}.publication-recommendations__items{margin-top:20px;margin-bottom:20px}.native-app-recruitment-container{position:fixed;top:0;left:0;right:0;width:100%;z-index:1002}.native-app-recruitment-container__app-image{background-color:#fafafa;max-height:30px;max-width:30px}.native-app-recruitment-container__app-image .lite-image{width:30px;height:30px;border-radius:.125rem}</style>
<title>Developmental Robotics: From Babies to Robots | Request PDF</title>
<meta name="description" content="Request PDF | Developmental Robotics: From Babies to Robots | Developmental robotics is a collaborative and interdisciplinary approach to robotics that is directly inspired by the developmental principles and... | Find, read and cite all the research you need on ResearchGate"/>
</head><body>
<div class="lite-cookie-consent lite-cookie-consent--hidden js-target-cookie-consent"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-100 lite-cookie-consent__description">We use cookies to make interactions with our website easy and meaningful, to better understand the use of our services, and to tailor advertising. For further information, including about cookie settings, please read our <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-decorated" target="_blank" href="privacy-policy#cookies" rel="noopener">Cookie Policy</a> . By continuing to use this site, you consent to the use of cookies.</div><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-xs nova-c-button--color-white nova-c-button--theme-ghost nova-c-button--width-auto js-lite-click" type="button" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-cookie-consent&quot;,&quot;a&quot;:&quot;ac(lite-cookie-consent--hidden)&quot;}]}"><span class="nova-c-button__label">Got it</span></button></div><div id="lite-page" class="lite-page lite-page--wide publication"><div id="js-target-cookie-consent-modal" class="lite-page-modal lite-page-modal--hidden"><div><div class="nova-c-modal nova-c-modal--color-green nova-c-modal--position-center nova-c-modal--spacing-xxl nova-c-modal--width-s lite-cookie-consent-modal" style="z-index:9998"><div class="nova-c-modal__overlay"></div><div class="nova-c-modal__container"><div class="nova-c-modal__window" role="dialog" aria-modal="true" tabindex="0"><header class="nova-c-modal__header nova-c-modal__header--spacing-none lite-page-modal__header"><div class="nova-c-modal__header-content"></div><div class="nova-c-modal__header-image"></div></header><div class="nova-c-modal__body nova-c-modal__body--spacing-inherit"><div class="nova-c-modal__body-content"><div class="lite-cookie-consent-modal__header"><div class="nova-e-text nova-e-text--size-xxl nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">We value your privacy</div></div><div class="lite-cookie-consent-modal__copy"><p class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-m nova-e-text--color-inherit">We use cookies to offer you a better experience, personalize content, tailor advertising, provide social media features, and better understand the use of our services.</p><p class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">To learn more or modify/prevent the use of cookies, see our <a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated gtm-open-cookie-policy-link js-lite-click" target="_blank" rel="noopener" href="cookie-consent-policy" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.gtm-open-cookie-policy-link&quot;,&quot;a&quot;:&quot;tgtm(gtm-ev-cookie-policy)&quot;}]}">Cookie Policy</a> and <a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated gtm-open-privacy-policy-link js-lite-click" target="_blank" rel="noopener" href="privacy-policy" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.gtm-open-privacy-policy-link&quot;,&quot;a&quot;:&quot;tgtm(gtm-ev-privacy-policy)&quot;}]}">Privacy Policy</a>.</p></div></div></div><footer class="nova-c-modal__footer nova-c-modal__footer--align-right nova-c-modal__footer--spacing-inherit"><div class="nova-c-modal__footer-content"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-auto js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;#js-target-cookie-consent-modal&quot;,&quot;a&quot;:&quot;ac(lite-page-modal--hidden)&quot;},{&quot;t&quot;:&quot;window&quot;,&quot;a&quot;:&quot;tcl(cookie-consented)&quot;},{&quot;t&quot;:&quot;.gtm-cookie-consented&quot;,&quot;a&quot;:&quot;tgtm(gtm-ev-cookie-consented)&quot;}]}"><span class="nova-c-button__label gtm-cookie-consented">Accept Cookies</span></button></div></footer><div class="nova-e-spinner nova-e-spinner--size-m nova-e-spinner--color-inherit nova-c-modal__spinner"><div class="nova-e-spinner__container"><svg width="16" height="16" viewBox="0 0 16 16" class="nova-e-spinner__asset"><g stroke-width="2"><path class="nova-e-spinner__circle" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path><path class="nova-e-spinner__stroke" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path></g></svg></div></div></div></div></div></div></div><div id="lite-ad-4mzzq7e5f3n" class="lite-page-ad lite-page-ad--vertical-spacing-xs lite-page-ad--layout-header lite-page-ad--has-label lite-page-ad--empty js-lite-sticky"><div class="lite-page-ad__inner js-lite-sticky-target"><button class="nova-c-button nova-c-button--align-left nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-grey nova-c-button--theme-bare nova-c-button--width-square lite-page-ad__header-close" type="button" id="lite-ad-4mzzq7e5f3n-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-circle-s"></use></svg></button><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-4mzzq7e5f3n-close').onclick = function(e) {document.querySelector('#lite-ad-4mzzq7e5f3n').style.display = 'none'; RGDom.className.addClass(document.querySelector('#lite-page'), 'lite-page--header-ad-collapsed');};</script><div id="4mzzq7e5f3n"><rg-ad id="paxitub9tml" ad-unit="LoggedOut_PublicationWithoutFulltext_Top" ad-unit-path="/83500759/LoggedOut_PublicationWithoutFulltext_Top" ad-location="Top" refresh-timeout="-1" load-on-init="false" collapse="never" class="rg-ad" ad-unit-sizes="[[728,90],[970,90],&quot;fluid&quot;]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="paxitub9tml" targeting-sequence="1"><rg-ad-size size="[728,90]"></rg-ad-size><rg-ad-size size="[970,90]"></rg-ad-size><rg-ad-size size="&quot;fluid&quot;"></rg-ad-size><rg-ad-sizeset viewport-size="[0,0]" slot-sizes="[]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[300,0]" slot-sizes="[[300,100],[300,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[320,0]" slot-sizes="[[320,100],[320,50],[300,100],[300,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[728,0]" slot-sizes="[[728,90],&quot;fluid&quot;]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[970,0]" slot-sizes="[[970,90],[728,90],&quot;fluid&quot;]"></rg-ad-sizeset></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("paxitub9tml").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-4mzzq7e5f3n').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-4mzzq7e5f3n");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-4mzzq7e5f3n').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-4mzzq7e5f3n");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div><div class="lite-page__above"><section class="js-lite-sticky publication-resources-summary" role="navigation"><div class="js-lite-sticky-target publication-resources-summary__inner"><div class="publication-resources-summary__ad-slot"></div><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-none nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up publication-resources-summary__content"><div class="nova-l-flex__item publication-resources-summary__to-top"><button class="nova-c-button nova-c-button--align-left nova-c-button--radius-m nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-bare nova-c-button--width-auto js-lite-click" type="button" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;body&quot;,&quot;a&quot;:&quot;st&quot;}]}"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#arrow-up-s"></use></svg><span class="nova-c-button__label">top</span></button></div><div class="nova-l-flex__item hide-m"><div class="nova-o-pack nova-o-pack--gutter-xl nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item"><a class="nova-c-button nova-c-button--align-left nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto publication-resources-summary__see-all js-lite-click" role="button" href="publication/290749036_Developmental_Robotics_From_Babies_to_Robots#citations" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-citation&quot;,&quot;a&quot;:&quot;st(64)&quot;,&quot;d&quot;:100},{&quot;t&quot;:&quot;.js-target-citationReferences button.citations&quot;,&quot;a&quot;:&quot;tcl(click)&quot;}]}"><span class="nova-c-button__label publication-resources-summary__see-all-label"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-900">See all ›</div><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit publication-resources-summary__see-all-count"><strong>208</strong> Citations</div></span></a></div></div></div><div class="nova-l-flex__item nova-l-flex__item--grow publication-resources-summary__cta-box"><div class="publication-resources-summary__cta-box-inner"><div class="nova-o-pack nova-o-pack--gutter-l nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item hide-m"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto" href="https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots/citation/download"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#download-s"></use></svg><span class="nova-c-button__label gtm-download-citation-btn">Download citation</span></a></div><div class="nova-o-pack__item"><div class="lite-dropdown"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto lite-share js-lite-click" type="button" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-share-content&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;,&quot;r&quot;:&quot;rco&quot;}]}"><span class="nova-c-button__label">Share <svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium lite-share__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#arrow-down-s"></use></svg></span></button><div class="lite-dropdown__dropdown-wrapper lite-page-hidden js-target-share-content"><div role="menu" class="nova-c-dropdown lite-dropdown__dropdown"><div class="nova-c-dropdown__body"><button type="button" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-share-content&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}" class="nova-c-dropdown__action lite-dropdown__dropdown-close js-lite-click" role="menuitem"><span class="nova-c-dropdown__label-container"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-black nova-e-icon--luminosity-low"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></div></span></button><span class="nova-c-dropdown__divider lite-dropdown__dropdown-close" aria-hidden="true"></span><a href="http://www.facebook.com/share.php?u=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F290749036_Developmental_Robotics_From_Babies_to_Robots" data-provider-context="facebook" class="nova-c-dropdown__action share-facebook" role="menuitem"><span class="nova-c-dropdown__label-container"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#social-facebook-s"></use></svg> Facebook</div></span></a><a href="http://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F290749036_Developmental_Robotics_From_Babies_to_Robots" data-provider-context="twitter" class="nova-c-dropdown__action share-twitter" role="menuitem"><span class="nova-c-dropdown__label-container"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#social-twitter-s"></use></svg> Twitter</div></span></a><a href="http://www.linkedin.com/shareArticle?mini=true&amp;source=ResearchGate&amp;url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F290749036_Developmental_Robotics_From_Babies_to_Robots" data-provider-context="linkedin" class="nova-c-dropdown__action share-linkedin" role="menuitem"><span class="nova-c-dropdown__label-container"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#social-linkedin-s"></use></svg> LinkedIn</div></span></a><a href="https://www.reddit.com/submit?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F290749036_Developmental_Robotics_From_Babies_to_Robots" data-provider-context="reddit" class="nova-c-dropdown__action share-reddit" role="menuitem"><span class="nova-c-dropdown__label-container"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#social-reddit-s"></use></svg> Reddit</div></span></a></div></div></div></div></div></div></div></div></div></div></section></div><main class="lite-page__main" role="main"><section class="lite-page__content"><section class="publication-details__section"><h1 class="nova-e-text nova-e-text--size-xxxl nova-e-text--family-sans-serif nova-e-text--spacing-l nova-e-text--color-grey-900 publication-details__title" itemProp="headline">Developmental Robotics: From Babies to Robots</h1><div class="publication-meta"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-xxs nova-e-text--color-grey-700"><span class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-900"><strong><span data-testid="research-meta-type" class="publication-meta__type">Book</span></strong></span><span> · January 2015</span> <em class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-500">with</em> 568 Reads <div class="lite-page-tooltip"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#info-circle-s"></use></svg><div class="lite-page-tooltip__content lite-page-tooltip__content--above"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><strong>How we measure &#x27;reads&#x27;</strong></div><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">A &#x27;read&#x27; is counted each time someone views a publication summary (such as the title, abstract, and list of authors), clicks on a figure, or views or downloads the full-text. <a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="https://explore.researchgate.net/display/support/Reads" target="_blank" rel="noopener">Learn more</a></div><div class="lite-page-tooltip__arrow lite-page-tooltip__arrow--above"><div class="lite-page-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-500">DOI: <a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="https://www.researchgate.net/deref/http%3A%2F%2Fdx.doi.org%2F10.7551%2Fmitpress%2F9320.001.0001" target="_blank" rel="noopener">10.7551/mitpress/9320.001.0001</a><div>Isbn: <span class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-900">9780262028011</span></div><div>Publisher: <span class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-900">MIT PRESS</span></div></div><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto" href="https://www.researchgate.net/publication/290749036_Developmental_Robotics_From_Babies_to_Robots/citation/download"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#download-s"></use></svg><span class="nova-c-button__label gtm-cite-publication-btn">Cite this publication</span></a></div><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-bare nova-e-list--spacing-s publication-author-list load-more-authors"><li class="nova-e-list__item publication-author-list__item" data-testid="research-header-author-item"><div class="nova-c-card nova-c-card--spacing-xs nova-c-card--elevation-none"><div class="nova-c-card__body nova-c-card__body--spacing-inherit"><div class="nova-v-person-list-item has-image"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xs nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><a href="https://www.researchgate.net/profile/Angelo_Cangelosi"><picture class="lite-page-avatar lite-page-avatar--size-m lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q64/Angelo_Cangelosi.jpg" data-srcset="https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q64/Angelo_Cangelosi.jpg 1x, https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q128/Angelo_Cangelosi.jpg 2x" class="lite-image" alt="Angelo Cangelosi at The University of Manchester"/></picture></a></div><div class="nova-l-flex__item nova-l-flex__item--grow nova-v-person-list-item__body"><div class="nova-v-person-list-item__stack nova-v-person-list-item__stack--gutter-s"><div class="nova-v-person-list-item__stack-item"><div class="nova-v-person-list-item__align"><div class="nova-v-person-list-item__align-content"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp nova-v-person-list-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Angelo_Cangelosi">Angelo Cangelosi</a></div><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-person-list-item__meta"><li class="nova-e-list__item nova-v-person-list-item__meta-item"><span><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Angelo_Cangelosi"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#rg-score-s"></use></svg>38.03</a></span></li><li class="nova-e-list__item nova-v-person-list-item__meta-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="institution/The_University_of_Manchester">The University of Manchester</a></span></li></ul></div></div></div></div><footer class="nova-v-person-list-item__footer"><div class="nova-v-person-list-item__footer-metrics"></div></footer></div><div class="nova-l-flex__item nova-l-flex__item--shrink"><div class="nova-v-person-list-item__align"><div class="nova-v-person-list-item__align-content"></div></div></div></div></div></div></div></li><li class="nova-e-list__item publication-author-list__item" data-testid="research-header-author-item"><div class="nova-c-card nova-c-card--spacing-xs nova-c-card--elevation-none"><div class="nova-c-card__body nova-c-card__body--spacing-inherit"><div class="nova-v-person-list-item has-image"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xs nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><a href="https://www.researchgate.net/scientific-contributions/2139240686_Matthew_Schlesinger"><picture class="lite-page-avatar lite-page-avatar--size-m lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg" data-srcset="https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg 1x, https://c5.rgstatic.net/m/420791293341780/images/template/default/author/author_default_l.jpg 2x" class="lite-image" alt="Matthew Schlesinger"/></picture></a></div><div class="nova-l-flex__item nova-l-flex__item--grow nova-v-person-list-item__body"><div class="nova-v-person-list-item__stack nova-v-person-list-item__stack--gutter-s"><div class="nova-v-person-list-item__stack-item"><div class="nova-v-person-list-item__align"><div class="nova-v-person-list-item__align-content"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp nova-v-person-list-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2139240686_Matthew_Schlesinger">Matthew Schlesinger</a></div></div></div></div></div><footer class="nova-v-person-list-item__footer"><div class="nova-v-person-list-item__footer-metrics"></div></footer></div><div class="nova-l-flex__item nova-l-flex__item--shrink"><div class="nova-v-person-list-item__align"><div class="nova-v-person-list-item__align-content"></div></div></div></div></div></div></div></li></ul><div class="nova-c-card nova-c-card--spacing-m nova-c-card--elevation-none"><div class="nova-c-card__header nova-c-card__header--spacing-inherit"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-700">Abstract</div></div><div class="nova-c-card__body nova-c-card__body--spacing-inherit"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-auto nova-e-text--color-inherit" itemProp="description">Developmental robotics is a collaborative and interdisciplinary approach to robotics that is directly inspired by the developmental principles and mechanisms observed in children’s cognitive development. It builds on the idea that the robot, using a set of intrinsic developmental principles regulating the real-time interaction of its body, brain, and environment, can autonomously acquire an increasingly complex set of sensorimotor and mental capabilities. This volume, drawing on insights from psychology, computer science, linguistics, neuroscience, and robotics, offers the first comprehensive overview of a rapidly growing field. 

After providing some essential background information on robotics and developmental psychology, the book looks in detail at how developmental robotics models and experiments have attempted to realize a range of behavioral and cognitive capabilities. The examples in these chapters were chosen because of their direct correspondence with specific issues in child psychology research; each chapter begins with a concise and accessible overview of relevant empirical and theoretical findings in developmental psychology. The chapters cover intrinsic motivation and curiosity; motor development, examining both manipulation and locomotion; perceptual development, including face recognition and perception of space; social learning, emphasizing such phenomena as joint attention and cooperation; language, from phonetic babbling to syntactic processing; and abstract knowledge, including models of number learning and reasoning strategies. Boxed text offers technical and methodological details for both psychology and robotics experiments.</div></div></div><div id="lite-ad-fysh5ycky2q" class="lite-page-ad lite-page-ad--vertical-spacing-xl lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-fysh5ycky2q-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-fysh5ycky2q-close').onclick = function(e) {document.querySelector('#lite-ad-fysh5ycky2q').style.display = 'none';};</script><div id="fysh5ycky2q"><rg-ad id="z2e1x1vr88b" ad-unit="Instream_Video_Mobile" ad-unit-path="/83500759/Instream_Video_Mobile" refresh-timeout="0" load-on-init="true" collapse="never" class="rg-ad" ad-unit-sizes="[[1,1]]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="z2e1x1vr88b" targeting-sequence="1"><rg-ad-size size="[1,1]"></rg-ad-size></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("z2e1x1vr88b").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-fysh5ycky2q').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-fysh5ycky2q");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-fysh5ycky2q').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-fysh5ycky2q");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div><div class="signup-promo"><h2 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-l nova-e-text--color-inherit">Discover the world&#x27;s research</h2><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-unordered nova-e-list--spacing-auto"><li class="nova-e-list__item"><strong>17+ million</strong> members</li><li class="nova-e-list__item"><strong>135+ million</strong> publications</li><li class="nova-e-list__item"><strong>700k+</strong> research projects</li></ul><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-auto signup-promo__cta" href="signup.SignUp.html"><span class="nova-c-button__label">Join for free</span></a><picture class="signup-promo__image" width="102" height="102"><source data-srcset="images/icons/svgicons/promo-with-stats-icon.svg" type="image/svg+xml" width="102" height="102"/> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="images/icons/svgicons/promo-with-stats-icon.svg" class="lite-image" width="102" height="102"/></picture></div></section><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-none nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-center@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><div id="nativo-slot-1"><div class="rgntvpdf"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-m nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-center@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit ntv-disc">Advertisement</div></div></div><div class="ntv-stories"><div id="ntv-3-1"></div><div id="ntv-3-2" style="display:none"></div><div id="ntv-3-3" style="display:none"></div></div></div></div></div></div><div id="lite-ad-0n40oxixa22l" class="lite-page-ad lite-page-ad--has-label lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-0n40oxixa22l-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-0n40oxixa22l-close').onclick = function(e) {document.querySelector('#lite-ad-0n40oxixa22l').style.display = 'none';};</script><div id="0n40oxixa22l"><rg-ad id="2ht92dw6gb9" ad-unit="LoggedOut_PublicationWithoutFulltext_Middle" ad-unit-path="/83500759/LoggedOut_PublicationWithoutFulltext_Middle" ad-location="Middle" refresh-timeout="-1" load-on-init="false" collapse="never" class="rg-ad" ad-unit-sizes="[[728,90],[300,250],&quot;fluid&quot;]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="2ht92dw6gb9" targeting-sequence="1"><rg-ad-size size="[728,90]"></rg-ad-size><rg-ad-size size="[300,250]"></rg-ad-size><rg-ad-size size="&quot;fluid&quot;"></rg-ad-size><rg-ad-sizeset viewport-size="[0,0]" slot-sizes="[]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[250,0]" slot-sizes="[[250,250]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[300,0]" slot-sizes="[[250,250],[300,250],[300,100],[300,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[320,0]" slot-sizes="[[250,250],[300,250],[300,100],[300,50],[320,100],[320,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[336,0]" slot-sizes="[[336,280],[300,250],[250,250],[300,100],[300,50],[320,100],[320,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[992,0]" slot-sizes="[[728,90],&quot;fluid&quot;]"></rg-ad-sizeset></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("2ht92dw6gb9").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-0n40oxixa22l').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-0n40oxixa22l");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-0n40oxixa22l').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-0n40oxixa22l");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div><section class="publication-bottom"><section class="publication-details__section"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none publication-file-list"></div></section><section class="publication-details__section"><div class="js-target-citationReferences lite-tabs"><div style="position:relative;width:100%;height:100%" class="publication-citations__menubar"><nav class="nova-c-nav nova-c-nav--direction-horizontal nova-c-nav--theme-tabs nova-c-nav--spread-auto"><div class="nova-c-nav__wrapper"><div class="nova-c-nav__items" role="tablist"><button class="nova-c-nav__item is-selected citations js-lite-click" aria-disabled="false" aria-selected="true" role="tab" type="menu" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-citationReferences .lite-page-visible&quot;,&quot;a&quot;:&quot;rc(lite-page-visible)&quot;},{&quot;t&quot;:&quot;.js-target-citationReferences .is-selected&quot;,&quot;a&quot;:&quot;rc(is-selected)&quot;},{&quot;t&quot;:&quot;.js-target-citations&quot;,&quot;a&quot;:&quot;ac(lite-page-visible)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(is-selected)&quot;}]}" tabindex="-1"><div><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-c-nav__item-label"><h3 class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">Citations (208)</h3></div></div></button><button class="nova-c-nav__item references js-lite-click" aria-disabled="false" role="tab" type="menu" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-citationReferences .lite-page-visible&quot;,&quot;a&quot;:&quot;rc(lite-page-visible)&quot;},{&quot;t&quot;:&quot;.js-target-citationReferences .is-selected&quot;,&quot;a&quot;:&quot;rc(is-selected)&quot;},{&quot;t&quot;:&quot;.js-target-references&quot;,&quot;a&quot;:&quot;ac(lite-page-visible)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(is-selected)&quot;}]}"><div><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-c-nav__item-label"><h3 class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">References (0)</h3></div></div></button></div><div class="nova-c-nav__scroll-overflow" aria-hidden="true"><button class="nova-c-nav__scroll-overflow-trigger"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#arrow-right-s"></use></svg></button></div><div class="nova-c-nav__plus-overflow" aria-hidden="true"><button class="nova-c-nav__plus-overflow-trigger" tabindex="-1"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#ellipsis-s"></use></svg></button></div></div></nav><span><span><div class="nova-c-nav__plus-overflow-mount" aria-hidden="true" aria-haspopup="true" aria-expanded="false"></div></span></span></div><div class="lite-tabs__tab js-target-citations lite-page-visible"><div class="js-target-citation" id="citations"><div class="nova-o-stack nova-o-stack--gutter-s nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-bare nova-e-list--spacing-none publication-citations"><div><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Nous faisons ici le choix d&#x27;employer le terme de « motivation intrinsèque », celui-ciétant utilisé par les roboticiens dans les travaux dont nous nous inspirons (voir par ex. Baldassarre, 2011;<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi &amp; Schlesinger, 2015;</mark>. Nous choisissons de définir la motivation intrinsèque comme la réalisation d&#x27;une action pour son intérêt propre (« for its own sake », 2), autrement dit pour sa satisfaction inhérente (« for its inherent satisfactions », Ryan &amp; Deci, 2000a, p. 56), et non pour la satisfaction d&#x27;un besoin physiologique (Baldassarre, 2011) ou l&#x27;obtention d&#x27;une récompense (Ryan &amp; Deci, 2000a (Pavlov, 1927).  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Roboticists have recently also become interested in how infants explore their body and the world around them, since mimicking how babies learn may be a way to develop autonomous agents capable of open-ended learning (see, e.g., <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi &amp; Schlesinger, 2015)</mark>. This paper focuses on an underlying ability needed for this exploration to be effective, namely the ability to detect when an action is associated with an outcome -in other words the ability to detect &quot;sensorimotor contingencies&quot;.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Indeed, the main source of learning used by development roboticians is the interaction between the agent and its environment. In this way, roboticians integrate into their algorithms contingency detection mechanisms similar to the ones observed in humans <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi &amp; Schlesinger, 2015)</mark>. This calls for the need for formalization of this mechanism and a precise understanding of its properties in immature agents such as babies.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/341426178_La_sensibilite_aux_contingences_sensorimotrices_chez_le_bebe_et_son_role_dans_le_developpement_du_savoir-faire_corporel_Approche_croisee_en_robotique_et_psychologie_du_developpement">La sensibilité aux contingences sensorimotrices chez le bébé et son rôle dans le développement du savoir-faire corporel: Approche croisée en robotique et psychologie du développement</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Thesis</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Nov 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Lisa_Jacquey"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/856964636606466-1581327699371_Q64/Lisa_Jacquey.jpg" class="lite-image" alt="Lisa Jacquey"/></picture></span><span class="nova-v-person-inline-item__fullname">Lisa Jacquey</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd53030e lite-page-hidden">Au regard de la place croissante occupée par l&#x27;intelligence artificielle dans nos sociétés, la nécessité d&#x27;affiner notre compréhension de la notion d&#x27;apprentissage semble plus importante que jamais. S&#x27;intégrant dans une telle optique, cette thèse de doctorat cherche à mettre en lumière les mécanismes d&#x27;apprentissage permettant au bébé d&#x27;acquérir au cours de la première année de vie une utilisation appropriée et différenciée de son corps lui permettant d&#x27;interagir de manière efficace avec son environnement physique et social, ce que nous désignons ici par le terme de &quot;savoir-fairecorporel&quot;. Le postulat au cœur de ce travail de recherche est le suivant : l&#x27;acquisition progressive du savoir-faire corporel au cours de la vie fœtale et des premiers mois de vie post-partum est sous-tendue par deux mécanismes d&#x27;apprentissage, l&#x27;exploitation de la sensibilité aux contingencessensorimotrices et la motivation intrinsèque. Ce travail de thèse explore la première partie de ce postulat, c&#x27;est-à-dire le rôle jouée par la sensibilité aux contingences sensorimotrices dans le développement du savoir-faire corporel. Afin d&#x27;investiguer cette hypothèse, ce travail de thèsese concentre dans un premier temps sur l&#x27;analyse critique des données expérimentales déjà existantes sur le sujet, à la fois en psychologie du développement et en robotique développementale. Dans un second temps, cette recherche vise à approfondir notre compréhension de l&#x27;acquisition du savoir-faire corporel à travers l&#x27;expérimentation chez le bébé âgé de moins d&#x27;un an.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/341426178_La_sensibilite_aux_contingences_sensorimotrices_chez_le_bebe_et_son_role_dans_le_developpement_du_savoir-faire_corporel_Approche_croisee_en_robotique_et_psychologie_du_developpement"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd53030e&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Today, this is still a challenge due to the complexity of the dynamic, non-standardized environments and tasks involved. A promising approach for coping with this complexity is to take inspiration from biological systems and develop neurocognitive learning models embodied in developmental robots <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark> that learn, similar to a human child or infant, from interaction with the environment and imitation of, or teaching by, adult experts. A spectrum of such learning approaches exists in the literature, ranging from relying entirely on the imitation of a human teacher to nearly autodidactic approaches without any human assistance.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... However, these approaches often have difficulties adjusting to novel challenges due to their lack of inherent learning ability. A complementary approach is to learn visuomotor skills through interaction with the environment <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. Deep reinforcement approaches employ trial and error learning.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Considering the challenging nature of many real-world robotic tasks, the ability and willingness of non-expert users to aid in the necessary learning process is an important resource. The presented results are in line with the concept of developmental robotics <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>: when looking at early human development, social interaction, especially scaffolding provided by caretakers, is critical for the development of cognitive abilities.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342057541_Teaching_NICO_How_to_Grasp_An_Empirical_Study_on_Crossmodal_Social_Interaction_as_a_Key_Factor_for_Robots_Learning_From_Humans">Teaching NICO How to Grasp: An Empirical Study on Crossmodal Social Interaction as a Key Factor for Robots Learning From Humans</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jun 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Theresa_Pekarek_Rosin"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="Theresa Pekarek Rosin"/></picture></span><span class="nova-v-person-inline-item__fullname">Theresa Pekarek Rosin</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Erik_Strahl"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/277050150670344-1443065311805_Q64/Erik_Strahl.jpg" class="lite-image" alt="Erik Strahl"/></picture></span><span class="nova-v-person-inline-item__fullname">Erik Strahl</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Matthias_Kerzel"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/638785058529281-1529309634390_Q64/Matthias_Kerzel.jpg" class="lite-image" alt="Matthias Kerzel"/></picture></span><span class="nova-v-person-inline-item__fullname">Matthias Kerzel</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Stefan_Wermter"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/312714742697984-1451568412045_Q64/Stefan_Wermter.jpg" class="lite-image" alt="Stefan Wermter"/></picture></span><span class="nova-v-person-inline-item__fullname">Stefan Wermter</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52f6ee lite-page-hidden">To overcome novel challenges in complex domestic environments, humanoid robots can learn from human teachers. We propose that the capability for social interaction should be a key factor in this teaching process and benefits both the subjective experience of the human user and the learning process itself. To support our hypothesis, we present a Human-Robot Interaction study on human-assisted visuomotor learning with the robot NICO, the Neuro-Inspired COmpanion, a child-sized humanoid. NICO is a flexible, social platform with sensing and manipulation abilities. We give a detailed description of NICO&#x27;s design and a comprehensive overview of studies that use or evaluate NICO. To engage in social interaction, NICO can express stylized facial expressions and utter speech via an Embodied Dialogue System. NICO is characterized in particular by combining these social interaction capabilities with the abilities for human-like object manipulation and crossmodal perception. In the presented study, NICO acquires visuomotor grasping skills by interacting with its environment. In contrast to methods like motor babbling, the learning process is, in part, supported by a human teacher. To begin the learning process, an object is placed into NICO&#x27;s hand, and if this object is accidentally dropped, the human assistant has to recover it. The study is conducted with 24 participants with little or no prior experience with robots. In the robot-guided experimental condition, assistance is actively requested by NICO via the Embodied Dialogue System. In the human-guided condition, instructions are given by a human experimenter, while NICO remains silent. Evaluation using established questionnaires like Godspeed, Mind Perception, and Uncanny Valley Indices, along with a structured interview and video analysis of the interaction, show that the robot&#x27;s active requests for assistance foster the participant&#x27;s engagement and benefit the learning process. This result supports the hypothesis that the ability for social interaction is a key factor for companion robots that learn with the help of non-expert teachers, as these robots become capable of communicating active requests or questions that are vital to their learning process. We also show how the design of NICO both enables and is driven by this approach.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342057541_Teaching_NICO_How_to_Grasp_An_Empirical_Study_on_Crossmodal_Social_Interaction_as_a_Key_Factor_for_Robots_Learning_From_Humans"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52f6ee&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... One reason seems to be the lack of physical body in AI. We reason that the physical body is very important in the development of human-like intelligence <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. This fact is a strong motivation for the premise of using robots in this research.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... On the other hand, developmental robotics is a research area that emphasizes the physical body <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. In the context of developmental robotics, various aspects of development, such as language learning (Morse and Cangelosi, 2017), motor learning (Billard, 2000;Demiris and Khadhouri, 2006), and affordance learning (Stoytchev, 2008;Jamone et al., 2018) are realized using robots.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/337643066_Integrated_Cognitive_Architecture_for_Robot_Learning_of_Action_and_Language">Integrated Cognitive Architecture for Robot Learning of Action and Language</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Nov 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2167312373_Kazuki_Miyazawa"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Kazuki Miyazawa</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Takato_Horii"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/378619366002692-1467281299704_Q64/Takato_Horii.jpg" class="lite-image" alt="Takato Horii"/></picture></span><span class="nova-v-person-inline-item__fullname">Takato Horii</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2117794322_Tatsuya_Aoki"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Tatsuya Aoki</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Takayuki_Nagai"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="Takayuki Nagai"/></picture></span><span class="nova-v-person-inline-item__fullname">Takayuki Nagai</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd532fff lite-page-hidden">The manner in which humans learn, plan, and decide actions is a very compelling subject. Moreover, the mechanism behind high-level cognitive functions, such as action planning, language understanding, and logical thinking, has not yet been fully implemented in robotics. In this paper, we propose a framework for the simultaneously comprehension of concepts, actions, and language as a first step toward this goal. This can be achieved by integrating various cognitive modules and leveraging mainly multimodal categorization by using multilayered multimodal latent Dirichlet allocation (mMLDA). The integration of reinforcement learning and mMLDA enables actions based on understanding. Furthermore, the mMLDA, in conjunction with grammar learning and based on the Bayesian hidden Markov model (BHMM), allows the robot to verbalize its own actions and understand user utterances. We verify the potential of the proposed architecture through experiments using a real robot.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/337643066_Integrated_Cognitive_Architecture_for_Robot_Learning_of_Action_and_Language"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd532fff&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... The aim of RL is to endow an autonomous agent with the ability to learn a new skill by interacting with the environment. While RL has been shown to be an effective learning approach in diverse areas such as human cognition [10,29], developmental robotics <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[3,</mark>5], and video games [16,44], among others, an open issue is the lack of a mechanism that allows the agent to clearly communicate the reasons why it chooses certain actions given a particular state. Therefore, it is not easy for a person to entrust important tasks to an AI-based system (e.g.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Nowadays, RL is studied as a decision-making mechanism in both cognitive and artificial agents <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[3]</mark>. In RL, there is no explicit instructor but rather the awareness of how the environment responds to actions performed by the learning agent.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342435632_Explainable_robotic_systems_Interpreting_outcome-focused_actions_in_a_reinforcement_learning_scenario">Explainable robotic systems: Interpreting outcome-focused actions in a reinforcement learning scenario</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jun 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2167031880_Francisco_Cruz"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Francisco Cruz</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Richard_Dazeley"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272659330105375-1442018458682_Q64/Richard_Dazeley.jpg" class="lite-image" alt="Richard Dazeley"/></picture></span><span class="nova-v-person-inline-item__fullname">Richard Dazeley</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Peter_Vamplew"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/568836671930369-1512632639962_Q64/Peter_Vamplew.jpg" class="lite-image" alt="Peter Vamplew"/></picture></span><span class="nova-v-person-inline-item__fullname">Peter Vamplew</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52ed86 lite-page-hidden">Robotic systems are more present in our society every day. In human-robot interaction scenarios, it is crucial that end-users develop trust in their robotic team-partners, in order to collaboratively complete a task. To increase trust, users demand more understanding about the decisions by the robot in particular situations. Recently, explainable robotic systems have emerged as an alternative focused not only on completing a task satisfactorily but also in justifying, in a human-like manner, the reasons that lead to making a decision. In reinforcement learning scenarios, a great effort has been focused on providing explanations from the visual input modality, particularly using deep learning-based approaches. In this work, we focus on the decision-making process of a reinforcement learning agent performing a navigation task in a robotic scenario. As a way to explain the robot&#x27;s behavior, we use the probability of success computed by three different proposed approaches: memory-based, learning-based, and phenomenological-based. The difference between these approaches is the additional memory required to compute or estimate the probability of success as well as the kind of reinforcement learning representation where they could be used. In this regard, we use the memory-based approach as a baseline since it is obtained directly from the agent&#x27;s observations. When comparing the learning-based and the phenomenological-based approaches to this baseline, both are found to be suitable alternatives to compute the probability of success, obtaining high levels of similarity when compared using both the Pearson&#x27;s correlation and the mean squared error.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342435632_Explainable_robotic_systems_Interpreting_outcome-focused_actions_in_a_reinforcement_learning_scenario"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52ed86&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... We surmise that interaction might be a crucial factor in enhancing acceptance of robot use by establishing and calibrating expectancies. Evidence for the importance of social interaction history as a factor in human-robot relationships comes from developmental robotics studies (reviewed in <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi &amp; Schlesinger, 2015)</mark>. This effect is reminiscent of Allport&#x27;s (1954Allport&#x27;s ( /2012 contact hypothesis, an influential theory postulating that intergroup contacts reduce prejudice and enhance attitudes toward a foreign group.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... The typical outcome is that children are motivated when interacting with social robots, especially when they appear weaker than the children themselves, thereby not frightening the children (see, for example, Tanaka &amp; Kimura, 2009). Thus, developmental robotics is an emerging field which combines different areas of research such as child psychology, neuroscience, and robotics (for an overview, see <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi &amp; Schlesinger, 2015)</mark>. Nevertheless, ethical implications in the use of social robots with children always have to be taken into account carefully, especially the longterm effects which might appear when social robots are used as learning assistants for toddlers.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/334518172_The_Use_of_Social_Robots_and_the_Uncanny_Valley_Phenomenon">The Use of Social Robots and the Uncanny Valley Phenomenon</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jul 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Melinda_Mende"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/690678216552448-1541681927105_Q64/Melinda_Mende.jpg" class="lite-image" alt="Melinda Mende"/></picture></span><span class="nova-v-person-inline-item__fullname">Melinda Mende</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Katharina_Kuehne2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/367518507126785-1464634648316_Q64/Katharina_Kuehne2.jpg" class="lite-image" alt="Katharina Kühne"/></picture></span><span class="nova-v-person-inline-item__fullname">Katharina Kühne</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Martin_Fischer2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272449614905356-1441968458206_Q64/Martin_Fischer2.jpg" class="lite-image" alt="Martin H Fischer"/></picture></span><span class="nova-v-person-inline-item__fullname">Martin H Fischer</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd535289 lite-page-hidden">Social robots are increasingly used in different areas of society such as public health, elderly care, education, and commerce. They have also been successfully employed in autism spectrum disorders therapy with children. Humans strive to find in them not only assistants but also friends. Although forms and functionalities of such robots vary, there is a strong tendency to anthropomorphize artificial agents, making them look and behave as human as possible and imputing human attributes to them. The more human a robot looks, the more appealing it will be considered by humans. However, this linear link between likeness and liking only holds to the point where a feeling of strangeness and eeriness emerges. We discuss possible explanations of this so-called uncanny valley phenomenon that emerges in human–robot interaction. We also touch upon important ethical questions surrounding human–robot interaction in different social settings, such as elderly care or autism spectrum disorders therapy.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/334518172_The_Use_of_Social_Robots_and_the_Uncanny_Valley_Phenomenon"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd535289&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... While developmental robotics has made a number of advances in recent years (e.g., Asada et al., 2009;Breazeal &amp; Scassellati, 2002;<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi &amp; Schlesinger, 2015;</mark>Lungarella, Metta, Pfeifer, &amp; Sandini, 2003;Minato, Thomas, Yoshikawa, &amp; Ishiguro, 2010;Scassellati et al., 2006;Silva, Correia, &amp; Lyhne Christensen, 2017), none of these advances can compare to the mastery of cultural behavior and the understanding of self and others as persons that humans acquire during development. Some success has occurred for earlier stages of development.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... ln any event it is difficult for me even to imagine criteria by which to evaluate success in these endeavors at this point in time. And I can&#x27;t find any social robotic studies that even attempt to approach the problem (Breazeal, 2001;<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi &amp; Schlesinger, 2015;</mark>Lake, Ullman, Tenenbaum, &amp; Gershman, 2017;Lallee et al., 2015;Pfeifer &amp; Bongard, 2006).  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/335082599_On_building_a_person_benchmarks_for_robotic_personhood">On building a person: benchmarks for robotic personhood</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Aug 2019</span></li><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="journal/0952-813X_Journal_of_Experimental_Theoretical_Artificial_Intelligence">J EXP THEOR ARTIF IN</a></span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/John_Barresi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272501099986955-1441980733471_Q64/John_Barresi.jpg" class="lite-image" alt="John Barresi"/></picture></span><span class="nova-v-person-inline-item__fullname">John Barresi</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd534a6e lite-page-hidden">I present criteria or benchmarks of personhood that robotic agents must pass for them to match us in social intelligence and indicate some
current successes and difficulties for the future with respect to these
criteria. I focus on five problematic areas: 1) How can inorganic material
substances mimic organic processes?; 2) How can robots evolve adaptations similar to those of human persons?; 3) How can robots develop into adult personhood?; 4) How can social identities of robots transform in an ever evolving robotic society?; 5) How can robots display intellectual and moral ideals that can advance knowledge and social goals?</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/335082599_On_building_a_person_benchmarks_for_robotic_personhood"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd534a6e&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... The main framework available to the robotics community in order to tackle open-ended learning scenarios in an incremental manner is that of Developmental Robotics (DR) [7]. DR studies different approaches so that robots can autonomously acquire an increasingly complex set of sensorimotor and mental capabilities through the interaction between their bodies and brains in the sequence of domains they encounter in their lifetime <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[8]</mark>. This field is inspired by the development of experience and motor 2 skills in humans, from childhood to adulthood.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... This implies that morphologies with less legs are able to combine body twist and leg reach in order to provide longer strides, thus making them able to walk farther in the same amount of time. <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">8</mark> However, if we change these results into relative values, understanding relative values as the absolute values shown in Fig. 4 left, divided by the maximum fitness value reached at the end of the learning process for each experiment expressed as a percentage, the graph changes remarkably (Fig. 4 right). Although the statistical dispersion varies significantly from one growth ratio to another, it can be seen how the values of the median are quite similar, being 9.5274% the maximum difference, for growth up to generation 80. Fig 4 left and Fig 4 right, seem to lead to the conclusion that the maximum fitness achievable and thus, the fitness increase margin is dependent on the constraints the morphology imposes and not on the morphological development strategies.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/339898577_Some_Experiments_on_the_influence_of_Problem_Hardness_in_Morphological_Development_based_Learning_of_Neural_Controllers">Some Experiments on the influence of Problem Hardness in Morphological Development based Learning of Neural Controllers</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Mar 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Martin_Naya"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/466431264464896-1488217287095_Q64/Martin_Naya.jpg" class="lite-image" alt="Martin Naya"/></picture></span><span class="nova-v-person-inline-item__fullname">Martin Naya</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Andres_Faina"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/525483636948993-1502296470688_Q64/Andres_Faina.jpg" class="lite-image" alt="Andres Faiña"/></picture></span><span class="nova-v-person-inline-item__fullname">Andres Faiña</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Richard_Duro"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272217246269465-1441913057338_Q64/Richard_Duro.jpg" class="lite-image" alt="Richard Duro"/></picture></span><span class="nova-v-person-inline-item__fullname">Richard Duro</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd530908 lite-page-hidden">Natural beings undergo a morphological development process of their bodies while they are learning and adapting to the environments they face from infancy to adulthood. In fact, this is the period where the most important learning pro-cesses, those that will support learning as adults, will take place. However, in artificial systems, this interaction between morphological development and learning, and its possible advantages, have seldom been considered. In this line, this paper seeks to provide some insights into how morphological development can be harnessed in order to facilitate learning in em-bodied systems facing tasks or domains that are hard to learn. In particular, here we will concentrate on whether morphological development can really provide any advantage when learning complex tasks and whether its relevance towards learning in-creases as tasks become harder. To this end, we present the results of some initial experiments on the application of morpho-logical development to learning to walk in three cases, that of a quadruped, a hexapod and that of an octopod. These results seem to confirm that as task learning difficulty increases the application of morphological development to learning becomes more advantageous.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/339898577_Some_Experiments_on_the_influence_of_Problem_Hardness_in_Morphological_Development_based_Learning_of_Neural_Controllers"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd530908&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Instead, it would be preferable if the system could continuously self-adapt to changes in the environment, e.g. a different layout of furniture that is required to compensate for mobility challenges of residents. Life-long learning is also relevant in many other domains of applications ranging from cognitive robotics <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi et al. 2015)</mark>, self driving cars or surveillance systems. Due to the incremental nature of the proposed model, the model is applicable in life-long learning scenarios even in conditions such as concept drift that are often associated with life-long learning.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... In this section we briefly introduce the concept of intrinsic motivation with regard to curiosity driven exploration. A comprehensive conceptual overview on how intrinsic motivation is applied in cognitive robotics as an inductive signal that drives the learning process can be found in <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi et al. (2015)</mark>.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/341654093_Learning_action_models_by_curiosity_driven_self_exploration_and_language_guided_generalization">Learning action models by curiosity driven self exploration and language guided generalization</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Thesis</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">May 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Maximilian_Panzner"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/304079211827204-1449509541980_Q64/Maximilian_Panzner.jpg" class="lite-image" alt="Maximilian Panzner"/></picture></span><span class="nova-v-person-inline-item__fullname">Maximilian Panzner</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5300c5 lite-page-hidden">Most theories and models of language acquisition so far have adopted a ‘mapping’ paradigm according to which novel words or constructions are ‘mapped’ onto existing, priorly acquired or innate concepts. Departing from this mapping approach, the thesis develops a computational model of the co-emergence of linguistic and conceptual structures with a particular focus on the case of action verbs. The model is inspired by emergentist theories of language acquisition and transfers the underlying ideas also to the domain of action learning. The emergentist cross-modal learning process spells out how a learner can distill the essence of the meaning of a verbal construction as a process of incremental generalization of the meaning of action verbs, starting from a meaning that is specific to a certain situation in which the verb has been encountered. The meaning of action verbs is understood as evoking a grounded simulation rather than a static concept. We show that cross-modal learning can provide an advantage over uni-modal models especially when observations are ambiguous and hard to differentiate. The connection between the theoretical foundation and the technical implementation is bidirectional. On the one hand, the technical properties of the model such as the fully incremental and data-driven approach to learn concepts within a modality that are grounded in concepts with similar semantics of another modality are relevant in many technical applications that are common in human-computer interaction and robotics. On the other hand technical implementations that are closely based on key concepts of theoretical frameworks can as well serve as computer implemented theories that allow other researchers to interactively test hypothesis that arise from the theoretical foundation. The thesis details connections to ongoing interdisciplinary research in linguistics and cognitive sciences.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/341654093_Learning_action_models_by_curiosity_driven_self_exploration_and_language_guided_generalization"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5300c5&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Models for development and language acquisition in the human brain must consider the fact that a child has acquired several sensory and motor skills before it makes use of language. In fact, a child develops the ability to learn concepts from its perceptions and active interactions and refers to these concepts during communication in natural language <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[1]</mark>. For the human brain, it is overall seen plausible that concepts and thus language is represented in a network which is distributed over large parts of the cortex and comprises multiple sensory and motor areas that are not primarily language-related [2], [3].  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Particularly successful applications have been shown in knowledge inference [9] and question answering [10]. Based on the inspiration from the brain, we can adopt a distributed representation for entities but at the same time contribute insights into concept learning because the KG approach allows for studying a much larger setting than currently possible in developmental research <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[1]</mark>.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336250917_Integrating_Image-based_and_Knowledge-based_Representation_Learning">Integrating Image-based and Knowledge-based Representation Learning</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Apr 2019</span></li><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="journal/2379-8920_IEEE_Transactions_on_Cognitive_and_Developmental_Systems">TCDS</a></span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2163377975_Ruobing_Xie"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Ruobing Xie</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Zhiyuan_Liu"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/647976968024066-1531501156358_Q64/Zhiyuan_Liu.jpg" class="lite-image" alt="Zhiyuan Liu"/></picture></span><span class="nova-v-person-inline-item__fullname">Zhiyuan Liu</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/70737274_Maosong_Sun"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Maosong Sun</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Stefan_Heinrich"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/860086712942592-1582072060346_Q64/Stefan_Heinrich.jpg" class="lite-image" alt="Stefan Heinrich"/></picture></span><span class="nova-v-person-inline-item__fullname">Stefan Heinrich</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd533b4b lite-page-hidden">A variety of brain areas is involved in language understanding and generation, accounting for the scope of language that can refer to many real-world matters. In this work, we investigate how regularities among real-world entities impact on emergent language representations. Specifically, we consider knowledge bases, which represent entities and their relations as structured triples, and image representations, which are obtained via deep convolutional networks. We combine these sources of information to learn representations of an Image-based Knowledge Representation Learning model (IKRL). An attention mechanism lets more informative images contribute more to the image-based representations. Evaluation results show that the model outperforms all baselines on the tasks of knowledge graph completion and triple classification. In analysing the learned models we found that the structure-based and image-based representations integrate different aspects of the entities and the attention mechanism provides robustness during learning.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336250917_Integrating_Image-based_and_Knowledge-based_Representation_Learning"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd533b4b&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Besides, we consider the acquisition of new knowledge representations as a challenge per se, that may require specific processes that are not necessarily task-oriented. As human infants, the proposed approach needs to face the challenges of understanding the robot&#x27;s environment and its own capabilities [34,35,36,<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">37]</mark>.  ...</div></div></div></div><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... The decomposition of the learning process into different phases is a feature of developmental robotics [34,35,<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">37]</mark>, that draws inpiration from human and animal development. In these approaches, the robot is not ready to solve a task when it is first turned on.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/341369189_DREAM_Architecture_a_Developmental_Approach_to_Open-Ended_Learning_in_Robotics">DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">May 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Stephane_Doncieux"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272463351250946-1441971733318_Q64/Stephane_Doncieux.jpg" class="lite-image" alt="Stéphane Doncieux"/></picture></span><span class="nova-v-person-inline-item__fullname">Stéphane Doncieux</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Nicolas_Bredeche"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272410838564917-1441959213908_Q64/Nicolas_Bredeche.jpg" class="lite-image" alt="Nicolas Bredeche"/></picture></span><span class="nova-v-person-inline-item__fullname">Nicolas Bredeche</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2122326604_Leni_K_Le_Goff"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Leni K. Le Goff</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Richard_Duro"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272217246269465-1441913057338_Q64/Richard_Duro.jpg" class="lite-image" alt="Richard Duro"/></picture></span><span class="nova-v-person-inline-item__fullname">Richard Duro</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5304de lite-page-hidden">Robots are still limited to controlled conditions, that the robot designer knows with enough details to endow the robot with the appropriate models or behaviors. Learning algorithms add some flexibility with the ability to discover the appropriate behavior given either some demonstrations or a reward to guide its exploration with a reinforcement learning algorithm. Reinforcement learning algorithms rely on the definition of state and action spaces that define reachable behaviors. Their adaptation capability critically depends on the representations of these spaces: small and discrete spaces result in fast learning while large and continuous spaces are challenging and either require a long training period or prevent the robot from converging to an appropriate behavior. Beside the operational cycle of policy execution and the learning cycle, which works at a slower time scale to acquire new policies, we introduce the redescription cycle, a third cycle working at an even slower time scale to generate or adapt the required representations to the robot, its environment and the task. We introduce the challenges raised by this cycle and we present DREAM (Deferred Restructuring of Experience in Autonomous Machines), a developmental cognitive architecture to bootstrap this redescription process stage by stage, build new state representations with appropriate motivations, and transfer the acquired knowledge across domains or tasks or even across robots. We describe results obtained so far with this approach and end up with a discussion of the questions it raises in Neuroscience.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/341369189_DREAM_Architecture_a_Developmental_Approach_to_Open-Ended_Learning_in_Robotics"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5304de&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... An innovative approach for studying the embodied learning is cognitive developmental robotics (CDR), which was defined as the &quot;interdisciplinary approach to the autonomous design of behavioural and cognitive capabilities in artificial agents (robots) that takes direct inspiration from the developmental principles and mechanisms observed in natural cognitive systems (children)&quot; <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">31</mark> . The application of embodied theory in artificial agents is among the motivations for designing new robotic platforms for research to resemble the shape of a human body, known as &#x27;humanoids&#x27; , such as ASIMO 32 , and in particular that of a child, notably iCub 33 .  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/337876291_Developing_the_knowledge_of_number_digits_in_a_child-like_robot">Developing the knowledge of number digits in a child-like robot</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Dec 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2167841258_James_L_McClelland"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">James L. McClelland</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Alessandro_Di_Nuovo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272455281410087-1441969809775_Q64/Alessandro_Di_Nuovo.jpg" class="lite-image" alt="Alessandro Di Nuovo"/></picture></span><span class="nova-v-person-inline-item__fullname">Alessandro Di Nuovo</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5327c4 lite-page-hidden">Number knowledge can be boosted initially by embodied strategies such as the use of fingers. This Article explores the perceptual process of grounding number symbols in artificial agents, particularly the iCub robot—a child-like humanoid with fully functional, five-fingered hands. It studies the application of convolutional neural network models in the context of cognitive developmental robotics, where the training information is likely to be gradually acquired while operating, rather than being abundant and fully available as in many machine learning scenarios. The experimental analyses show increased efficiency of the training and similarities with studies in developmental psychology. Indeed, the proprioceptive information from the robot hands can improve accuracy in the recognition of spoken digits by supporting a quicker creation of a uniform number line. In conclusion, these findings reveal a novel way for the humanization of artificial training strategies, where the embodiment can make the robot’s learning more efficient and understandable for humans. Number processing is linked to bodily systems, especially finger movements. The authors apply convolutional neural network models in the context of cognitive developmental robotics. They show that proprioceptive information in the child-like robot iCub improves accuracy and recognition of spoken digits.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/337876291_Developing_the_knowledge_of_number_digits_in_a_child-like_robot"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5327c4&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Tangiuchi et al. (2019) summarized their studies and related works on cognitive developmental robotics that can learn a language from interaction with their environment and unsupervised learning methods that enable robots to learn a language without hand-crafted training data. As studies on developmental robotics <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2014)</mark>, Cangelosi and his group have proposed computational models for an iCub humanoid robot to ground action words through embodied communications (Marocco et al., 2010;Stramandinoli et al., 2017;Taniguchi et al., 2017;Zhong et al., 2019). Marocco et al. (2010) proposed a computational model that enables the iCub humanoid robot to learn the meaning of action words by physically interacting with the environment and linking the effects of actions with the behavior observed on an object before and after the action.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/337860529_Symbol_Emergence_as_an_Interpersonal_Multimodal_Categorization">Symbol Emergence as an Interpersonal Multimodal Categorization</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Dec 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Yoshinobu_Hagiwara"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273782359195650-1442286209102_Q64/Yoshinobu_Hagiwara.jpg" class="lite-image" alt="Yoshinobu Hagiwara"/></picture></span><span class="nova-v-person-inline-item__fullname">Yoshinobu Hagiwara</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2139895506_Hiroyoshi_Kobayashi"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Hiroyoshi Kobayashi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Akira_Taniguchi2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/421315027718145-1477460738439_Q64/Akira_Taniguchi2.jpg" class="lite-image" alt="Akira Taniguchi"/></picture></span><span class="nova-v-person-inline-item__fullname">Akira Taniguchi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Tadahiro_Taniguchi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/277845675921468-1443254979450_Q64/Tadahiro_Taniguchi.jpg" class="lite-image" alt="Tadahiro Taniguchi"/></picture></span><span class="nova-v-person-inline-item__fullname">Tadahiro Taniguchi</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd532a11 lite-page-hidden">This study focuses on category formation for individual agents and the dynamics of symbol emergence in a multi-agent system through semiotic communication. In this study, the semiotic communication refers to exchanging signs composed of the signifier (i.e., words) and the signified (i.e., categories). We define the generation and interpretation of signs associated with the categories formed through the agent&#x27;s own sensory experience or by exchanging signs with other agents as basic functions of the semiotic communication. From the viewpoint of language evolution and symbol emergence, organization of a symbol system in a multi-agent system (i.e., agent society) is considered as a bottom-up and dynamic process, where individual agents share the meaning of signs and categorize sensory experience. A constructive computational model can explain the mutual dependency of the two processes and has mathematical support that guarantees a symbol system&#x27;s emergence and sharing within the multi-agent system. In this paper, we describe a new computational model that represents symbol emergence in a two-agent system based on a probabilistic generative model for multimodal categorization. It models semiotic communication via a probabilistic rejection based on the receiver&#x27;s own belief. We have found that the dynamics by which cognitively independent agents create a symbol system through their semiotic communication can be regarded as the inference process of a hidden variable in an interpersonal multimodal categorizer, i.e., the complete system can be regarded as a single agent performing multimodal categorization using the sensors of all agents, if we define the rejection probability based on the Metropolis-Hastings algorithm. The validity of the proposed model and algorithm for symbol emergence, i.e., forming and sharing signs and categories, is also verified in an experiment with two agents observing daily objects in the real-world environment. In the experiment, we compared three communication algorithms: no communication, no rejection, and the proposed algorithm. The experimental results demonstrate that our model reproduces the phenomena of symbol emergence, which does not require a teacher who would know a pre-existing symbol system. Instead, the multi-agent system can form and use a symbol system without having pre-existing categories.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/337860529_Symbol_Emergence_as_an_Interpersonal_Multimodal_Categorization"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd532a11&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... We are aware that remarkable work has already linked data from developmental robotics and developmental psychology (e.g., Guerin et al., 2013;Caligiore et al., 2014a;<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi and Schlesinger, 2015;</mark>Jamone et al., 2016;Oudeyer et al., 2016;Caligiore and Baldassarre, 2019). The novelty of our approach is to focus more precisely on how sensorimotor contingencies are exploited (since as such it has not been a main focus of developmental robotics) and to illustrate with four examples how this sensitivity contributes to development.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/337732920_Sensorimotor_Contingencies_as_a_Key_Drive_of_Development_From_Babies_to_Robots">Sensorimotor Contingencies as a Key Drive of Development: From Babies to Robots</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Dec 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Lisa_Jacquey"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/856964636606466-1581327699371_Q64/Lisa_Jacquey.jpg" class="lite-image" alt="Lisa Jacquey"/></picture></span><span class="nova-v-person-inline-item__fullname">Lisa Jacquey</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Gianluca_Baldassarre"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273803989221394-1442291366879_Q64/Gianluca_Baldassarre.jpg" class="lite-image" alt="Gianluca Baldassarre"/></picture></span><span class="nova-v-person-inline-item__fullname">Gianluca Baldassarre</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Vieri_Santucci"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272446259462193-1441967658892_Q64/Vieri_Santucci.jpg" class="lite-image" alt="Vieri Giuliano Santucci"/></picture></span><span class="nova-v-person-inline-item__fullname">Vieri Giuliano Santucci</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/J_Oregan"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="J. Kevin O’Regan"/></picture></span><span class="nova-v-person-inline-item__fullname">J. Kevin O’Regan</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd532b5e lite-page-hidden">Much current work in robotics focuses on the development of robots capable of autonomous unsupervised learning. An essential prerequisite for such learning to be possible is that the agent should be sensitive to the link between its actions and the consequences of its actions, called sensorimotor contingencies. This sensitivity, and more particularly its role as a key drive of development, has been widely studied by developmental psychologists. However, the results of these studies may not necessarily be accessible or intelligible to roboticians. In this paper, we review the main experimental data demonstrating the role of sensitivity to sensorimotor contingencies in infants&#x27; acquisition of four fundamental motor and cognitive abilities: body knowledge, memory, generalization, and goal-directedness. We relate this data from developmental psychology to work in robotics, highlighting the links between these two domains of research. In the last part of the article we present a blueprint architecture demonstrating how exploitation of sensitivity to sensorimotor contingencies, combined with the notion of &quot;goal,&quot; allows an agent to develop new sensorimotor skills. This architecture can be used to guide the design of specific computational models, and also to possibly envisage new empirical experiments.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/337732920_Sensorimotor_Contingencies_as_a_Key_Drive_of_Development_From_Babies_to_Robots"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd532b5e&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... This objective has been strengthened by a new interdisciplinary approach to robotics, i.e. Developmental Robotics <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. For example, Vinanzi et al. (2019) have proposed an artificial cognitive architecture to simulate human decision making in the robot by using concepts from developmental theories, such as Theory of Mind (ToM).  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/334502304_Shall_I_trust_you_From_child_human-robot_interaction_to_trusting_relationships">Shall I trust you? From child human-robot interaction to trusting relationships</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jul 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Cinzia_Di_Dio"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/352570078973953-1461070665386_Q64/Cinzia_Di_Dio.jpg" class="lite-image" alt="Cinzia Di Dio"/></picture></span><span class="nova-v-person-inline-item__fullname">Cinzia Di Dio</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Federico_Manzi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/664543986462723-1535451041907_Q64/Federico_Manzi.jpg" class="lite-image" alt="Federico Manzi"/></picture></span><span class="nova-v-person-inline-item__fullname">Federico Manzi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Giulia_Peretti"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/702967627919363-1544611951687_Q64/Giulia_Peretti.jpg" class="lite-image" alt="Giulia Peretti"/></picture></span><span class="nova-v-person-inline-item__fullname">Giulia Peretti</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Antonella_Marchetti"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/326859662348290-1454940824962_Q64/Antonella_Marchetti.jpg" class="lite-image" alt="Antonella Marchetti"/></picture></span><span class="nova-v-person-inline-item__fullname">Antonella Marchetti</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd535803 lite-page-hidden">Studying trust within human-robot interaction is of great importance given the social relevance of robotic agents in a variety of contexts. We investigated the acquisition, loss and restoration of trust when preschool and school-age children played with either a human or a humanoid robot in-vivo. The relationship between trust and the quality of attachment relationships, Theory of Mind, and executive function skills was also investigated. No differences were found in children’s trust in the play-partner as a function of agency (human or robot). Nevertheless, 3-years-olds showed a trend toward trusting the human more than the robot, while 7-years-olds displayed the reverse behavioral pattern, thus highlighting the developing interplay between affective and cognitive correlates of trust.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/334502304_Shall_I_trust_you_From_child_human-robot_interaction_to_trusting_relationships"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd535803&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Adaptive learning and emergence of integrative cognitive system that involve not only low-level but also high-level cognitive capabilities are crucially important in robotics (Cangelosi et al., 2010;<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi and Schlesinger, 2015;</mark>Ugur and Piater, 2015;Tani, 2016;Taniguchi et al., 2016Taniguchi et al., , 2018. Recent advancement in machine learning methods, e.g., deep learning and hierarchical Bayesian modeling, enables us to develop cognitive systems that integrate multi-level sensory-motor and cognitive capabilities.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336719137_Editorial_Machine_Learning_Methods_for_High-Level_Cognitive_Capabilities_in_Robotics">Editorial: Machine Learning Methods for High-Level Cognitive Capabilities in Robotics</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Oct 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Emre_Ugur2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272478324916228-1441975303225_Q64/Emre_Ugur2.jpg" class="lite-image" alt="Emre Ugur"/></picture></span><span class="nova-v-person-inline-item__fullname">Emre Ugur</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Tetsuya_Ogata"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/687174005583874-1540846458230_Q64/Tetsuya_Ogata.jpg" class="lite-image" alt="Tetsuya Ogata"/></picture></span><span class="nova-v-person-inline-item__fullname">Tetsuya Ogata</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Yiannis_Demiris"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/280750613123073-1443947570259_Q64/Yiannis_Demiris.jpg" class="lite-image" alt="Yiannis Demiris"/></picture></span><span class="nova-v-person-inline-item__fullname">Yiannis Demiris</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Tadahiro_Taniguchi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/277845675921468-1443254979450_Q64/Tadahiro_Taniguchi.jpg" class="lite-image" alt="Tadahiro Taniguchi"/></picture></span><span class="nova-v-person-inline-item__fullname">Tadahiro Taniguchi</span></span></a></li></ul></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336719137_Editorial_Machine_Learning_Methods_for_High-Level_Cognitive_Capabilities_in_Robotics"><span class="nova-c-button__label">View</span></a></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Starting in developmental robotics (Lungarella et al., 2003;<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">Cangelosi and Schlesinger, 2015)</mark>, and gradually expanding into other fields, intrinsically motivated learning (sometimes called &quot;curiosity-driven learning&quot;) has been studied by many researchers as an approach to autonomous lifelong learning in machines (Oudeyer et al., 2007;Schmidhuber, 2010;Barto, 2013;Mirolli and Baldassarre, 2013). Inspired by the ability of humans and other mammals to discover how to produce &quot;interesting&quot; effects in the environment driven by self-generated motivational signals not related to specific tasks or instructions (White, 1959;Berlyne, 1960;Deci and Ryan, 1985), the research in the field of intrinsically motivated open-ended learning aims to develop agents that autonomously generate motivational signals (Merrick, 2010) to acquire repertoires of diverse skills that are likely to become useful later when specific &quot;extrinsic&quot; tasks need to be performed (e.g., Barto et al., 2004;Baldassarre, 2011;Baranes and Oudeyer, 2013;Kulkarni et al., 2016;Santucci et al., 2016).  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/338663463_Editorial_Intrinsically_Motivated_Open-Ended_Learning_in_Autonomous_Robots">Editorial: Intrinsically Motivated Open-Ended Learning in Autonomous Robots</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jan 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Pierre-Yves_Oudeyer"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272448205619223-1441968122611_Q64/Pierre-Yves_Oudeyer.jpg" class="lite-image" alt="Pierre-Yves Oudeyer"/></picture></span><span class="nova-v-person-inline-item__fullname">Pierre-Yves Oudeyer</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/3224749_Andrew_G_Barto"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Andrew G. Barto</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Vieri_Santucci"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272446259462193-1441967658892_Q64/Vieri_Santucci.jpg" class="lite-image" alt="Vieri Giuliano Santucci"/></picture></span><span class="nova-v-person-inline-item__fullname">Vieri Giuliano Santucci</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Gianluca_Baldassarre"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273803989221394-1442291366879_Q64/Gianluca_Baldassarre.jpg" class="lite-image" alt="Gianluca Baldassarre"/></picture></span><span class="nova-v-person-inline-item__fullname">Gianluca Baldassarre</span></span></a></li></ul></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/338663463_Editorial_Intrinsically_Motivated_Open-Ended_Learning_in_Autonomous_Robots"><span class="nova-c-button__label">View</span></a></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... In the field of developmental robotics, a robot is regarded as the model of a human infant. Developing a machine-learning method that enables a robot to discover phonemes and words from unlabeled speech signals is crucial <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. This study aims to create a machine-learning method that can discover phonemes and words from unlabeled data for developing a constructive model of language acquisition similar to human infants and to leverage the large amount of unlabeled data spoken by multiple speakers in the context of developmental robotics (Taniguchi et al., 2016a).  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336194481_Unsupervised_Phoneme_and_Word_Discovery_From_Multiple_Speakers_Using_Double_Articulation_Analyzer_and_Neural_Network_With_Parametric_Bias">Unsupervised Phoneme and Word Discovery From Multiple Speakers Using Double Articulation Analyzer and Neural Network With Parametric Bias</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Oct 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2076583141_Ryo_Nakashima"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Ryo Nakashima</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2159157407_Ryo_Ozaki"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Ryo Ozaki</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Tadahiro_Taniguchi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/277845675921468-1443254979450_Q64/Tadahiro_Taniguchi.jpg" class="lite-image" alt="Tadahiro Taniguchi"/></picture></span><span class="nova-v-person-inline-item__fullname">Tadahiro Taniguchi</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd534027 lite-page-hidden">This paper describes a new unsupervised machine-learning method for simultaneous phoneme and word discovery from multiple speakers. Phoneme and word discovery from multiple speakers is a more challenging problem than that from one speaker, because the speech signals from different speakers exhibit different acoustic features. The existing method, a nonparametric Bayesian double articulation analyzer (NPB-DAA) with deep sparse autoencoder (DSAE) only performed phoneme and word discovery from a single speaker. Extending NPB-DAA with DSAE to a multi-speaker scenario is, therefore, the research problem of this paper.This paper proposes the employment of a DSAE with parametric bias in the hidden layer (DSAE-PBHL) as a feature extractor for unsupervised phoneme and word discovery. DSAE-PBHL is designed to subtract speaker-dependent acoustic features and speaker-independent features by introducing parametric bias input to the DSAE hidden layer. An experiment demonstrated that DSAE-PBHL could subtract distributed representations of acoustic signals, enabling extraction based on the types of phonemes rather than the speakers. Another experiment demonstrated that a combination of NPB-DAA and DSAE-PBHL outperformed other available methods accomplishing phoneme and word discovery tasks involving speech signals with Japanese vowel sequences from multiple speakers.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336194481_Unsupervised_Phoneme_and_Word_Discovery_From_Multiple_Speakers_Using_Double_Articulation_Analyzer_and_Neural_Network_With_Parametric_Bias"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd534027&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Originating from Piaget&#x27;s concept of constructionism [20], the major focus of evolutionary and developmental robotics has been how an embodied agent can acquire sensory perception, motor control, and higher cognitive capabilities through bottom-up unsupervised interactions with the physical world, including other robots and humans [21,<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">22]</mark>.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336182040_Toward_evolutionary_and_developmental_intelligence">Toward evolutionary and developmental intelligence</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Oct 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Kenji_Doya"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/874025555329024-1585395339066_Q64/Kenji_Doya.jpg" class="lite-image" alt="Kenji Doya"/></picture></span><span class="nova-v-person-inline-item__fullname">Kenji Doya</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2164349806_Tadahiro_Taniguchi"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Tadahiro Taniguchi</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd534215 lite-page-hidden">Given the phenomenal advances in artificial intelligence in specific domains like visual object recognition and game playing by deep learning, expectations are rising for building artificial general intelligence (AGI) that can flexibly find solutions in unknown task domains. One approach to AGI is to set up a variety of tasks and design AI agents that perform well in many of them, including those the agent faces for the first time. One caveat for such an approach is that the best performing agent may be just a collection of domain-specific AI agents switched for a given domain. Here we propose an alternative approach of focusing on the process of acquisition of intelligence through active interactions in an environment. We call this approach evolutionary and developmental intelligence (EDI). We first review the current status of artificial intelligence, brain-inspired computing and developmental robotics and define the conceptual framework of EDI. We then explore how we can integrate advances in neuroscience, machine learning, and robotics to construct EDI systems and how building such systems can help us understand animal and human intelligence.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336182040_Toward_evolutionary_and_developmental_intelligence"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd534215&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Application of these concepts to robotics formed what is now known as Cognitive Developmental Robotics (CDR), defined as the &quot;interdisciplinary approach to the autonomous design of behavioural and cognitive capabilities in artificial agents (robots) that takes direct inspiration from the developmental principles and mechanisms observed in natural cognitive systems (humans)&quot; [1,<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">2]</mark>.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/334503805_Preliminary_Investigation_on_Visual_Finger-Counting_with_the_iCub_Robot_Cameras_and_Hands">Preliminary Investigation on Visual Finger-Counting with the iCub Robot Cameras and Hands</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jul 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Alexandr_Lucas"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/669671934152710-1536673639855_Q64/Alexandr_Lucas.jpg" class="lite-image" alt="Alexandr Lucas"/></picture></span><span class="nova-v-person-inline-item__fullname">Alexandr Lucas</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2160034975_Carlos_Ricolfe-Viala"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Carlos Ricolfe-Viala</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Alessandro_Di_Nuovo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272455281410087-1441969809775_Q64/Alessandro_Di_Nuovo.jpg" class="lite-image" alt="Alessandro Di Nuovo"/></picture></span><span class="nova-v-person-inline-item__fullname">Alessandro Di Nuovo</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd535677 lite-page-hidden">This short paper describes an approach for collecting a dataset of hand’s pictures and training a Deep Learning network that could enable the iCub robot to count on its fingers using solely its own cameras. Such a skill, mimicking children’s habits, can support arithmetic learning in a baby robot, an important step in creating artificial intelligence for robots that could learn like children in the context of cognitive developmental robotics. Preliminary results show the approach is promising in terms of accuracy.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/334503805_Preliminary_Investigation_on_Visual_Finger-Counting_with_the_iCub_Robot_Cameras_and_Hands"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd535677&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... In our study, we distinguish between the exploration that takes place when subjects perform tentative movements as they are seeking a goal, and the performance of aimless limb motions, which we refer to as &quot;motor babbling&quot; in accordance with the literature on human and robotic development <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[59,</mark>60]. In our experiments, babbling takes place during the dance calibration used to establish the forward map.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/338092098_The_dynamics_of_motor_learning_through_the_formation_of_internal_models">The dynamics of motor learning through the formation of internal models</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Dec 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Camilla_Pierella"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/279156945375233-1443567610203_Q64/Camilla_Pierella.jpg" class="lite-image" alt="Camilla Pierella"/></picture></span><span class="nova-v-person-inline-item__fullname">Camilla Pierella</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/38329540_Maura_Casadio"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Maura Casadio</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/4981705_Ferdinando_A_Mussa-Ivaldi"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Ferdinando A Mussa-Ivaldi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Sara_Solla"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272700656582668-1442028311985_Q64/Sara_Solla.jpg" class="lite-image" alt="Sara A Solla"/></picture></span><span class="nova-v-person-inline-item__fullname">Sara A Solla</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd532116 lite-page-hidden">A medical student learning to perform a laparoscopic procedure or a recently paralyzed user of a powered wheelchair must learn to operate machinery via interfaces that translate their actions into commands for an external device. Since the user’s actions are selected from a number of alternatives that would result in the same effect in the control space of the external device, learning to use such interfaces involves dealing with redundancy. Subjects need to learn an externally chosen many-to-one map that transforms their actions into device commands. Mathematically, we describe this type of learning as a deterministic dynamical process, whose state is the evolving forward and inverse internal models of the interface. The forward model predicts the outcomes of actions, while the inverse model generates actions designed to attain desired outcomes. Both the mathematical analysis of the proposed model of learning dynamics and the learning performance observed in a group of subjects demonstrate a first-order exponential convergence of the learning process toward a particular state that depends only on the initial state of the inverse and forward models and on the sequence of targets supplied to the users. Noise is not only present but necessary for the convergence of learning through the minimization of the difference between actual and predicted outcomes.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/338092098_The_dynamics_of_motor_learning_through_the_formation_of_internal_models"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd532116&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Therefore, we employ kerzel / wermter @informatik.uni-hamburg.de methods from developmental robotics <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[2]</mark>, where increasingly complex interactions with the environment facilitate learning of visuomotor abilities.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/338067808_Learning_of_Neurobotic_Visuomotor_Abilities_based_on_Interactions_with_the_Environment">Learning of Neurobotic Visuomotor Abilities based on Interactions with the Environment</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Nov 2017</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Matthias_Kerzel"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/638785058529281-1529309634390_Q64/Matthias_Kerzel.jpg" class="lite-image" alt="Matthias Kerzel"/></picture></span><span class="nova-v-person-inline-item__fullname">Matthias Kerzel</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Stefan_Wermter"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/312714742697984-1451568412045_Q64/Stefan_Wermter.jpg" class="lite-image" alt="Stefan Wermter"/></picture></span><span class="nova-v-person-inline-item__fullname">Stefan Wermter</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd53236e lite-page-hidden">Robotic visuomotor abilities, like grasping, can either be realized through conventional means of independent modules for subtasks like object localization, grasp planning, and inverse kinematics. These modules, however, rely on the availability of accurate robot and environment models. An alternative is to acquire visuomotor abilities through end-to-end machine learning. While deep neural networks have proved successful in many areas, they depend on large amounts of annotated training data or long periods of trial-and-error learning. To overcome this issue, developmental robotics leverages principles of incremental learning in biological agents. Increasingly complex visuomotor abilities are learned through mostly autonomous interaction with the environment. Following this paradigm, we present current research on acquiring visuomotor skills with a humanoid robot through self-learning and minimal human assistance. The robot engages in a learning cycle where it repeatedly manipulates an object to gather training samples that link its actions (joint configurations) to states of the environment (images from the robot&#x27;s perspective). Human assistance is only requested if errors occur during this phase, e.g., the training object is accidentally dropped out of reach. Based on these training samples, supervised end-to-end learning of visuomotor skills is realized with a deep convolutional neural architecture. The results show that the approach generalizes well to novel objects that were not included in learning. To enable this research, we developed NICO, the Neuro Inspired COmpanion, a humanoid research platform for embodied neurobotic models and human-robot interaction.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/338067808_Learning_of_Neurobotic_Visuomotor_Abilities_based_on_Interactions_with_the_Environment"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd53236e&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Evidences suggest that experienced sensorimotor contin-gencies and action-effect regularities are stored in the brain, allowing later processes of anticipation of sensorimotor activity. This has been shown to be crucial for adaptive behaviours, perception [31,14], motor control [1,50], language [21], memory [8,13] and many other cognitive functions [17,33,36], and has inspired a wide range of computational models for artificial systems [40,<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">4,</mark>29,7,37,35,12]. However, despite the promising results in robotics and AI, a number of challenges still remain open.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342408930_Prediction_error-driven_memory_consolidation_for_continual_learning_On_the_case_of_adaptive_greenhouse_models">Prediction error-driven memory consolidation for continual learning. On the case of adaptive greenhouse models</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">May 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Luis_Miranda40"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/772182376452097-1561114033784_Q64/Luis_Miranda40.jpg" class="lite-image" alt="Luis Miranda"/></picture></span><span class="nova-v-person-inline-item__fullname">Luis Miranda</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2176835974_Uwe_Schmidt"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Uwe Schmidt</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Guido_Schillaci"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/282267919372300-1444309324682_Q64/Guido_Schillaci.jpg" class="lite-image" alt="Guido Schillaci"/></picture></span><span class="nova-v-person-inline-item__fullname">Guido Schillaci</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52effb lite-page-hidden">This work presents an adaptive architecture that performs online learning and faces catastrophic forgetting issues by means of episodic memories and prediction-error driven memory consolidation. In line with evidences from the cognitive science and neuroscience, memories are retained depending on their congruency with the prior knowledge stored in the system.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342408930_Prediction_error-driven_memory_consolidation_for_continual_learning_On_the_case_of_adaptive_greenhouse_models"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52effb&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... These findings represent a valuable source of inspiration for roboticists, whose aim is to develop autonomous robots capable of living in and interacting with the human society. Developmental robotics addresses this challenge by implementing methods and algorithms for motor and cognitive development in artificial systems inspired by infant development <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. In developmental robotics, state of the art machine learning techniques are applied to computational models, creating artificial systems that can adapt to new situations and learn in an open-ended fashion.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/339411547_Prerequisites_for_an_Artificial_Self">Prerequisites for an Artificial Self</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Feb 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Verena_Hafner"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/871462063857664-1584784155569_Q64/Verena_Hafner.jpg" class="lite-image" alt="Verena Vanessa Hafner"/></picture></span><span class="nova-v-person-inline-item__fullname">Verena Vanessa Hafner</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Pontus_Loviken"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/543559944474624-1506606197892_Q64/Pontus_Loviken.jpg" class="lite-image" alt="Pontus Loviken"/></picture></span><span class="nova-v-person-inline-item__fullname">Pontus Loviken</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2159995706_Antonio_Pico_Villalpando"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Antonio Pico Villalpando</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Guido_Schillaci"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/282267919372300-1444309324682_Q64/Guido_Schillaci.jpg" class="lite-image" alt="Guido Schillaci"/></picture></span><span class="nova-v-person-inline-item__fullname">Guido Schillaci</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd530e67 lite-page-hidden">Traditionally investigated in philosophy, body ownership and agency—two main components of the minimal self—have recently gained attention from other disciplines, such as brain, cognitive and behavioral sciences, and even robotics and artificial intelligence. In robotics, intuitive human interaction in natural and dynamic environments becomes more and more important, and requires skills such as self-other distinction and an understanding of agency effects. In a previous review article, we investigated studies on mechanisms for the development of motor and cognitive skills in robots (Schillaci et al., 2016). In this review article, we argue that these mechanisms also build the foundation for an understanding of an artificial self. In particular, we look at developmental processes of the minimal self in biological systems, transfer principles of those to the development of an artificial self, and suggest metrics for agency and body ownership in an artificial self.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/339411547_Prerequisites_for_an_Artificial_Self"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd530e67&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... These findings represent a valuable source of inspiration for roboticists, whose aim is to develop autonomous robots capable of living in and interacting with the human society. Developmental robotics addresses this challenge by implementing methods and algorithms for motor and cognitive development in artificial systems inspired by infant development <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. In developmental robotics, state of the art machine learning techniques are applied to computational models, creating artificial systems that can adapt to new situations and learn in an open-ended fashion.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/339486753_Prerequisites_for_an_Artificial_Self">Prerequisites for an Artificial Self</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Feb 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Verena_Hafner"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/871462063857664-1584784155569_Q64/Verena_Hafner.jpg" class="lite-image" alt="Verena Vanessa Hafner"/></picture></span><span class="nova-v-person-inline-item__fullname">Verena Vanessa Hafner</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Pontus_Loviken"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/543559944474624-1506606197892_Q64/Pontus_Loviken.jpg" class="lite-image" alt="Pontus Loviken"/></picture></span><span class="nova-v-person-inline-item__fullname">Pontus Loviken</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2159995706_Antonio_Pico_Villalpando"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Antonio Pico Villalpando</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Guido_Schillaci"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/282267919372300-1444309324682_Q64/Guido_Schillaci.jpg" class="lite-image" alt="Guido Schillaci"/></picture></span><span class="nova-v-person-inline-item__fullname">Guido Schillaci</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd530c5f lite-page-hidden">Traditionally investigated in philosophy, body ownership and agency—two main components of the minimal self—have recently gained attention from other disciplines, such as brain, cognitive and behavioral sciences, and even robotics and artificial intelligence. In robotics, intuitive human interaction in natural and dynamic environments becomes more and more important, and requires skills such as self-other distinction and an understanding of agency effects. In a previous review article, we investigated studies on mechanisms for the development of motor and cognitive skills in robots (Schillaci et al., 2016). In this review article, we argue that these mechanisms also build the foundation for an understanding of an artificial self. In particular, we look at developmental processes of the minimal self in biological systems, transfer principles of those to the development of an artificial self, and suggest metrics for agency and body ownership in an artificial self.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/339486753_Prerequisites_for_an_Artificial_Self"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd530c5f&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Since the brain is so closely coupled to the body and situated in the environment, Neurorobots can be a powerful tool for studying neural function in a holistic fashion.&quot; [9], <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">[10]</mark> The application of embodied theory in artificial agents is among the motivations for designing new robotic platforms for research to resemble the shape of a human body, known as &quot;humanoids&quot;, e.g. ASIMO [11], and in particular that of a child, notably iCub [12] and NAO [13].  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/340114852_A_Developmental_Neuro-Robotics_Approach_for_Boosting_the_Recognition_of_Handwritten_Digits">A Developmental Neuro-Robotics Approach for Boosting the Recognition of Handwritten Digits</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Mar 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Alessandro_Di_Nuovo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272455281410087-1441969809775_Q64/Alessandro_Di_Nuovo.jpg" class="lite-image" alt="Alessandro Di Nuovo"/></picture></span><span class="nova-v-person-inline-item__fullname">Alessandro Di Nuovo</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd530793 lite-page-hidden">Developmental psychology and neuroimaging research identified a close link between numbers and fingers, which can boost the initial number knowledge in children. Recent evidence shows that a simulation of the children&#x27;s embodied strategies can improve the machine intelligence too. This article explores the application of embodied strategies to convolutional neural network models in the context of developmental neuro-robotics, where the training information is likely to be gradually acquired while operating rather than being abundant and fully available as the classical machine learning scenarios. The experimental analyses show that the proprioceptive information from the robot fingers can improve network accuracy in the recognition of handwritten Arabic digits when training examples and epochs are few. This result is comparable to brain imaging and longitudinal studies with young children. In conclusion, these findings also support the relevance of the embodiment in the case of artificial agents&#x27; training and show a possible way for the humanization of the learning process, where the robotic body can express the internal processes of artificial intelligence making it more understandable for humans.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/340114852_A_Developmental_Neuro-Robotics_Approach_for_Boosting_the_Recognition_of_Handwritten_Digits"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd530793&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... This objective has been strengthened by a new interdisciplinary approach to robotics, i.e. Developmental Robotics <mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">(Cangelosi and Schlesinger, 2015)</mark>. For example, Vinanzi et al. (2019) have proposed an artificial cognitive architecture to simulate human decision making in the robot by using concepts from developmental theories, such as Theory of Mind (ToM).  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/340429465_Shall_I_Trust_You_From_Child-Robot_Interaction_to_Trusting_Relationships">Shall I Trust You? From Child-Robot Interaction to Trusting Relationships</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Apr 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Cinzia_Di_Dio"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/352570078973953-1461070665386_Q64/Cinzia_Di_Dio.jpg" class="lite-image" alt="Cinzia Di Dio"/></picture></span><span class="nova-v-person-inline-item__fullname">Cinzia Di Dio</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Federico_Manzi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/664543986462723-1535451041907_Q64/Federico_Manzi.jpg" class="lite-image" alt="Federico Manzi"/></picture></span><span class="nova-v-person-inline-item__fullname">Federico Manzi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Giulia_Peretti"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/702967627919363-1544611951687_Q64/Giulia_Peretti.jpg" class="lite-image" alt="Giulia Peretti"/></picture></span><span class="nova-v-person-inline-item__fullname">Giulia Peretti</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Antonella_Marchetti"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/326859662348290-1454940824962_Q64/Antonella_Marchetti.jpg" class="lite-image" alt="Antonella Marchetti"/></picture></span><span class="nova-v-person-inline-item__fullname">Antonella Marchetti</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd530632 lite-page-hidden">Studying trust in the context of human–robot interaction is of great importance given the increasing relevance and presence of robotic agents in the social sphere, including educational and clinical. We investigated the acquisition, loss, and restoration of trust when preschool and school-age children played with either a human or a humanoid robot in vivo. The relationship between trust and the representation of the quality of attachment relationships, Theory of Mind, and executive function skills was also investigated. Additionally, to outline children’s beliefs about the mental competencies of the robot, we further evaluated the attribution of mental states to the interactive agent. In general, no substantial differences were found in children’s trust in the play partner as a function of agency (human or robot). Nevertheless, 3-year-olds showed a trend toward trusting the human more than the robot, as opposed to 7-year-olds, who displayed the reverse pattern. These findings align with results showing that, for 3- and 7-year-olds, the cognitive ability to switch was significantly associated with trust restoration in the human and the robot, respectively. Additionally, supporting previous findings, we found a dichotomy between attributions of mental states to the human and robot and children’s behavior: while attributing to the robot significantly lower mental states than the human, in the Trusting Game, children behaved in a similar way when they related to the human and the robot. Altogether, the results of this study highlight that similar psychological mechanisms are at play when children are to establish a novel trustful relationship with a human and robot partner. Furthermore, the findings shed light on the interplay – during development – between children’s quality of attachment relationships and the development of a Theory of Mind, which act differently on trust dynamics as a function of the children’s age as well as the interactive partner’s nature (human vs. robot).</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/340429465_Shall_I_Trust_You_From_Child-Robot_Interaction_to_Trusting_Relationships"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd530632&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-c-tooltip nova-c-tooltip--position-above-left nova-c-tooltip--elevation-none nova-c-tooltip--color-white nova-c-tooltip--spacing-m nova-c-tooltip--luminosity-medium nova-v-citation-item__context"><div class="nova-c-tooltip__body"><div class="nova-c-tooltip__body-content"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--show-divider nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-v-citation-item__context-body">... Developmental robotics, also known as epigenetic robotics or ontogenetic robotics, is a subfield of robotics whose main aims are (i) modeling the development of increasingly complex cognitive processes (for example, the understanding of language, or the acquisition of manipulation skills), and also (ii) understanding how such processes emerge through physical and social interaction [Lun+03;<mark class="nova-e-text-highlight nova-e-text-highlight--color-yellow">CS15]</mark>. Developmental robotics takes direct inspiration from the progressive learning phenomena observed in children&#x27;s cognitive development.  ...</div></div></div></div></div></div></div></div><div class="nova-c-tooltip__arrow nova-c-tooltip__arrow nova-c-tooltip__arrow--body"><div class="nova-c-tooltip__arrow-tip"></div></div></div></div></div><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/340941248_Gestures_and_Object_Affordances_for_Human-Robot_Interaction">Gestures and Object Affordances for Human–Robot Interaction</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Thesis</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Sep 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Giovanni_Saponaro"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/835789256658945-1576279095556_Q64/Giovanni_Saponaro.jpg" class="lite-image" alt="Giovanni Saponaro"/></picture></span><span class="nova-v-person-inline-item__fullname">Giovanni Saponaro</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd530524 lite-page-hidden">Personal and service robots may benefit society in different activities and challenges, thanks to their increasingly advanced mechanical and decision-making capabilities. However, for that to happen one of the essential conditions is to have coherent, natural and intuitive interfaces so that humans can make a fruitful and effective use of these intelligent machines. In general, the goal of reaching intuitive interfaces for human–robot interaction has not yet been attained, partly due to the fact that robots are not yet widespread in public spaces, partly due to the technical difficulties in interpreting human intentions.

Making service robots that understand their surroundings entails that they should possess capabilities that allow them to operate in unstructured environments and under unpredictable conditions, unlike industrial robots which usually operate in highly structured, controlled and repeatable environments. To tackle these challenges, in this thesis we develop computational models based on findings from developmental psychology (object affordances) and from neuroscience (mirror neuron system). The proposed models stem from the consideration that objects carry information about actions and interactions.

We contribute a modular framework for visual robot affordance learning, based on autonomous robot exploration of the world, sensorimotor data collection and statistical models. We combine affordances with communication (gestures and language), to interpret and describe human actions in manipulative scenes by reusing previous robot experience. We show a model that deals with multiple objects, giving rise to tool use, including the link from hand affordances (i.e., action possibilities by using the hands) to tool affordances (i.e., action possibilities by using tools). We illustrate how affordances can be used for planning complex manipulation tasks under noise and uncertainty. In two appendixes, we describe a robot model for recognizing human gestures in manipulation scenarios, and we report a study about how people perceive robot gestures when the facial information is turned off.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/340941248_Gestures_and_Object_Affordances_for_Human-Robot_Interaction"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd530524&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/339129170_Improved_and_scalable_online_learning_of_spatial_concepts_and_language_models_with_mapping">Improved and scalable online learning of spatial concepts and language models with mapping</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Feb 2020</span></li><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="journal/0929-5593_Autonomous_Robots">AUTON ROBOT</a></span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Tetsunari_Inamura"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273743087927303-1442276846418_Q64/Tetsunari_Inamura.jpg" class="lite-image" alt="Tetsunari Inamura"/></picture></span><span class="nova-v-person-inline-item__fullname">Tetsunari Inamura</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Akira_Taniguchi2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/421315027718145-1477460738439_Q64/Akira_Taniguchi2.jpg" class="lite-image" alt="Akira Taniguchi"/></picture></span><span class="nova-v-person-inline-item__fullname">Akira Taniguchi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Yoshinobu_Hagiwara"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273782359195650-1442286209102_Q64/Yoshinobu_Hagiwara.jpg" class="lite-image" alt="Yoshinobu Hagiwara"/></picture></span><span class="nova-v-person-inline-item__fullname">Yoshinobu Hagiwara</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Tadahiro_Taniguchi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/277845675921468-1443254979450_Q64/Tadahiro_Taniguchi.jpg" class="lite-image" alt="Tadahiro Taniguchi"/></picture></span><span class="nova-v-person-inline-item__fullname">Tadahiro Taniguchi</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd53149f lite-page-hidden">We propose a novel online learning algorithm, called SpCoSLAM 2.0, for spatial concepts and lexical acquisition with high accuracy and scalability. Previously, we proposed SpCoSLAM as an online learning algorithm based on unsupervised Bayesian probabilistic model that integrates multimodal place categorization, lexical acquisition, and SLAM. However, our original algorithm had limited estimation accuracy owing to the influence of the early stages of learning, and increased computational complexity with added training data. Therefore, we introduce techniques such as fixed-lag rejuvenation to reduce the calculation time while maintaining an accuracy higher than that of the original algorithm. The results show that, in terms of estimation accuracy, the proposed algorithm exceeds the original algorithm and is comparable to batch learning. In addition, the calculation time of the proposed algorithm does not depend on the amount of training data and becomes constant for each step of the scalable algorithm. Our approach will contribute to the realization of long-term spatial language interactions between humans and robots.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/339129170_Improved_and_scalable_online_learning_of_spatial_concepts_and_language_models_with_mapping"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd53149f&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336156652_Replication_of_Infant_Behaviours_with_a_Babybot_Early_Pointing_Gesture_Comprehension">Replication of Infant Behaviours with a Babybot: Early Pointing Gesture Comprehension</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Aug 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Baris_Serhan"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/492784201236480-1494500317359_Q64/Baris_Serhan.jpg" class="lite-image" alt="Baris Serhan"/></picture></span><span class="nova-v-person-inline-item__fullname">Baris Serhan</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Angelo_Cangelosi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q64/Angelo_Cangelosi.jpg" class="lite-image" alt="Angelo Cangelosi"/></picture></span><span class="nova-v-person-inline-item__fullname">Angelo Cangelosi</span></span></a></li></ul></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336156652_Replication_of_Infant_Behaviours_with_a_Babybot_Early_Pointing_Gesture_Comprehension"><span class="nova-c-button__label">View</span></a></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/334516278_Intimate_Relationships_with_Humanoid_Robots_Exploring_Human_Sexuality_in_the_Twenty-First_Century">Intimate Relationships with Humanoid Robots: Exploring Human Sexuality in the Twenty-First Century</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jul 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2145220295_Yuefang_Zhou"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Yuefang Zhou</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Martin_Fischer2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272449614905356-1441968458206_Q64/Martin_Fischer2.jpg" class="lite-image" alt="Martin H Fischer"/></picture></span><span class="nova-v-person-inline-item__fullname">Martin H Fischer</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd535443 lite-page-hidden">Sex robots are humanoid robots with artificial intelligence, designed to interact sexually with humans. They have received much attention in recent discussions about technology, human relationships and the future of human sexuality. Based on available evidence so far, this outlook aims to give tentative answers to two fundamental questions surrounding the topic of human–robot intimate relationships. First, whether intelligent humanoid robots are technologically ready to be our intimate partners. Second, whether humans are ready to accept the idea of developing intimate relationships with robots, and how far we have engaged and will engage in such activity. We highlight the importance of a scientific transdisciplinary approach to the study of human sexuality in the twenty-first century.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/334516278_Intimate_Relationships_with_Humanoid_Robots_Exploring_Human_Sexuality_in_the_Twenty-First_Century"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd535443&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342090428_Guest_Editorial_Special_Issue_on_Multidisciplinary_Perspectives_on_Mechanisms_of_Language_Learning">Guest Editorial Special Issue on Multidisciplinary Perspectives on Mechanisms of Language Learning</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jun 2020</span></li><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="journal/2379-8920_IEEE_Transactions_on_Cognitive_and_Developmental_Systems">TCDS</a></span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Malte_Schilling"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272734458478644-1442036371001_Q64/Malte_Schilling.jpg" class="lite-image" alt="Malte Schilling"/></picture></span><span class="nova-v-person-inline-item__fullname">Malte Schilling</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Katharina_Rohlfing"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/278465891848193-1443402850972_Q64/Katharina_Rohlfing.jpg" class="lite-image" alt="Katharina J. Rohlfing"/></picture></span><span class="nova-v-person-inline-item__fullname">Katharina J. Rohlfing</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2176198330_Paul_Vogt"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Paul Vogt</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2176185437_Michael_Spranger"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Michael Spranger</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52f3cb lite-page-hidden">Humans excel at learning from other humans [item 1) in the Appendix). Language facilitates such learning and plays a crucial role. On the one hand, it coordinates our interactions and cooperative behavior [item 2) in the Appendix). On the other hand, language and communication allow to directly incorporate novel knowledge gathered from social interaction or from reading [item 3) in the Appendix). It has been a long-standing goal of artificial intelligence to leverage such communicative abilities [item 4) in the Appendix) for robots and smart software agents which would, at first, simplify our interactions with machines through a more human-like way of coordinating between humans and robots. But furthermore, this would allow us to easily teach these machines, increasing their abilities and skills further which would allow them to become real partners and companions.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342090428_Guest_Editorial_Special_Issue_on_Multidisciplinary_Perspectives_on_Mechanisms_of_Language_Learning"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52f3cb&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/334771680_Inclusive_Robotics_and_AI_-_Some_Urgent_Ethical_and_Societal_Issues">Inclusive Robotics and AI – Some Urgent Ethical and Societal Issues</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jan 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Mark_Coeckelbergh"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/340894101327896-1458286895972_Q64/Mark_Coeckelbergh.jpg" class="lite-image" alt="Mark Coeckelbergh"/></picture></span><span class="nova-v-person-inline-item__fullname">Mark Coeckelbergh</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd535171 lite-page-hidden">Many discussions about robotics and Artificial Intelligence (AI) focus on far future scenarios such as superintelligence, but for the near future ethics of robotics AI it is necessary to think about more concrete ethical issues that pervade the daily use of the technologies. This talks gives a very brief overview of ethical and societal issues raised by robotics and AI, with a focus on inclusive robotics. It also offers some remarks on robot and AI policy.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/334771680_Inclusive_Robotics_and_AI_-_Some_Urgent_Ethical_and_Societal_Issues"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd535171&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/334771935_On-Line_Educational_Resources_on_Robotics_A_Review">On-Line Educational Resources on Robotics: A Review</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jan 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Maria_Pozzi3"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/496118087204864-1495295177603_Q64/Maria_Pozzi3.jpg" class="lite-image" alt="Maria Pozzi"/></picture></span><span class="nova-v-person-inline-item__fullname">Maria Pozzi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Domenico_Prattichizzo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272304324214786-1441933818268_Q64/Domenico_Prattichizzo.jpg" class="lite-image" alt="Domenico Prattichizzo"/></picture></span><span class="nova-v-person-inline-item__fullname">Domenico Prattichizzo</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Monica_Malvezzi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/338066851024898-1457612826913_Q64/Monica_Malvezzi.jpg" class="lite-image" alt="Monica Malvezzi"/></picture></span><span class="nova-v-person-inline-item__fullname">Monica Malvezzi</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5350ef lite-page-hidden">There is a growing need of experts in the field of robotics, as well as of proper educational resources to train them. In this paper, we present a review of educational resources that are available in different formats on-line. We focus on material that is thought for undergraduate or graduate students and we divide introductory courses from more advanced ones, with a special focus on Human-centred Robotics courses.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/334771935_On-Line_Educational_Resources_on_Robotics_A_Review"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5350ef&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/335062725_Reaching_development_through_visuo-proprioceptive-tactile_integration_on_a_humanoid_robot_-_a_deep_learning_approach">Reaching development through visuo-proprioceptive-tactile integration on a humanoid robot - a deep learning approach</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Aug 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Phuong_Nguyen370"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/541300690612225-1506067549564_Q64/Phuong_Nguyen370.jpg" class="lite-image" alt="Phuong D. H. Nguyen"/></picture></span><span class="nova-v-person-inline-item__fullname">Phuong D. H. Nguyen</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Matej_Hoffmann"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/552586101129216-1508758201339_Q64/Matej_Hoffmann.jpg" class="lite-image" alt="Matej Hoffmann"/></picture></span><span class="nova-v-person-inline-item__fullname">Matej Hoffmann</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/70648233_Ugo_Pattacini"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Ugo Pattacini</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/3227665_Giorgio_Metta"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Giorgio Metta</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd534f86 lite-page-hidden">The development of reaching in infants has been studied for nearly nine decades. Originally, it was thought that early reaching is visually guided, but more recent evidence is suggestive of &quot;visually elicited&quot; reaching, i.e. infant is gazing at the object rather than its hand during the reaching movement. The importance of haptic feedback has also been emphasized. Inspired by these findings, in this work we use the simulated iCub humanoid robot to construct a model of reaching development. The robot is presented with different objects, gazes at them, and performs motor babbling with one of its arms. Successful contacts with the object are detected through tactile sensors on hand and forearm. Such events serve as the training set, constituted by images from the robot&#x27;s two eyes, head joints, tactile activation, and arm joints. A deep neural network is trained with images and head joints as inputs and arm configuration and touch as output. After learning, the network can successfully infer arm configurations that would result in a successful reach, together with prediction of tactile activation (i.e. which body part would make contact). Our main contribution is twofold: (i) our pipeline is end-to-end from stereo images and head joints (6 DoF) to arm-torso configurations (10 DoF) and tactile activations, without any preprocessing, explicit coordinate transformations etc.; (ii) unique to this approach, reaches with multiple effectors corresponding to different regions of the sensitive skin are possible.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/335062725_Reaching_development_through_visuo-proprioceptive-tactile_integration_on_a_humanoid_robot_-_a_deep_learning_approach"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd534f86&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342048052_Hierarchical_Interest-Driven_Goal_Babbling_for_Efficient_Bootstrapping_of_Sensorimotor_skills">Hierarchical Interest-Driven Goal Babbling for Efficient Bootstrapping of Sensorimotor skills</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jun 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Rania_Rayyes"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/436368292749312-1481049716227_Q64/Rania_Rayyes.jpg" class="lite-image" alt="Rania Rayyes"/></picture></span><span class="nova-v-person-inline-item__fullname">Rania Rayyes</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Heiko_Donat"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/515037208027136-1499805848030_Q64/Heiko_Donat.jpg" class="lite-image" alt="Heiko Donat"/></picture></span><span class="nova-v-person-inline-item__fullname">Heiko Donat</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Jochen_Steil"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/641503240155136-1529957699271_Q64/Jochen_Steil.jpg" class="lite-image" alt="Jochen J. Steil"/></picture></span><span class="nova-v-person-inline-item__fullname">Jochen J. Steil</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52f9ca lite-page-hidden">We propose a novel hierarchical online learning scheme for fast and efficient bootstrapping of sensorimotor skills. Our scheme permits rapid data-driven robot model learning in a &quot;learning while behaving&quot; fashion. It is updated continuously to adapt to time-dependent changes and driven by an intrinsic motivation signal. It utilizes an online associative radial basis function network, which is the first associative dynamic network to be constructed from scratch with high stability. Moreover, we propose a parameter-sharing technique to increase efficiency, stabilize the online scheme, avoid exhaustive parameter tuning, and speed up the learning process. We apply our proposed algorithms on a 7-DoF physical robot manipulator and demonstrate their performance and efficiency.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342048052_Hierarchical_Interest-Driven_Goal_Babbling_for_Efficient_Bootstrapping_of_Sensorimotor_skills"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52f9ca&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/335699290_Mixed-Reality_Deep_Reinforcement_Learning_for_a_Reach-to-grasp_Task">Mixed-Reality Deep Reinforcement Learning for a Reach-to-grasp Task</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Sep 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Hadi_Beik_Mohammadi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/564953765642240-1511706882029_Q64/Hadi_Beik_Mohammadi.jpg" class="lite-image" alt="Hadi Beik Mohammadi"/></picture></span><span class="nova-v-person-inline-item__fullname">Hadi Beik Mohammadi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Mohammad_Ali_Zamani2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/613432185339905-1523265038449_Q64/Mohammad_Ali_Zamani2.jpg" class="lite-image" alt="Mohammad Ali Zamani"/></picture></span><span class="nova-v-person-inline-item__fullname">Mohammad Ali Zamani</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Matthias_Kerzel"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/638785058529281-1529309634390_Q64/Matthias_Kerzel.jpg" class="lite-image" alt="Matthias Kerzel"/></picture></span><span class="nova-v-person-inline-item__fullname">Matthias Kerzel</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Stefan_Wermter"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/312714742697984-1451568412045_Q64/Stefan_Wermter.jpg" class="lite-image" alt="Stefan Wermter"/></picture></span><span class="nova-v-person-inline-item__fullname">Stefan Wermter</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5349f9 lite-page-hidden">Deep Reinforcement Learning (DRL) has become successful across various robotic applications. However, DRL methods are not sample-efficient and require long learning times. We present an approach for online continuous deep reinforcement learning for a reach-to-grasp task in a mixed-reality environment: A human places targets for the robot in a physical environment; DRL for reaching these targets is carried out in simulation before actual actions are carried out in the physical environment. We extend previous work on a modified Deep Deterministic Policy Gradient (DDPG) algorithm with an architecture for online learning and evaluate different strategies to accelerate learning while ensuring learning stability. Our approach provides a neural inverse kinematics solution that increases over time its performance regarding the execution time while focusing on those areas of the Cartesian space where targets are often placed by the human operator, thus enabling efficient learning. We evaluate reward shaping and augmented targets as strategies for accelerating deep reinforcement learning and analyze the learning stability.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/335699290_Mixed-Reality_Deep_Reinforcement_Learning_for_a_Reach-to-grasp_Task"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5349f9&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336156474_Mindreading_for_Robots_Predicting_Intentions_via_Dynamical_Clustering_of_Human_Postures">Mindreading for Robots: Predicting Intentions via Dynamical Clustering of Human Postures</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Aug 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Samuele_Vinanzi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/543561789317120-1506606637460_Q64/Samuele_Vinanzi.jpg" class="lite-image" alt="Samuele Vinanzi"/></picture></span><span class="nova-v-person-inline-item__fullname">Samuele Vinanzi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2164366571_Christian_Goerick"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Christian Goerick</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Angelo_Cangelosi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q64/Angelo_Cangelosi.jpg" class="lite-image" alt="Angelo Cangelosi"/></picture></span><span class="nova-v-person-inline-item__fullname">Angelo Cangelosi</span></span></a></li></ul></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336156474_Mindreading_for_Robots_Predicting_Intentions_via_Dynamical_Clustering_of_Human_Postures"><span class="nova-c-button__label">View</span></a></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/341444281_Searching_for_Models_for_Psychological_Science_A_Possible_Contribution_of_Simulation">Searching for Models for Psychological Science: A Possible Contribution of Simulation</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">May 2020</span></li><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="journal/1932-4502_Integrative_Psychological_and_Behavioral_Science">INTEGR PSYCHOL BEHAV</a></span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Santo_Di_Nuovo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273665375862784-1442258318030_Q64/Santo_Di_Nuovo.jpg" class="lite-image" alt="Santo F. Di Nuovo"/></picture></span><span class="nova-v-person-inline-item__fullname">Santo F. Di Nuovo</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd53013e lite-page-hidden">The problem of the theoretical precariousness of psychology requires defining, at an epistemological level, its concepts and languages and the use of models for finding core concepts and building more or less ‘hard’ theories. After reviewing some main aims and models of psychology, and comparing them with those of other sciences, the commentary deals with the possibility of simulating life as a tool for building evolutionary psychological models. In particular, evolutionary and developmental robotics and the evo-devo models can simulate the simultaneous adaptation effects of the phylogenetic and ontogenetic development of the embodied mind in response to the characteristics of the environment. The simulation using Artificial Intelligence and robotics could contribute significantly to the definition, experimentation, and validation of the models that make up the theories of psychology in its different domains, enhancing the interdisciplinarity with other sciences that study the same objects. Further studies, both epistemological and empirical, are suggested to confirm whether this evo-devo approach can be integrated with “evolutionary psychology” and contribute to a unifying meta-theory of psychological science.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/341444281_Searching_for_Models_for_Psychological_Science_A_Possible_Contribution_of_Simulation"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd53013e&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336156737_Neurocognitive_Shared_Visuomotor_Network_for_End-to-end_Learning_of_Object_Identification_Localization_and_Grasping_on_a_Humanoid">Neurocognitive Shared Visuomotor Network for End-to-end Learning of Object Identification, Localization and Grasping on a Humanoid</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Aug 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Manfred_Eppe"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/439227298127872-1481731356508_Q64/Manfred_Eppe.jpg" class="lite-image" alt="Manfred Eppe"/></picture></span><span class="nova-v-person-inline-item__fullname">Manfred Eppe</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Stefan_Heinrich"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/860086712942592-1582072060346_Q64/Stefan_Heinrich.jpg" class="lite-image" alt="Stefan Heinrich"/></picture></span><span class="nova-v-person-inline-item__fullname">Stefan Heinrich</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Matthias_Kerzel"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/638785058529281-1529309634390_Q64/Matthias_Kerzel.jpg" class="lite-image" alt="Matthias Kerzel"/></picture></span><span class="nova-v-person-inline-item__fullname">Matthias Kerzel</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Stefan_Wermter"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/312714742697984-1451568412045_Q64/Stefan_Wermter.jpg" class="lite-image" alt="Stefan Wermter"/></picture></span><span class="nova-v-person-inline-item__fullname">Stefan Wermter</span></span></a></li></ul></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336156737_Neurocognitive_Shared_Visuomotor_Network_for_End-to-end_Learning_of_Object_Identification_Localization_and_Grasping_on_a_Humanoid"><span class="nova-c-button__label">View</span></a></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342047886_Efficient_Online_Interest-Driven_Exploration_for_Developmental_Robots">Efficient Online Interest-Driven Exploration for Developmental Robots</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jun 2020</span></li><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="journal/2379-8920_IEEE_Transactions_on_Cognitive_and_Developmental_Systems">TCDS</a></span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Rania_Rayyes"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/436368292749312-1481049716227_Q64/Rania_Rayyes.jpg" class="lite-image" alt="Rania Rayyes"/></picture></span><span class="nova-v-person-inline-item__fullname">Rania Rayyes</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Heiko_Donat"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/515037208027136-1499805848030_Q64/Heiko_Donat.jpg" class="lite-image" alt="Heiko Donat"/></picture></span><span class="nova-v-person-inline-item__fullname">Heiko Donat</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Jochen_Steil"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/641503240155136-1529957699271_Q64/Jochen_Steil.jpg" class="lite-image" alt="Jochen J. Steil"/></picture></span><span class="nova-v-person-inline-item__fullname">Jochen J. Steil</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52fae5 lite-page-hidden">A major challenge for online and data-driven model
learning in robotics is the high sample-complexity. This hinders
its efficiency and practical feasibility for lifelong learning, in
particular for developmental robots that autonomously bootstrap
their sensorimotor skills in an open-ended environment. In this
work, we propose new methods to mediate this problem in order
to permit the learning of robot models online, from scratch, and
in a learning while behaving fashion. Exploration is utilized and
autonomously driven by a novel intrinsic motivation signal which
combines knowledge-based and competence-based elements and
surpasses other state-of-art methods. In addition, we propose an
Episodic Online Mental Replay to accelerate the online learning,
to ensure the sample-efficiency, and to update the model online
rapidly. The efficiency as well as the applicability of our methods
are demonstrated with a physical 7-DoF Baxter manipulator. We
show that our learning schemes are able to drastically reduce
the sample complexity and learn the data-driven model online,
even within a limited time frame.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342047886_Efficient_Online_Interest-Driven_Exploration_for_Developmental_Robots"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52fae5&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342008351_Intrinsic_motivation_and_episodic_memories_for_robot_exploration_of_high-dimensional_sensory_spaces">Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jun 2020</span></li><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class=""><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="journal/1059-7123_Adaptive_Behavior">ADAPT BEHAV</a></span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Timothee_Wintz"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="Timothée Wintz"/></picture></span><span class="nova-v-person-inline-item__fullname">Timothée Wintz</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Guido_Schillaci"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/282267919372300-1444309324682_Q64/Guido_Schillaci.jpg" class="lite-image" alt="Guido Schillaci"/></picture></span><span class="nova-v-person-inline-item__fullname">Guido Schillaci</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2159995706_Antonio_Pico_Villalpando"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Antonio Pico Villalpando</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Verena_Hafner"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/871462063857664-1584784155569_Q64/Verena_Hafner.jpg" class="lite-image" alt="Verena Vanessa Hafner"/></picture></span><span class="nova-v-person-inline-item__fullname">Verena Vanessa Hafner</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52fee1 lite-page-hidden">This work presents an architecture that generates curiosity-driven goal-directed exploration behaviours for an image sensor of a microfarming robot. A combination of deep neural networks for offline unsupervised learning of low-dimensional features from images and of online learning of shallow neural networks representing the inverse and forward kinematics of the system have been used. The artificial curiosity system assigns interest values to a set of pre-defined goals and drives the exploration towards those that are expected to maximise the learning progress. We propose the integration of an episodic memory in intrinsic motivation systems to face catastrophic forgetting issues, typically experienced when performing online updates of artificial neural networks. Our results show that adopting an episodic memory system not only prevents the computational models from quickly forgetting knowledge that has been previously acquired but also provides new avenues for modulating the balance between plasticity and stability of the models.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342008351_Intrinsic_motivation_and_episodic_memories_for_robot_exploration_of_high-dimensional_sensory_spaces"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52fee1&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/338957151_A_curious_formulation_robot_enables_the_discovery_of_a_novel_protocell_behavior">A curious formulation robot enables the discovery of a novel protocell behavior</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jan 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Jonathan_Grizou"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="Jonathan Grizou"/></picture></span><span class="nova-v-person-inline-item__fullname">Jonathan Grizou</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Laurie_Points"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="Laurie Points"/></picture></span><span class="nova-v-person-inline-item__fullname">Laurie Points</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2146051871_Abhishek_Sharma"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Abhishek Sharma</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/39871589_Leroy_Cronin"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Leroy Cronin</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5318b4 lite-page-hidden">We describe a chemical robotic assistant equipped with a curiosity algorithm (CA) that can efficiently explore the states a complex chemical system can exhibit. The CA-robot is designed to explore formulations in an open-ended way with no explicit optimization target. By applying the CA-robot to the study of self-propelling multicomponent oil-in-water protocell droplets, we are able to observe an order of magnitude more variety in droplet behaviors than possible with a random parameter search and given the same budget. We demonstrate that the CA-robot enabled the observation of a sudden and highly specific response of droplets to slight temperature changes. Six modes of self-propelled droplet motion were identified and classified using a time-temperature phase diagram and probed using a variety of techniques including NMR. This work illustrates how CAs can make better use of a limited experimental budget and significantly increase the rate of unpredictable observations, leading to new discoveries with potential applications in formulation chemistry.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/338957151_A_curious_formulation_robot_enables_the_discovery_of_a_novel_protocell_behavior"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5318b4&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/336594986_Maschinenethik_und_Roboterethik">Maschinenethik und Roboterethik</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Oct 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2125144249_Janina_Loh"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Janina Loh</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5337ed lite-page-hidden">Die Roboterethik ist eine Teilbereichsethik der Maschinenethik, in der man zwei Forschungsbereiche unterscheidet. Im einen wird diskutiert, inwiefern Roboter als moral patients zu verstehen sind, also passiv als Träger moralischer Rechte, bzw. inwiefern ihnen ein moralischer Wert zukommt. Im anderen Feld geht es um die Frage, inwiefern Roboter moral agents, also aktiv Träger moralischer Pflichten bzw. moralische Handlungssubjekte, sein können. Die beiden Arbeitsbereiche sind nicht exklusiv, sondern ergänzen einander.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/336594986_Maschinenethik_und_Roboterethik"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5337ed&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/337052393_Using_AI_Methods_to_Evaluate_a_Minimal_Model_for_Perception">Using AI Methods to Evaluate a Minimal Model for Perception</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Nov 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Robert_Prentner"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/281170844635137-1444047761279_Q64/Robert_Prentner.jpg" class="lite-image" alt="Robert Prentner"/></picture></span><span class="nova-v-person-inline-item__fullname">Robert Prentner</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Chris_Fields2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/277701085679619-1443220506609_Q64/Chris_Fields2.jpg" class="lite-image" alt="Chris Fields"/></picture></span><span class="nova-v-person-inline-item__fullname">Chris Fields</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd533397 lite-page-hidden">The relationship between philosophy and research on artificial intelligence (AI) has been difficult since its beginning, with mutual misunderstanding and sometimes even hostility. By contrast, we show how an approach informed by both philosophy and AI can be productive. After reviewing some popular frameworks for computation and learning, we apply the AI methodology of “build it and see” to tackle the philosophical and psychological problem of characterizing perception as distinct from sensation. Our model comprises a network of very simple, but interacting agents which have binary experiences of the “yes/no”-type and communicate their experiences with each other. When does such a network refer to a single agent instead of a distributed network of entities? We apply machine learning techniques to address the following related questions: i) how can the model explain stability of compound entities, and ii) how could the model implement a single task such as perceptual inference? We thereby find consistency with previous work on “interface” strategies from perception research.
While this reflects some necessary conditions for the ascription of agency, we suggest that it is not sufficient. Here, AI research, if it is intended to contribute to conceptual understanding, would benefit from issues previously raised by philosophy. We thus conclude the article with a discussion of action-selection, the role of embodiment, and consciousness to make this more explicit. We conjecture that a combination of AI research and philosophy allows general principles of mind and being to emerge from a “quasi-empirical” investigation.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/337052393_Using_AI_Methods_to_Evaluate_a_Minimal_Model_for_Perception"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd533397&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/337490581_Intrinsically_Motivated_Active_Perception_for_Multi-areas_View_Tasks">Intrinsically Motivated Active Perception for Multi-areas View Tasks</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Chapter</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Nov 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2167037781_Dashun_Pei"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Dashun Pei</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2167032254_Linhua_Jiang"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Linhua Jiang</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5332aa lite-page-hidden">The target recognition of the human eye in real scenes is still far superior to any robot vision system. We believe that there are two major essential reasons. First, humans can observe the environment in which the object is located, get the probability of the object category. Second, human can use foveal to focus on the object and get more object detail features from the high resolution image containing the object and make it easier to identify. This paper proposes a novel method for searching and locating surrounding objects using a monocular Panning/Tilting/Zooming (PTZ) camera with free rotation and zoom functions.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/337490581_Intrinsically_Motivated_Active_Perception_for_Multi-areas_View_Tasks"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5332aa&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/339972044_An_Experiment_in_Morphological_Development_for_Learning_ANN_Based_Controllers">An Experiment in Morphological Development for Learning ANN Based Controllers</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Mar 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Martin_Naya"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/466431264464896-1488217287095_Q64/Martin_Naya.jpg" class="lite-image" alt="Martin Naya"/></picture></span><span class="nova-v-person-inline-item__fullname">Martin Naya</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Andres_Faina"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/525483636948993-1502296470688_Q64/Andres_Faina.jpg" class="lite-image" alt="Andres Faiña"/></picture></span><span class="nova-v-person-inline-item__fullname">Andres Faiña</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Richard_Duro"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272217246269465-1441913057338_Q64/Richard_Duro.jpg" class="lite-image" alt="Richard Duro"/></picture></span><span class="nova-v-person-inline-item__fullname">Richard Duro</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd530866 lite-page-hidden">Morphological development is part of the way any human or animal learns. The learning processes starts with the morphology at birth and progresses through changing morphologies until adulthood is reached. Biologically, this seems to facilitate learning and make it more robust. However, when this approach is transferred to robotic systems, the results found in the literature are inconsistent: morphological development does not provide a learning advantage in every case. In fact, it can lead to poorer results than when learning with a fixed morphology. In this paper we analyze some of the issues involved by means of a simple, but very informative experiment in quadruped walking. From the results obtained an initial series of insights on when and under what conditions to apply morphological development for learning are presented.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/339972044_An_Experiment_in_Morphological_Development_for_Learning_ANN_Based_Controllers"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd530866&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/339528799_Come_I_bambini_pensano_alla_mente_di_un_robot_Il_ruolo_dell&#x27;attaccamento_e_della_Teoria_della_Mente_nell&#x27;attribuzione_di_stati_mentali_a_un_agente_robotico_How_children_think_about_the_robot&#x27;s_mind_Th">Come I bambini pensano alla mente di un robot. Il ruolo dell’attaccamento e della Teoria della Mente nell’attribuzione di stati mentali a un agente robotico [How children think about the robot’s mind. The role of attachment and Theory of Mind in the attribution of mental states to a robotic agent]</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Feb 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Cinzia_Di_Dio"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/352570078973953-1461070665386_Q64/Cinzia_Di_Dio.jpg" class="lite-image" alt="Cinzia Di Dio"/></picture></span><span class="nova-v-person-inline-item__fullname">Cinzia Di Dio</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Federico_Manzi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/664543986462723-1535451041907_Q64/Federico_Manzi.jpg" class="lite-image" alt="Federico Manzi"/></picture></span><span class="nova-v-person-inline-item__fullname">Federico Manzi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Giulia_Peretti"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/702967627919363-1544611951687_Q64/Giulia_Peretti.jpg" class="lite-image" alt="Giulia Peretti"/></picture></span><span class="nova-v-person-inline-item__fullname">Giulia Peretti</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Antonella_Marchetti"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/326859662348290-1454940824962_Q64/Antonella_Marchetti.jpg" class="lite-image" alt="Antonella Marchetti"/></picture></span><span class="nova-v-person-inline-item__fullname">Antonella Marchetti</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd5309c5 lite-page-hidden">L’inclusione di agenti artificiali in ambienti in cui essi sono pro- grammati per diventare partner relazionali suggerisce un cambiamento nel nostro paradigma sociale incentrato sull’uomo (Belpaeme, Kennedy, Ramachandran, Scassellati e Tanaka, 2018). In quest’ottica, è essenziale studiare come si costruiscono le relazioni quando queste coinvolgono almeno un agente robotico partendo dallo studio dei processi psicologici che regolano le relazioni tra esseri umani nei diversi periodi di sviluppo (Cangelosi e Schlesinger, 2015; Vinanzi, Patacchiola, Chella e Cange- losi, 2019). Una condizione necessaria affinché si possa instaurare una relazione con un agente robotico è l’attribuzione di stati mentali allo stesso. Studiare la rappresentazione degli stati mentali del robot nei bambini è rilevante, non solo perché è possibile apprezzare variazioni di attribuzione in funzione dello sviluppo di competenze psicologiche, ma anche perché si prospetta uno scenario futuro prossimo nel quale i bambini interagiranno con agenti robotici in vari ambiti, da quello educativo a quello clinico. Le competenze psicologiche indagate nel presente studio, classicamente associate ai processi socio-relazionali tra esseri umani, sono la qualità delle relazioni di attaccamento verso la figura di accudimento primaria del bambino e le abilità di Teoria della Mente (ToM) di primo ordine e secondo ordine.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/339528799_Come_I_bambini_pensano_alla_mente_di_un_robot_Il_ruolo_dell&#x27;attaccamento_e_della_Teoria_della_Mente_nell&#x27;attribuzione_di_stati_mentali_a_un_agente_robotico_How_children_think_about_the_robot&#x27;s_mind_Th"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd5309c5&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/337981905_Developing_Robot_Reaching_Skill_via_Look-ahead_Planning">Developing Robot Reaching Skill via Look-ahead Planning</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Conference Paper</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Aug 2019</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2120291959_Tianlin_Liu"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Tianlin Liu</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2147912668_Mengxi_Nie"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Mengxi Nie</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/39459825_Xihong_Wu"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Xihong Wu</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Dingsheng_Luo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="Dingsheng Luo"/></picture></span><span class="nova-v-person-inline-item__fullname">Dingsheng Luo</span></span></a></li></ul></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/337981905_Developing_Robot_Reaching_Skill_via_Look-ahead_Planning"><span class="nova-c-button__label">View</span></a></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/339315108_Cross-Talk_of_Low-Level_Sensory_and_High-Level_Cognitive_Processing_Development_Mechanisms_and_Relevance_for_Cross-Modal_Abilities_of_the_Brain">Cross-Talk of Low-Level Sensory and High-Level Cognitive Processing: Development, Mechanisms, and Relevance for Cross-Modal Abilities of the Brain</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Article</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Feb 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Xiaxia_Xu"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/656636414812160-1533565729002_Q64/Xiaxia_Xu.jpg" class="lite-image" alt="Xiaxia Xu"/></picture></span><span class="nova-v-person-inline-item__fullname">Xiaxia Xu</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Ileana_Hanganu-Opatz2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/459819359444993-1486640886401_Q64/Ileana_Hanganu-Opatz2.jpg" class="lite-image" alt="Ileana L Hanganu-Opatz"/></picture></span><span class="nova-v-person-inline-item__fullname">Ileana L Hanganu-Opatz</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2071956521_Malte_Bieler"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Malte Bieler</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd531140 lite-page-hidden">The emergence of cross-modal learning capabilities requires the interaction of neural areas accounting for sensory and cognitive processing. Convergence of multiple sensory inputs is observed in low-level sensory cortices including primary somatosensory (S1), visual (V1), and auditory cortex (A1), as well as in high-level areas such as prefrontal cortex (PFC). Evidence shows that local neural activity and functional connectivity between sensory cortices participate in cross-modal processing. However, little is known about the functional interplay between neural areas underlying sensory and cognitive processing required for cross-modal learning capabilities across life. Here we review our current knowledge on the interdependence of low- and high-level cortices for the emergence of cross-modal processing in rodents. First, we summarize the mechanisms underlying the integration of multiple senses and how cross-modal processing in primary sensory cortices might be modified by top-down modulation of the PFC. Second, we examine the critical factors and developmental mechanisms that account for the interaction between neuronal networks involved in sensory and cognitive processing. Finally, we discuss the applicability and relevance of cross-modal processing for brain-inspired intelligent robotics. An in-depth understanding of the factors and mechanisms controlling cross-modal processing might inspire the refinement of robotic systems by better mimicking neural computations.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/339315108_Cross-Talk_of_Low-Level_Sensory_and_High-Level_Cognitive_Processing_Development_Mechanisms_and_Relevance_for_Cross-Modal_Abilities_of_the_Brain"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd531140&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li><li class="nova-e-list__item publication-citations__item"><div class="nova-v-citation-item"><div class="nova-o-stack nova-o-stack--gutter-m nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-v-citation-item__context-source"><div class="nova-v-publication-item nova-v-publication-item--size-m gtm-citation-item"><div class="nova-v-publication-item__body"><div class="nova-v-publication-item__stack nova-v-publication-item__stack--gutter-m"><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-2 nova-v-publication-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="publication/342542080_Towards_hybrid_primary_intersubjectivity_a_neural_robotics_library_for_human_science">Towards hybrid primary intersubjectivity: a neural robotics library for human science</a></div></div><div class="nova-v-publication-item__stack-item"><div class="nova-v-publication-item__meta"><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m nova-v-publication-item__type">Preprint</span></div><div class="nova-v-publication-item__meta-left"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-block nova-e-badge--luminosity-medium nova-e-badge--size-l nova-e-badge--theme-ghost nova-e-badge--radius-m nova-v-publication-item__fulltext">Full-text available</span></div><div class="nova-v-publication-item__meta-right"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__meta-data"><li class="nova-e-list__item nova-v-publication-item__meta-data-item"><span class="">Jun 2020</span></li></ul></div></div></div><div class="nova-v-publication-item__stack-item"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-publication-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Hendry_Chame"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/822236088901632-1573047768824_Q64/Hendry_Chame.jpg" class="lite-image" alt="Hendry F. Chame"/></picture></span><span class="nova-v-person-inline-item__fullname">Hendry F. Chame</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Ahmadreza_Ahmadi5"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/279090490822656-1443551766219_Q64/Ahmadreza_Ahmadi5.jpg" class="lite-image" alt="Ahmadreza Ahmadi"/></picture></span><span class="nova-v-person-inline-item__fullname">Ahmadreza Ahmadi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/11210515_Jun_Tani"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Jun Tani</span></span></a></li></ul></div><div class="nova-v-publication-item__stack-item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp-3 nova-v-publication-item__description"><span class="js-target-abstract-5f02efd52eb1b lite-page-hidden">Human-robot interaction is becoming an interesting area of research in cognitive science, notably, for the study of social cognition. Interaction theorists consider primary intersubjectivity a non-mentalist, pre-theoretical, non-conceptual sort of processes that ground a certain level of communication and understanding, and provide support to higher-level cognitive skills. We argue this sort of low level cognitive interaction, where control is shared in dyadic encounters, is susceptible of study with neural robots. Hence, in this work we pursue three main objectives. Firstly, from the concept of active inference we study primary intersubjectivity as a second person perspective experience characterized by predictive engagement, where perception, cognition, and action are accounted for an hermeneutic circle in dyadic interaction. Secondly, we propose an open-source methodology named neural robotics library (NRL) for experimental human-robot interaction, and a demonstration program for interacting in real-time with a virtual Cartesian robot (VCBot). Lastly, through a study case, we discuss some ways human-robot (hybrid) intersubjectivity can contribute to human science research, such as to the fields of developmental psychology, educational technology, and cognitive rehabilitation.</span></div></div></div></div><footer class="nova-v-publication-item__footer"><div class="nova-v-publication-item__footer-actions"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle"><div class="nova-o-pack__item nova-v-publication-item__footer-button-group"><div class="nova-c-button-group nova-c-button-group--gutter-m nova-c-button-group--orientation-horizontal nova-c-button-group--width-auto nova-v-publication-item__actions"><div class="nova-c-button-group__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" href="publication/342542080_Towards_hybrid_primary_intersubjectivity_a_neural_robotics_library_for_human_science"><span class="nova-c-button__label">View</span></a></div><div class="nova-c-button-group__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto nova-v-publication-item__action" type="button"><span class="nova-c-button__label"><span class=" js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-abstract-5f02efd52eb1b&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;}]}">Show abstract</span></span></button></div></div></div></div></div></footer></div></div></div></div></div></li></div><li class="nova-e-list__item publication-citations__item publication-citations__item--load-more js-target-citations-5f02efd535850"><div class="nova-o-stack nova-o-stack--gutter-s nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="publication-citations__more"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-ghost nova-c-button--width-auto js-lite-click" type="button" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-citations-5f02efd535850&quot;,&quot;a&quot;:&quot;law(https://www.researchgate.net/lite.PublicationDetailsLoadMore.getCitationsByOffset.html?publicationUid=290749036&amp;offset=50)&quot;},{&quot;t&quot;:&quot;.js-target-citations-5f02efd535850&quot;,&quot;a&quot;:&quot;rc(publication-citations__item--load-more)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}"><span class="nova-c-button__label">Show more</span></button></div></div></div></li></ul></div></div></div></div><div class="lite-tabs__tab js-target-references"><div class="js-target-reference" id="references"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit publication-citations__empty">ResearchGate has not been able to resolve any references for this publication.</div></div></div></div></section></section><div id="lite-ad-5mck7votskv" class="lite-page-ad lite-page-ad--has-label lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-5mck7votskv-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-5mck7votskv-close').onclick = function(e) {document.querySelector('#lite-ad-5mck7votskv').style.display = 'none';};</script><div id="5mck7votskv"><rg-ad id="938ftjotuhm" ad-unit="LoggedOut_PublicationWithoutFulltext_Middle" ad-unit-path="/83500759/LoggedOut_PublicationWithoutFulltext_Middle" ad-location="Middle" refresh-timeout="-1" load-on-init="false" collapse="never" class="rg-ad" ad-unit-sizes="[[728,90],[300,250],&quot;fluid&quot;]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="938ftjotuhm" targeting-sequence="2"><rg-ad-size size="[728,90]"></rg-ad-size><rg-ad-size size="[300,250]"></rg-ad-size><rg-ad-size size="&quot;fluid&quot;"></rg-ad-size><rg-ad-sizeset viewport-size="[0,0]" slot-sizes="[]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[250,0]" slot-sizes="[[250,250]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[300,0]" slot-sizes="[[250,250],[300,250],[300,100],[300,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[320,0]" slot-sizes="[[250,250],[300,250],[300,100],[300,50],[320,100],[320,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[336,0]" slot-sizes="[[336,280],[300,250],[250,250],[300,100],[300,50],[320,100],[320,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[992,0]" slot-sizes="[[728,90],&quot;fluid&quot;]"></rg-ad-sizeset></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("938ftjotuhm").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-5mck7votskv').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-5mck7votskv");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-5mck7votskv').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-5mck7votskv");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div></section><aside class="lite-page__side hide-l" role="complementary"><div id="lite-ad-ie31d7ifgom" class="lite-page-ad lite-page-ad--vertical-spacing-xl lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-ie31d7ifgom-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-ie31d7ifgom-close').onclick = function(e) {document.querySelector('#lite-ad-ie31d7ifgom').style.display = 'none';};</script><div id="ie31d7ifgom"><rg-ad id="koy3rve4dy" ad-unit="Instream_Video" ad-unit-path="/83500759/Instream_Video" refresh-timeout="0" load-on-init="true" collapse="never" class="rg-ad" ad-unit-sizes="[[1,1]]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="koy3rve4dy" targeting-sequence="1"><rg-ad-size size="[1,1]"></rg-ad-size></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("koy3rve4dy").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-ie31d7ifgom').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-ie31d7ifgom");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-ie31d7ifgom').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-ie31d7ifgom");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div><div id="lite-ad-h8tvrobjti" class="lite-page-ad lite-page-ad--vertical-spacing-xl lite-page-ad--sticky lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-h8tvrobjti-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-h8tvrobjti-close').onclick = function(e) {document.querySelector('#lite-ad-h8tvrobjti').style.display = 'none';};</script><div id="h8tvrobjti"><rg-ad id="hde62i9bvnm" ad-unit="LoggedOut_PublicationWithoutFulltext_Right" ad-unit-path="/83500759/LoggedOut_PublicationWithoutFulltext_Right" ad-location="Right" refresh-timeout="-1" load-on-init="false" collapse="never" class="rg-ad" ad-unit-sizes="[[336,280],[300,250],[300,600],[160,600],[120,600],[320,100],[300,50],[300,100],[320,50],&quot;fluid&quot;]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="hde62i9bvnm" targeting-sequence="1"><rg-ad-size size="[336,280]"></rg-ad-size><rg-ad-size size="[300,250]"></rg-ad-size><rg-ad-size size="[300,600]"></rg-ad-size><rg-ad-size size="[160,600]"></rg-ad-size><rg-ad-size size="[120,600]"></rg-ad-size><rg-ad-size size="[320,100]"></rg-ad-size><rg-ad-size size="[300,50]"></rg-ad-size><rg-ad-size size="[300,100]"></rg-ad-size><rg-ad-size size="[320,50]"></rg-ad-size><rg-ad-size size="&quot;fluid&quot;"></rg-ad-size><rg-ad-sizeset viewport-size="[0,0]" slot-sizes="[]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[992,0]" slot-sizes="[[336,280],[300,250],&quot;fluid&quot;]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[992,500]" slot-sizes="[[336,280],[300,250],&quot;fluid&quot;]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[992,850]" slot-sizes="[[336,280],[300,250],[300,600],[160,600],[120,600],&quot;fluid&quot;]"></rg-ad-sizeset></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("hde62i9bvnm").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-h8tvrobjti').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-h8tvrobjti");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-h8tvrobjti').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-h8tvrobjti");elem && elem.classList.remove('lite-page-ad--empty');}}})</script><div class="lite-page-ad__fallback"><div class="lite-page-ad-fallback"><div class="publication-ad-fallback-promo"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-l nova-e-text--color-inherit">Join ResearchGate to find the people and research you need to help your work.</div><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-unordered nova-e-list--spacing-auto"><li class="nova-e-list__item"><strong>17+ million</strong> members</li><li class="nova-e-list__item"><strong>135+ million</strong> publications</li><li class="nova-e-list__item"><strong>700k+</strong> research projects</li></ul><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-full" href="signup.SignUp.html"><span class="nova-c-button__label">Join for free</span></a></div></div></div></div></div></div></aside></main><aside class="lite-page__below" role="complementary"><div class="publication-recommendations"><div class="publication-recommendations__outer"><div class="publication-recommendations__inner"><div class="nova-o-grid nova-o-grid--gutter-m nova-o-grid--order-normal nova-o-grid--horizontal-align-left nova-o-grid--vertical-align-top"><div class="nova-o-grid__column nova-o-grid__column--width-12/12@s-up"><div class="nova-o-grid__column nova-o-grid__column--width-12/12@s-up nova-o-grid__column--width-6/12@m-up"><h2 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xxs nova-e-text--color-inherit">Recommendations</h2></div><hr/></div></div><div class="nova-o-grid nova-o-grid--gutter-m nova-o-grid--order-normal nova-o-grid--horizontal-align-left nova-o-grid--vertical-align-top publication-recommendations__items"><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up project-recommendation-item__item"><div class="project-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up project-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Project</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit project-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="project/GummiArm-a-compliant-and-printable-robot-arm">GummiArm - a compliant and printable robot arm</a></h3><div class="project-recommendation-item__researcher"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-project-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Martin_Stoelen"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/582806575484928-1515963323823_Q64/Martin_Stoelen.jpg" class="lite-image" alt="Martin F. Stoelen"/></picture></span><span class="nova-v-person-inline-item__fullname">Martin F. Stoelen</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Angelo_Cangelosi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q64/Angelo_Cangelosi.jpg" class="lite-image" alt="Angelo Cangelosi"/></picture></span><span class="nova-v-person-inline-item__fullname">Angelo Cangelosi</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Ricardo_De_Azambuja2"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/563366797287424-1511328519240_Q64/Ricardo_De_Azambuja2.jpg" class="lite-image" alt="Ricardo De Azambuja"/></picture></span><span class="nova-v-person-inline-item__fullname">Ricardo De Azambuja</span></span></a></li><li class="nova-e-list__item"><span class="nova-v-project-item__person-list-truncation">[...]</span></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Roel_Pieters"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/441648611106818-1482308642631_Q64/Roel_Pieters.jpg" class="lite-image" alt="Roel Pieters"/></picture></span><span class="nova-v-person-inline-item__fullname">Roel Pieters</span></span></a></li></ul></div><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 project-recommendation-item__description">The GummiArm project was initiated to develop a robot arm that could interact physically with the environment, while developing its sensorimotor skills, and without expensive breakdowns from collis <span class="lite-page-hidden js-target-57ac4ddc96b7e4839c12d293">ions. The arm can vary its stiffness by co-contracting its rubbery tendons, and is quite robust to impacts in the &#x27;relaxed&#x27; state. A broken piece can be redesigned, reprinted, and assembled in minutes, thanks to the 3D printable structure. Replications welcome! http://mstoelen.github.io/GummiArm/</span> <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-57ac4ddc96b7e4839c12d293&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}">... [more]</a></div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent project-recommendation-item__more" href="project/GummiArm-a-compliant-and-printable-robot-arm">View project</a></div></div><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up project-recommendation-item__item"><div class="project-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up project-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Project</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit project-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="project/An-Intelligent-Integrated-Marine-Observation-System">An Intelligent Integrated Marine Observation System</a></h3><div class="project-recommendation-item__researcher"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-project-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Riccardo_Polvara"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/299617397821440-1448445762628_Q64/Riccardo_Polvara.jpg" class="lite-image" alt="Riccardo Polvara"/></picture></span><span class="nova-v-person-inline-item__fullname">Riccardo Polvara</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Jian_Wan3"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273753703710729-1442279377148_Q64/Jian_Wan3.jpg" class="lite-image" alt="Jian Wan"/></picture></span><span class="nova-v-person-inline-item__fullname">Jian Wan</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Sanjay_Sharma53"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/278647454879758-1443446138606_Q64/Sanjay_Sharma53.jpg" class="lite-image" alt="Sanjay Sharma"/></picture></span><span class="nova-v-person-inline-item__fullname">Sanjay Sharma</span></span></a></li><li class="nova-e-list__item"><span class="nova-v-project-item__person-list-truncation">[...]</span></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="scientific-contributions/2067828513_Massimiliano_Patacchiola"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname">Massimiliano Patacchiola</span></span></a></li></ul></div><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 project-recommendation-item__description">Springer is a mobile, rapidly sampling, remotely and autonomously operated sensor platform for various hydrological surveys, real-time mapping of pollutant spills, tracking and qualification of pol <span class="lite-page-hidden js-target-57cfe12f217e200fce2e44ca">lutant spills. Its novel navigation technology significantly improves user control for physical oceanographic deployments. 

The four rotor quadcopter has a flying time of 30 minutes and it is equipped with a wide-angled HD camera. The addition of the quadcopter into Springer would provide the user the ability to identify key areas (e.g. the centre and perimeter of a sediment discharge plume) where Springer needs to be deployed, thus optimising the data collection phase and significantly improving survey efficiency.
 
The project aims to integrate fully these two autonomous systems to form an intelligent marine observation system for more efficient and intelligent conducting of surveying tasks. Therefore innovative multi-sensor-data-fusion techniques are to be developed to combine data from multiple types of multiple sensors from multiple perspectives to improve the situational awareness of the environment, to increase the probability of detecting hazards or features and to offer redundancy. Furthermore, an energy-based control system is also to be designed to guide the integrated system to determine cost effective paths which will be both safe and still leading to the waypoints, and directs the quadcopter to follow the best path. The integrated system will also be tested for its situational awareness, autonomy and efficiency under different operating conditions and hostile marine environments.</span> <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-57cfe12f217e200fce2e44ca&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}">... [more]</a></div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent project-recommendation-item__more" href="project/An-Intelligent-Integrated-Marine-Observation-System">View project</a></div></div><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up project-recommendation-item__item"><div class="project-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up project-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Project</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit project-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="project/1-The-use-of-psychometric-tests-for-assessing-Mild-Cognitive-Impairment-2-Technological-aids-for-interventions-in-disabilities-and-learning-disorders">1) The use of psychometric tests for assessing Mild Cognitive Impairment; 2) Technological aids for interventions in disabilities and learning disorders  </a></h3><div class="project-recommendation-item__researcher"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-project-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Santo_Di_Nuovo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/273665375862784-1442258318030_Q64/Santo_Di_Nuovo.jpg" class="lite-image" alt="Santo F. Di Nuovo"/></picture></span><span class="nova-v-person-inline-item__fullname">Santo F. Di Nuovo</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Alessandro_Di_Nuovo"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/272455281410087-1441969809775_Q64/Alessandro_Di_Nuovo.jpg" class="lite-image" alt="Alessandro Di Nuovo"/></picture></span><span class="nova-v-person-inline-item__fullname">Alessandro Di Nuovo</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Angelo_Cangelosi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q64/Angelo_Cangelosi.jpg" class="lite-image" alt="Angelo Cangelosi"/></picture></span><span class="nova-v-person-inline-item__fullname">Angelo Cangelosi</span></span></a></li><li class="nova-e-list__item"><span class="nova-v-project-item__person-list-truncation">[...]</span></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Serafino_Buono"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg" class="lite-image" alt="Serafino Buono"/></picture></span><span class="nova-v-person-inline-item__fullname">Serafino Buono</span></span></a></li></ul></div><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 project-recommendation-item__description">My main current projects regard two issues: the use of tests for assessing MCI in elderly and associate disorders (e.g., depression),with special focus on cognitive tasks, like as imagery tests; an <span class="lite-page-hidden js-target-57e427e596b7e48cdf72301c">d the use of technologies (e.g., robotics) in rehabilitation of disability, Autism and Special Educational Needs. This project is carried out in cooperation with an international network, including Plymouth and Sheffield Universities (UK).</span> <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-57e427e596b7e48cdf72301c&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}">... [more]</a></div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent project-recommendation-item__more" href="project/1-The-use-of-psychometric-tests-for-assessing-Mild-Cognitive-Impairment-2-Technological-aids-for-interventions-in-disabilities-and-learning-disorders">View project</a></div></div><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up project-recommendation-item__item"><div class="project-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up project-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Project</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit project-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="project/From-single-words-to-compositional-language-via-gestures-Applications-in-robot-language-learning"> From single words to compositional language via gestures: Applications in robot language learning</a></h3><div class="project-recommendation-item__researcher"><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-none nova-v-project-item__person-list"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Gabriella_Pizzuto"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/422091892760576-1477645957194_Q64/Gabriella_Pizzuto.jpg" class="lite-image" alt="Gabriella Pizzuto"/></picture></span><span class="nova-v-person-inline-item__fullname">Gabriella Pizzuto</span></span></a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="profile/Angelo_Cangelosi"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/784867574951938-1564138420404_Q64/Angelo_Cangelosi.jpg" class="lite-image" alt="Angelo Cangelosi"/></picture></span><span class="nova-v-person-inline-item__fullname">Angelo Cangelosi</span></span></a></li></ul></div><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 project-recommendation-item__description">http://www.dcomm.eu  </div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent project-recommendation-item__more" href="project/From-single-words-to-compositional-language-via-gestures-Applications-in-robot-language-learning">View project</a></div></div><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up publication-recommendation-item__item"><div class="publication-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up publication-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Article</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit publication-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare gtm-recommendation-item-link" href="publication/46606195_Sociale_Erfenissen_Orientaties_van_ouders_bij_het_opvoeden">Sociale Erfenissen. Oriëntaties van ouders bij het opvoeden</a></h3><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-grey-500"><span>January 2000</span></div><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-auto"><li class="nova-e-list__item"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/47148857_LE_Bolt">L.E. Bolt</a></span></span></li></ul><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 publication-recommendation-item__abstract">Parents in a modern society are expected to raise their children to become socially skilful and critical citizens. Parents themselves also seem to adhere to an ideal of conscious
parenthood. What values and attitudes do parents think are important for the future of
their children? This sociological study about raising children in contemporary society
focuses on the question of what orientations <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-5f02efd535fd4&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}">... [Show full abstract]</a><span class="lite-page-hidden js-target-5f02efd535fd4"> parents use in the process of child-rearing and
in particular how their own upbringing is important in this respect. An approach that dealt
only with the meaning of the past would not, however, be sufficient to give an
understanding of the attitudes and skills valued by parents in the raising their children.
This research therefore also gives attention to significant others with whom parents talk
about child-rearing and from whom they receive support.
The research question is twofold. First, how do social legacies affect the way in which
parents raise their children? What child-rearing support networks can be distinguished,
and how do they offer support for parents in raising their children? This study makes use
of Bourdieu=s concept of capital and Elias=s notion of social inheritance. Bourdieu
distinguishes three forms of capital: economic capital, social capital (social relations) and
cultural capital (education and style). In this study the last two are the most important.
Parents transfer to their children not only a genetic and a material legacy, but also a social
legacy. Social legacy is used as a sensitizing concept. In this study it is seen as the values,
attitudes and practices which people take over from their parents and which they consider
to be meaningful in the raising of their children.
The second part of the research question addresses the social embeddedness of child-rearing.
I have referred in this context to social child-rearing networks. Sociological
theories about modernization assert that modern individuals are increasingly becoming
the &gt;directors= of their own lives. An approach which sees child-rearing as part of a
lifestyle, allows social inheritance and parents= social child-rearing networks to be studied
in their mutual relation. Lifestyles are social practices and ways of living that reflect
identities. Lifestyles may be seen as individual and collective expressions of differences.
This study can be situated in the qualitative research tradition, where typically a
restricted number of cases is studied on several related aspects. In this study sixty parents
have been individually questioned: mothers and fathers of different social backgrounds.
The results of the study do not permit statistical generalisation to Athe parents in general@.
They do, however, permit generalisation to social inheritance. The empirical results of the
study are considered in chapters 4, 5 and 6. With respect to the values and skills which
parents find important for their children, three main issues can be distinguished:
education, personal growth, and social skills. Of these, the parents in this study stress the
importance of personal growth and social skills. In what respects do parents make
reference to their own upbringing? Most of them remember education as an important goal
from their own youth, unlike to personal growth, which they do not remember. Social skills
are also remembered, but in a more religious context.?Social Legacies
220
How do parents deal with the task of raising their children? To whom do they turn for
support? Do they rely predominantly on their families, or do they rather turn to friends or
to professionals? The parents in this study virtually took no advice from professionals and
did not read magazines about raising children. Instead, they received most of their support
from their own social networks. I distinguished two types of networks: the family network,
dominated by kinship ties, and the mixed network, dominated by people other than kin.
In all networks, however, members of the family play a significant role.
Yet, this does not mean that kinship ties have the same meaning in every network. I
found three different types of meaning: parents may have a predominantly family network,
because they lack the skills to turn to others and therefore have no choice but to rely on
their family members. Second, there are parents with a traditional orientation, who
automatically turn to their family. A third motive is found in parents who make a
deliberate choice to turn to their family. These parents are mostly higher educated. In all
cases, the social networks of both mothers and fathers are largely made up of women. In
family networks the support seems to be of an encompassing nature, whereas in mixed
networks the support is more differentiated.
What different meanings are attributed to social legacies? The most positive valuation
was found among parents who see their legacy as a guideline: they see their own
upbringing as a criterion for the way they want to raise their own children. Next are
parents who see their legacy as an inspiration; they have selected a specific theme from
their past, which they explicitly wish to pass on to their children. A legacy of Amixed
feelings@ is found among parents who have neither a specifically positive, nor a
specifically negative attitude towards their own upbringing, or who are ambivalent
towards their past. Parents who are predominantly negative in their valuation of their own
upbringing are classified as seeing their legacy as a burden. These parents wish to follow
a different path in raising their children. The most striking conclusion in my opinion is
that social legacies are extremely important for most parents. In almost all the interviews
I conducted, parents spontaneously told me about their legacies, often in considerable
detail.
The last chapter of the book presents an answer to the question concerning the
relationship between social legacies, social networks, and the values which parents find
important for their children. As the starting point for this synthesis, I took social legacies,
since these were found to be important for the parents. I applied the lifestyle approach to
the raising of children, and called it the stylization of child-rearing. Stylization may be
regarded as referring to the essential &gt;colour= of child-rearing: it is stylized by the meaning
that every individual parent derives from her or his unique past. In this study five different
patterns of stylization were discerned.
A style of intergenerational continuity was found among parents of lower education, who
refer to the past to express the values they find important. These values are education and
social skills, and they have a family network. They see their legacy as a guideline in a
changing society.?Summary
221
A style of intergenerational innovation was found among more highly educated parents,
who see their legacy as inspiration. They have quite large mixed networks and see personal
growth and social skills as most important for their children.
A style of intergenerational ambivalence corresponds with a legacy of mixed feelings and
with large mixed networks. These parents are also more highly educated, have quite large
mixed networks and adhere to values of personal growth and social skills.
A style of intergenerational burdening. Parents with this style of child-rearing have a
negative legacy. They have predominantly bad memories of their own childhood, and wish
to raise their own children in a different manner. Their attitude can be characterized as
Ayou have to make the best of it.@ Most of them have lower education and their relationship
with their kinfolk is strained. They have not been able to break away from their past and
are stuck with relatively small family networks.
A style of intergenerational fracturing. Finally, there are parents who experience their
legacy as a burden, but have been succeeded in breaking with it, assisted by a supportive
social network. Their attitude is dominated by the wish to surpass their heritage. Their
networks are mixed; the attitudes and goals in child-rearing of these parents are the most
discontinuous with respect to their legacy. This style is found among parents with both
lower and higher education alike.
The book ends with a reflection on the way in which social legacies stylize child-rearing.
The past provides chances which shape lifestyle choices. The interplay of chance and
choice forms the basis of the concept of lifestyle. In the theoretical discussion about
modernization, much attention is devoted to individualization processes, which are seen
as a central feature of society in a stage of high modernity, and held to be responsible for
the fundamental changes in the way personal relationships are formed. These changes are
reflected in the expectations concerning children. The need to choose and the constant
interchange of orientations and values are strongly emphasized in this school of thought.
An important outcome of this study is that social legacies were found to be extremely
influential in the raising of a subsequent generation. Parents have many different ways of
giving meaning to their legacy; the variations are considerable. I have described five
different patterns of the ways in which social legacies can influence child-rearing
practices, calling them: stylizations of child-rearing. Parents in this study showed
convincingly that they use their social legacies in many different ways as a source of
support in a changing society.
In this study, the thesis on individualization is refined: in a rapidly changing society,
the influence of the past undoubtedly exists. A radical stance on individualization does not
seem appropriate in this respect. I would like to emphasize the importance of studying the
ways in which people give meaning to their past, without falling into the trap of creating
too static an image of a fixed past. The introduction of social inheritance as a sensitizing
concept can be a starting point. This study provides insight into the many different ways
in which parents make sense of their own rearing in the past. This may give a deeper?Social Legacies
222
understanding of the choices parents make, the goals for which they strive, and the values
to which they adhere in raising their children.</span></div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent publication-recommendation-item__more" href="publication/46606195_Sociale_Erfenissen_Orientaties_van_ouders_bij_het_opvoeden">Read more</a></div></div><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up publication-recommendation-item__item"><div class="publication-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up publication-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Article</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit publication-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare gtm-recommendation-item-link" href="publication/17006498_Information_Processing_and_Cognitive_Development">Information Processing and Cognitive Development</a></h3><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-grey-500"><span>February 1982</span> · <span>Advances in child development and behavior</span></div><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-auto"><li class="nova-e-list__item"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/39105772_Robert_Kail">Robert Kail</a></span></span></li><li class="nova-e-list__item"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/10658751_Jeffrey_Bisanz">Jeffrey Bisanz</a></span></span></li></ul><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 publication-recommendation-item__abstract">This chapter characterizes information processing as a general framework for understanding human cognitive growth that has had enormous impact on the study of cognition and over the past decade it has been adopted by a growing number of developmental psychologists. Journals and books contain numerous articles about the development of information-processing skills in children, in sharp contrast to <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-5f02efd5361a4&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}">... [Show full abstract]</a><span class="lite-page-hidden js-target-5f02efd5361a4"> the recent past when such topics were only rarely mentioned. It is emphasized that information processing, as a general perspective, has considerable potential for developmental work, and some relevant characteristics and implications of information processing is described. Information processing was not influential in developmental psychology until the late 1960s and early 1970s. During this period two essentially independent events brought information processing to the forefront of developmental research. First, psychologists studying the development of attention and memory based their work, in part, on information-processing models derived from experimental psychology. Second, several psychologists from the information-processing became interested in Piaget&#x27;s description of children&#x27;s understanding of concepts like transitivity and class inclusion. These psychologists proposed radically different interpretations of the phenomena, and their research sometimes produced findings that were hard to reconcile with Piaget&#x27;s account of development.</span></div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent publication-recommendation-item__more" href="publication/17006498_Information_Processing_and_Cognitive_Development">Read more</a></div></div><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up publication-recommendation-item__item"><div class="publication-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up publication-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Article</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit publication-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare gtm-recommendation-item-link" href="publication/316094893_What_about_dads_A_latent_and_growth_mixture_modeling_study_of_fathers&#x27;_anger_anxiety_and_depression">What about dads? ? A latent and growth mixture modeling study of fathers&#x27; anger, anxiety, and depres...</a></h3><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-grey-500"><span>January 2016</span></div><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-auto"><li class="nova-e-list__item"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/2110128502_Anna_Rhoad_Drogalis">Anna Rhoad Drogalis</a></span></span></li></ul><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 publication-recommendation-item__abstract">Little work has been conducted on the impact of fathers’ social-emotional functioning over time on children’s socioemotional and cognitive functioning. The bulk of research on children’s social-emotional outcomes has concentrated on mothers and has largely neglected fathers. As fathers become more involved with child care, it becomes increasingly important to understand the influence of their <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-5f02efd5365a9&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}">... [Show full abstract]</a><span class="lite-page-hidden js-target-5f02efd5365a9"> mental health on children. Growth mixture modeling analyses in the current study failed to uncover unobserved developmental subpopulations of fathers’ changing depressive, anger, and anxiety symptoms during their children’s primary school years. Regression analyses revealed that the association between parents’ psychology and child behavior varied based on the informant of child behavior. Ancillary analyses attempted to explain the informant inconsistencies by including all of the informants’ reports in a single model and testing a causal relationship between parent symptoms and the error terms associated with parent ratings. Structural equation models indicated a modest effect of both fathers’ and mothers’ depression, anger, and anxiety symptoms on distortion of the error terms associated with children’s problem behavior ratings. These results suggest that parental psychopathology symptoms lead to a perceptual bias in child ratings and cause modest overreporting of problem behaviors.</span></div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent publication-recommendation-item__more" href="publication/316094893_What_about_dads_A_latent_and_growth_mixture_modeling_study_of_fathers&#x27;_anger_anxiety_and_depression">Read more</a></div></div><div class="nova-o-grid__column nova-o-grid__column--width-6/12@m-up publication-recommendation-item__item"><div class="publication-recommendation-item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up publication-recommendation-item__item-badges"><div class="nova-l-flex__item"><span class="nova-e-badge nova-e-badge--color-green nova-e-badge--display-inline nova-e-badge--luminosity-high nova-e-badge--size-l nova-e-badge--theme-solid nova-e-badge--radius-m">Article</span></div></div><h3 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-inherit publication-recommendation-item__title"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare gtm-recommendation-item-link" href="publication/44350934_Psicologia_del_Desarrollo_edad_escolar_Ellen_A_Strommen_et_al_traduccion_de_Pedro_Rivera_Ramirez_revisado_por_Ma_de_Lourdes_Schnaas">Psicología del Desarrollo : edad escolar / Ellen A. Strommen .. . [et al.], traducción de Pedro Rive...</a></h3><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-xs nova-e-text--color-grey-500"><span></span></div><ul class="nova-e-list nova-e-list--size-m nova-e-list--type-inline nova-e-list--spacing-auto"><li class="nova-e-list__item"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/25801200_Ellen_A_Strommen">Ellen A Strommen</a></span></span></li><li class="nova-e-list__item"><span class="nova-v-person-inline-item"><span class="nova-v-person-inline-item__fullname"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/scientific-contributions/828886_Pedro_Rivera_Ramirez">Pedro Rivera Ramírez</a></span></span></li><li class="nova-e-list__item"><span class="nova-v-person-inline-item"><span class="lite-person-inline-item__image"><picture class="lite-page-avatar lite-page-avatar--size-xs lite-page-avatar--radius-full"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="https://i1.rgstatic.net/ii/profile.image/322909099233281-1453998936449_Q64/Lourdes_Schnaas.jpg" class="lite-image" alt="Lourdes Schnaas"/></picture></span><span class="nova-v-person-inline-item__fullname"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="https://www.researchgate.net/profile/Lourdes_Schnaas">Lourdes Schnaas</a></span></span></li></ul><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-grey-500 publication-recommendation-item__abstract">Traducción de: Developmental psichology: The school-aged child Incluye bibliografía e índice </div><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-silent publication-recommendation-item__more" href="publication/44350934_Psicologia_del_Desarrollo_edad_escolar_Ellen_A_Strommen_et_al_traduccion_de_Pedro_Rivera_Ramirez_revisado_por_Ma_de_Lourdes_Schnaas">Read more</a></div></div></div><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-auto" href="search"><span class="nova-c-button__label">Discover more</span></a></div></div></div><div id="lite-ad-mhgbq3vx7v" class="lite-page-ad lite-page-ad--has-label lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-mhgbq3vx7v-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-mhgbq3vx7v-close').onclick = function(e) {document.querySelector('#lite-ad-mhgbq3vx7v').style.display = 'none';};</script><div id="mhgbq3vx7v"><rg-ad id="ubcq3pgxkze" ad-unit="LoggedOut_PublicationWithFulltext_Middle" ad-unit-path="/83500759/LoggedOut_PublicationWithFulltext_Middle" ad-location="Middle" refresh-timeout="-1" load-on-init="false" collapse="never" class="rg-ad" ad-unit-sizes="[[728,90],[300,250],&quot;fluid&quot;]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="ubcq3pgxkze" targeting-sequence="2"><rg-ad-size size="[728,90]"></rg-ad-size><rg-ad-size size="[300,250]"></rg-ad-size><rg-ad-size size="&quot;fluid&quot;"></rg-ad-size><rg-ad-sizeset viewport-size="[0,0]" slot-sizes="[]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[250,0]" slot-sizes="[[250,250]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[300,0]" slot-sizes="[[250,250],[300,250],[300,100],[300,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[320,0]" slot-sizes="[[250,250],[300,250],[300,100],[300,50],[320,100],[320,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[336,0]" slot-sizes="[[336,280],[300,250],[250,250],[300,100],[300,50],[320,100],[320,50]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[992,0]" slot-sizes="[]"></rg-ad-sizeset></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("ubcq3pgxkze").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-mhgbq3vx7v').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-mhgbq3vx7v");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-mhgbq3vx7v').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-mhgbq3vx7v");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div><form method="GET" action="https://www.researchgate.net/lite.publication.PublicationDownloadCitationModal.downloadCitation.html"><div id="download-citation" class="lite-page-modal lite-page-modal--hidden lite-page-modal lite-page-modal--enable-click-outside"><div><div class="nova-c-modal nova-c-modal--color-green nova-c-modal--position-center nova-c-modal--spacing-xxl nova-c-modal--width-s" style="z-index:9998"><div class="nova-c-modal__overlay"></div><div class="nova-c-modal__container"><div class="nova-c-modal__window" role="dialog" aria-modal="true" tabindex="0"><header class="nova-c-modal__header nova-c-modal__header--spacing-none lite-page-modal__header"><div class="nova-c-modal__header-content"><button class="nova-c-button nova-c-button--align-left nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-grey nova-c-button--theme-bare nova-c-button--width-square lite-page-modal__close lite-page-modal__close--top-right" type="button"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-circle-s"></use></svg></button></div><div class="nova-c-modal__header-image"></div></header><div class="nova-c-modal__body nova-c-modal__body--spacing-inherit"><div class="nova-c-modal__body-content"><div class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-l nova-e-text--color-inherit">Download citation</div><div class="nova-l-form-group nova-l-form-group--layout-stack nova-l-form-group--gutter-s"><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><label class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-xxs nova-e-text--color-inherit nova-e-label" for=""><span class="nova-e-label__text">What type of file do you want?</span></label><div class="nova-l-form-group nova-l-form-group--layout-stack nova-l-form-group--gutter-s"><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div><label class="nova-e-radio"><input type="radio" class="nova-e-radio__input" name="fileType" value="RIS" checked=""/><span class="nova-e-radio__label"> RIS</span></label></div></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div><label class="nova-e-radio"><input type="radio" class="nova-e-radio__input" name="fileType" value="BibTeX"/><span class="nova-e-radio__label"> BibTeX</span></label></div></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div><label class="nova-e-radio"><input type="radio" class="nova-e-radio__input" name="fileType" value="Plain Text"/><span class="nova-e-radio__label"> Plain Text</span></label></div></div></div></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><label class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-xxs nova-e-text--color-inherit nova-e-label" for=""><span class="nova-e-label__text">What do you want to download?</span></label><div class="nova-l-form-group nova-l-form-group--layout-stack nova-l-form-group--gutter-s"><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div><label class="nova-e-radio"><input type="radio" class="nova-e-radio__input" name="citation" value="citation" checked=""/><span class="nova-e-radio__label"> Citation only</span></label></div></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div><label class="nova-e-radio"><input type="radio" class="nova-e-radio__input" name="citation" value="citationAndAbstract"/><span class="nova-e-radio__label"> Citation and abstract</span></label></div></div></div></div></div><input type="hidden" name="publicationUid" value="290749036"/></div></div><footer class="nova-c-modal__footer nova-c-modal__footer--align-right nova-c-modal__footer--spacing-inherit"><div class="nova-c-modal__footer-content"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-auto"><span class="nova-c-button__label">Download</span></button></div></footer><div class="nova-e-spinner nova-e-spinner--size-m nova-e-spinner--color-inherit nova-c-modal__spinner"><div class="nova-e-spinner__container"><svg width="16" height="16" viewBox="0 0 16 16" class="nova-e-spinner__asset"><g stroke-width="2"><path class="nova-e-spinner__circle" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path><path class="nova-e-spinner__stroke" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path></g></svg></div></div></div></div></div></div></div></form><div id="exit-modal" class="lite-page-modal lite-page-modal--hidden lite-page-modal lite-page-modal--enable-click-outside"><div><div class="nova-c-modal nova-c-modal--color-green nova-c-modal--position-center nova-c-modal--spacing-xl nova-c-modal--width-s" style="z-index:9998"><div class="nova-c-modal__overlay"></div><div class="nova-c-modal__container"><div class="nova-c-modal__window" role="dialog" aria-modal="true" tabindex="0"><header class="nova-c-modal__header nova-c-modal__header--spacing-none lite-page-modal__header"><div class="nova-c-modal__header-content"><button class="nova-c-button nova-c-button--align-left nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-grey nova-c-button--theme-bare nova-c-button--width-square lite-page-modal__close lite-page-modal__close--top-right" type="button"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-circle-s"></use></svg></button></div><div class="nova-c-modal__header-image"></div></header><div class="nova-c-modal__body nova-c-modal__body--spacing-inherit"><div class="nova-c-modal__body-content"><h2 class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-auto nova-e-text--color-inherit">Looking for the full-text?</h2><p class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-l nova-e-text--color-inherit">You can request the full-text of this book directly from the authors on ResearchGate.</p><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-full js-lite-click" type="button" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.lite-page&quot;,&quot;a&quot;:&quot;ft(https://www.researchgate.net/lite.publication.PublicationRequestFulltextPromo.requestFulltext.html?publicationUid=290749036&amp;ev=su_requestFulltext)&quot;}]}"><span class="nova-c-button__label">Request full-text</span></button></div></div><footer class="nova-c-modal__footer nova-c-modal__footer--align-right nova-c-modal__footer--spacing-inherit"><div class="nova-c-modal__footer-content"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">Already a member? <a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="login">Log in</a></div></div></footer><div class="nova-e-spinner nova-e-spinner--size-m nova-e-spinner--color-inherit nova-c-modal__spinner"><div class="nova-e-spinner__container"><svg width="16" height="16" viewBox="0 0 16 16" class="nova-e-spinner__asset"><g stroke-width="2"><path class="nova-e-spinner__circle" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path><path class="nova-e-spinner__stroke" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path></g></svg></div></div></div></div></div></div></div><div class="nova-c-card nova-c-card--spacing-xs nova-c-card--elevation-none native-app-recruitment-container native-app-recruitment-container native-app-recruitment-container--hidden js-target-ios-banner lite-page-hidden js-lite-trackingloaded" data-lite="{&quot;trackingloaded&quot;:[{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;rc(lite-page-hidden)&quot;,&quot;c&quot;:[&quot;hnsk(declinedIOSbanner)&quot;,4,5]}]}"><div class="nova-c-card__body nova-c-card__body--spacing-inherit"><div class="nova-o-stack nova-o-stack--gutter-xxs nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-s nova-l-flex--direction-row@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-xs nova-c-button--color-blue nova-c-button--theme-bare nova-c-button--width-auto js-lite-click" type="button" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-ios-banner&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;},{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;af(declinedIOSbanner)&quot;}]}"><span class="nova-c-button__label"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-grey nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></span></button></div><div class="nova-l-flex__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-silent" href="go.GetApp.html?_sg=zlfOM79NEpWhlzS3sXMCXFaHEAnsOFj67eTf41CTamvM-wBQVn688h-Dzl3jk8fZy4chYXFfynRDSwLE-aPWyvrrek6u83fL6XaLcOFyKmI&amp;originCh=bannerStatsCopy&amp;relativePath=publicationLoggedOut&amp;interested=true"><picture class="lite-page-avatar lite-page-avatar--size-l lite-page-avatar--radius-none native-app-recruitment-container__app-image" src="https://www.researchgate.net/images/nativeApp/rg_app_ios_logo_small.png"> <img src="https://www.researchgate.net/images/nativeApp/rg_app_ios_logo_small.png" class="lite-image" alt="RG Logo"/></picture></a></div><div class="nova-l-flex__item nova-l-flex__item--grow"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-silent" href="go.GetApp.html?_sg=zlfOM79NEpWhlzS3sXMCXFaHEAnsOFj67eTf41CTamvM-wBQVn688h-Dzl3jk8fZy4chYXFfynRDSwLE-aPWyvrrek6u83fL6XaLcOFyKmI&amp;originCh=bannerStatsCopy&amp;relativePath=publicationLoggedOut&amp;interested=true"><div class="nova-o-stack nova-o-stack--gutter-xxs nova-o-stack--spacing-none nova-o-stack--no-gutter-outside"><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><b>ResearchGate iOS App</b></div></div><div class="nova-o-stack__item"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-500">Get it from the App Store now.</div></div></div></a></div><div class="nova-l-flex__item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-auto" href="go.GetApp.html?_sg=zlfOM79NEpWhlzS3sXMCXFaHEAnsOFj67eTf41CTamvM-wBQVn688h-Dzl3jk8fZy4chYXFfynRDSwLE-aPWyvrrek6u83fL6XaLcOFyKmI&amp;originCh=bannerStatsCopy&amp;relativePath=publicationLoggedOut&amp;interested=true"><span class="nova-c-button__label">Install</span></a></div></div></div><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-silent" href="go.GetApp.html?_sg=zlfOM79NEpWhlzS3sXMCXFaHEAnsOFj67eTf41CTamvM-wBQVn688h-Dzl3jk8fZy4chYXFfynRDSwLE-aPWyvrrek6u83fL6XaLcOFyKmI&amp;originCh=bannerStatsCopy&amp;relativePath=publicationLoggedOut&amp;interested=true"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-green nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#check-circle-s"></use></svg></div><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-800">Keep up with your stats and more</div></div></div></a></div><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-silent" href="go.GetApp.html?_sg=zlfOM79NEpWhlzS3sXMCXFaHEAnsOFj67eTf41CTamvM-wBQVn688h-Dzl3jk8fZy4chYXFfynRDSwLE-aPWyvrrek6u83fL6XaLcOFyKmI&amp;originCh=bannerStatsCopy&amp;relativePath=publicationLoggedOut&amp;interested=true"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxs nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-green nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#check-circle-s"></use></svg></div><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-800">Access scientific knowledge from anywhere</div></div></div></a></div></div></div></div></aside><header class="lite-page__header" role="banner"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-none nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up lite-page__header-wrapper"><div class="nova-l-flex__item"><a href="https://www.researchgate.net/" rel="home" class="lite-page__header-logo-link"><picture class="brand" width="124" height="17"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==" data-src="images/icons/svgicons/researchgate-logo-white.svg" class="lite-image" alt="ResearchGate Logo" width="124" height="17"/></picture></a></div><div class="nova-l-flex__item"><div class="nova-o-pack nova-o-pack--gutter-m nova-o-pack--width-auto nova-o-pack--vertical-align-middle lite-page__header-search"><div class="nova-o-pack__item item"><form method="GET" action="search" class="lite-page__header-search-input-wrapper"><input type="hidden" name="context" readonly="" value="publicSearchHeader"/><input placeholder="Search for publications, researchers, or questions" name="q" autoComplete="off" class="lite-page__header-search-input"/><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-green nova-c-button--theme-bare nova-c-button--width-square lite-page__header-search-button" type="submit"><span class="nova-c-button__label"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#magnifier-s"></use></svg></span></button></form></div><div class="nova-o-pack__item item"><div class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-white lite-page__header-search-or">or</div></div><div class="nova-o-pack__item item"><a class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-s nova-c-button--color-white nova-c-button--theme-ghost nova-c-button--width-auto discover-button" href="topics"><span class="nova-c-button__label">Discover by subject area</span></a></div></div></div><div class="nova-l-flex__item nova-l-flex__item--grow"><ul class="lite-page__header-links"><li class="hide-m"><a href="scientific-recruitment/?utm_source=researchgate&amp;utm_medium=community-loggedout&amp;utm_campaign=indextop" class="lite-page__header-link">Recruit researchers</a></li><li class="lite-page__signup-element"><a href="signup.SignUp.html?hdrsu=1&amp;_sg%5B0%5D=PAEO6JsDdFSGFMWIb7noqkukVzAyZCkACyWGxFTdw5L2p-fwXCrqEX5eneDSo1FoDQxLw9msCh3K8h3PDveVz3EK7XY" class="lite-page__header-link">Join for free</a></li><li><div class="lite-page__header-login"><a class="lite-page__header-link js-lite-click" data-lite="{&quot;click&quot;:[{&quot;t&quot;:&quot;.js-target-login&quot;,&quot;a&quot;:&quot;tc(lite-page-hidden)&quot;,&quot;r&quot;:&quot;rco&quot;},{&quot;t&quot;:&quot;.js-target-trusted-device&quot;,&quot;a&quot;:&quot;ac(lite-page-hidden)&quot;}]}">Login</a><div class="lite-page__header-login-container lite-page-hidden js-target-login"><form method="post" action="https://www.researchgate.net/login?_sg=CMwCwlnZ7ByyGKcdEbaiYfYxe5fzXo6fxUVhELNheTuc5NvwIwKJLh1_og7aTePfHjZskCFHSyyh0g" name="loginForm" id="headerLoginForm"><input type="hidden" name="request_token" value="aad-Ux+cH7umPFKKVJ5MBGrUI70cbPRdBteXZWpM+dImQ5w1QWX8FpPm6j99xD3smbueXLrV5C2suCd+K2ezCJZQH+/deabfnC3jvZgMuAFl116qCjMv+N0AUh1cfo1cuzjlB1Y675W0IYFo+RZH7Q+DXlOkH+WeUckfs/BeqiofUPQ9CDi8ZC5cISAksvVViy2wFnBA6BcktWJoOqPrzGJuLqEXLWhly7w0TQMNJDevCKfFEawy92jSbQKbNyxdOrF053tbGxo9S8Fj0Xh0A5o="/><input type="hidden" name="urlAfterLogin" value="publication/290749036_Developmental_Robotics_From_Babies_to_Robots"/><input type="hidden" name="invalidPasswordCount" value="0"/><input type="hidden" name="headerLogin" value="yes"/><div class="lite-page__header-login-item"><label class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit lite-page__header-login-label" for="input-header-login">Email <div class="lite-page-tooltip"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#info-circle-s"></use></svg><div class="lite-page-tooltip__content lite-page-tooltip__content--above"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><b>Tip:</b> Most researchers use their institutional email address as their ResearchGate login</div><div class="lite-page-tooltip__arrow lite-page-tooltip__arrow--above"><div class="lite-page-tooltip__arrow-tip"></div></div></div></div></label></div><input type="email" required="" id="input-header-login" name="login" autoComplete="email" tabindex="1" placeholder="" class="nova-e-input__field nova-e-input__field--size-m lite-page__header-login-item nova-e-input__ambient nova-e-input__ambient--theme-default"/><div class="lite-page__header-login-item"><label class="lite-page__header-login-label" for="input-header-password">Password</label><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-bare lite-page__header-login-forgot" href="application.LostPassword.html">Forgot password?</a></div><input type="password" required="" id="input-header-password" name="password" autoComplete="current-password" tabindex="2" placeholder="" class="nova-e-input__field nova-e-input__field--size-m lite-page__header-login-item nova-e-input__ambient nova-e-input__ambient--theme-default"/><div><label class="nova-e-checkbox lite-page__header-login-checkbox"><input type="checkbox" class="nova-e-checkbox__input" aria-invalid="false" name="setLoginCookie" tabindex="3" value="yes" checked=""/><span class="nova-e-checkbox__label"> Keep me logged in</span></label></div><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-m nova-l-flex--direction-column@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-center@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-full" type="submit" tabindex="4"><span class="nova-c-button__label">Log in</span></button></div><div class="nova-l-flex__item nova-l-flex__item--align-self-center@s-up"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">or</div></div><div class="nova-l-flex__item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-m nova-l-flex--direction-column@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><a href="connector/facebook"><img width="247" height="40" src="https://c5.rgstatic.net/m/429978991806330/images/socialNetworks/loginVia/continue-with-facebook.png"/></a></div><div class="nova-l-flex__item"><a href="connector/linkedin"><div style="display:inline-block;width:247px;height:40px;text-align:left;border-radius:2px;white-space:nowrap;background:#0077B5"><span style="margin:1px 0 0 1px;display:inline-block;vertical-align:middle;width:38px;height:38px;background-color:#0077B5"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-m nova-e-icon--theme-bare nova-e-icon--color-white nova-e-icon--luminosity-medium" style="padding:8px 0 0 8px"><use xlink:href="/m/471424272954627/images/icons/nova/icon-stack-m.svg#social-linkedin-m"></use></svg></span><span style="color:#FFF;display:inline-block;vertical-align:middle;padding-left:15px;padding-right:42px;font-size:16px;font-family:Roboto, sans-serif">Continue with LinkedIn</span></div></a></div><div class="nova-l-flex__item"><a href="connector/google"><div style="display:inline-block;width:247px;height:40px;text-align:left;border-radius:2px;white-space:nowrap;color:#444;background:#4285F4"><span style="margin:1px 0 0 1px;display:inline-block;vertical-align:middle;width:38px;height:38px;background:url(&#x27;images/socialNetworks/logos-official-2019-05/google-logo.svg&#x27;) transparent 50% no-repeat"></span><span style="color:#FFF;display:inline-block;vertical-align:middle;padding-left:15px;padding-right:42px;font-size:16px;font-family:Roboto, sans-serif">Continue with Google</span></div></a></div></div></div></div></form></div><div class="lite-page-hidden js-lite-domready" data-lite="{&quot;domready&quot;:[{&quot;t&quot;:&quot;self&quot;,&quot;a&quot;:&quot;lawg(lite.TrustedDeviceLoginPromo.html)&quot;,&quot;c&quot;:1}]}"></div><div id="login-modal" class="lite-page-modal lite-page-modal--hidden lite-page-modal lite-page-modal--enable-click-outside"><div><div class="nova-c-modal nova-c-modal--color-green nova-c-modal--position-center nova-c-modal--spacing-xxxl nova-c-modal--width-s lite-page-modal__login" style="z-index:9998"><div class="nova-c-modal__overlay"></div><div class="nova-c-modal__container"><div class="nova-c-modal__window" role="dialog" aria-modal="true" tabindex="0"><header class="nova-c-modal__header nova-c-modal__header--spacing-none lite-page-modal__header"><div class="nova-c-modal__header-content"><button class="nova-c-button nova-c-button--align-left nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-grey nova-c-button--theme-bare nova-c-button--width-square lite-page-modal__close lite-page-modal__close--top-right" type="button"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-circle-s"></use></svg></button></div><div class="nova-c-modal__header-image"></div></header><div class="nova-c-modal__body nova-c-modal__body--spacing-inherit"><div class="nova-c-modal__body-content"><div class="nova-e-text nova-e-text--size-xl nova-e-text--family-sans-serif nova-e-text--spacing-s nova-e-text--color-inherit lite-page-center">Welcome back! Please log in.</div><form method="post" action="https://www.researchgate.net/login?_sg=CMwCwlnZ7ByyGKcdEbaiYfYxe5fzXo6fxUVhELNheTuc5NvwIwKJLh1_og7aTePfHjZskCFHSyyh0g" name="loginForm" id="modalLoginForm"><input type="hidden" name="request_token" value="aad-Ux+cH7umPFKKVJ5MBGrUI70cbPRdBteXZWpM+dImQ5w1QWX8FpPm6j99xD3smbueXLrV5C2suCd+K2ezCJZQH+/deabfnC3jvZgMuAFl116qCjMv+N0AUh1cfo1cuzjlB1Y675W0IYFo+RZH7Q+DXlOkH+WeUckfs/BeqiofUPQ9CDi8ZC5cISAksvVViy2wFnBA6BcktWJoOqPrzGJuLqEXLWhly7w0TQMNJDevCKfFEawy92jSbQKbNyxdOrF053tbGxo9S8Fj0Xh0A5o="/><input type="hidden" name="urlAfterLogin" value="publication/290749036_Developmental_Robotics_From_Babies_to_Robots"/><input type="hidden" name="invalidPasswordCount" value="0"/><input type="hidden" name="modalLogin" value="yes"/><div class="nova-l-form-group nova-l-form-group--layout-stack nova-l-form-group--gutter-s"><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><label class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-xxs nova-e-text--color-inherit nova-e-label" for="input-modal-login-label"><span class="nova-e-label__text">Email <div class="lite-page-tooltip"><span class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-500">· Hint</span><div class="lite-page-tooltip__content lite-page-tooltip__content--above"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit"><b>Tip:</b> Most researchers use their institutional email address as their ResearchGate login</div><div class="lite-page-tooltip__arrow lite-page-tooltip__arrow--above"><div class="lite-page-tooltip__arrow-tip"></div></div></div></div></span></label><input type="email" required="" id="input-modal-login" name="login" autoComplete="email" tabindex="1" placeholder="Enter your email" class="nova-e-input__field nova-e-input__field--size-m nova-e-input__ambient nova-e-input__ambient--theme-default"/></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div class="lite-page-modal__forgot"><label class="nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-xxs nova-e-text--color-inherit nova-e-label" for="input-modal-password-label"><span class="nova-e-label__text">Password</span></label><a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-bare lite-page-modal__forgot-link" href="application.LostPassword.html">Forgot password?</a></div><input type="password" required="" id="input-modal-password" name="password" autoComplete="current-password" tabindex="2" placeholder="" class="nova-e-input__field nova-e-input__field--size-m nova-e-input__ambient nova-e-input__ambient--theme-default"/></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div><label class="nova-e-checkbox"><input type="checkbox" class="nova-e-checkbox__input" aria-invalid="false" checked="" value="yes" name="setLoginCookie" tabindex="3"/><span class="nova-e-checkbox__label"> Keep me logged in</span></label></div></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-m nova-c-button--size-m nova-c-button--color-blue nova-c-button--theme-solid nova-c-button--width-full" type="submit" tabindex="4"><span class="nova-c-button__label">Log in</span></button></div><div class="nova-l-form-group__item nova-l-form-group__item--width-auto@m-up"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-m nova-l-flex--direction-column@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">or</div></div><div class="nova-l-flex__item"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-m nova-l-flex--direction-column@s-up nova-l-flex--align-items-center@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item"><a href="connector/facebook"><img width="247" height="40" src="https://c5.rgstatic.net/m/429978991806330/images/socialNetworks/loginVia/continue-with-facebook.png"/></a></div><div class="nova-l-flex__item"><a href="connector/linkedin"><div style="display:inline-block;width:247px;height:40px;text-align:left;border-radius:2px;white-space:nowrap;background:#0077B5"><span style="margin:1px 0 0 1px;display:inline-block;vertical-align:middle;width:38px;height:38px;background-color:#0077B5"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-m nova-e-icon--theme-bare nova-e-icon--color-white nova-e-icon--luminosity-medium" style="padding:8px 0 0 8px"><use xlink:href="/m/471424272954627/images/icons/nova/icon-stack-m.svg#social-linkedin-m"></use></svg></span><span style="color:#FFF;display:inline-block;vertical-align:middle;padding-left:15px;padding-right:42px;font-size:16px;font-family:Roboto, sans-serif">Continue with LinkedIn</span></div></a></div><div class="nova-l-flex__item"><a href="connector/google"><div style="display:inline-block;width:247px;height:40px;text-align:left;border-radius:2px;white-space:nowrap;color:#444;background:#4285F4"><span style="margin:1px 0 0 1px;display:inline-block;vertical-align:middle;width:38px;height:38px;background:url(&#x27;images/socialNetworks/logos-official-2019-05/google-logo.svg&#x27;) transparent 50% no-repeat"></span><span style="color:#FFF;display:inline-block;vertical-align:middle;padding-left:15px;padding-right:42px;font-size:16px;font-family:Roboto, sans-serif">Continue with Google</span></div></a></div></div></div><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-grey-500 lite-page-center">No account? <a class="nova-e-link nova-e-link--color-blue nova-e-link--theme-decorated" href="signup.SignUp.html?hdrsu=1&amp;_sg%5B0%5D=PAEO6JsDdFSGFMWIb7noqkukVzAyZCkACyWGxFTdw5L2p-fwXCrqEX5eneDSo1FoDQxLw9msCh3K8h3PDveVz3EK7XY">Sign up</a></div></div></div></div></div></form></div></div><div class="nova-e-spinner nova-e-spinner--size-m nova-e-spinner--color-inherit nova-c-modal__spinner"><div class="nova-e-spinner__container"><svg width="16" height="16" viewBox="0 0 16 16" class="nova-e-spinner__asset"><g stroke-width="2"><path class="nova-e-spinner__circle" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path><path class="nova-e-spinner__stroke" d="M8,1C4.1,1,1,4.1,1,8s3.1,7,7,7s7-3.1,7-7S11.9,1,8,1"></path></g></svg></div></div></div></div></div></div></div></div></li></ul></div></div></header><footer class="lite-page__footer" role="contentinfo"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xs nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-center@s-up nova-l-flex--wrap-nowrap@s-up lite-page__footer-native-app"><div class="nova-l-flex__item"><span style="background:none"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="go.GetApp.html?interested=true&amp;originCh=footerLoggedOut"><img src="https://i1.rgstatic.net/images/nativeApp/app_store_light_logo.svg" alt="App Store" style="height:48px;width:147px"/></a></span></div></div><div class="lite-page__footer-index"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-m nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-nowrap@s-up"><div class="nova-l-flex__item nova-l-flex__item--grow"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-xxxxl nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up lite-page constrained-box-sizing"><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-m nova-e-text--color-inherit lite-page__footer-index-title">Company</div><div class="nova-o-stack nova-o-stack--gutter-xs nova-o-stack--spacing-none nova-o-stack--no-gutter-outside lite-page__footer-index-column"><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="about">About us</a></div><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="blog">News</a></div><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="careers">Careers</a></div></div></div><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-m nova-e-text--color-inherit lite-page__footer-index-title">Support</div><div class="nova-o-stack nova-o-stack--gutter-xs nova-o-stack--spacing-none nova-o-stack--no-gutter-outside lite-page__footer-index-column"><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="https://explore.researchgate.net/?utm_source=researchgate&amp;utm_medium=community-loggedout&amp;utm_campaign=new-footer&amp;utm_content=helpcenter">Help Center</a></div></div></div><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-l nova-e-text--family-sans-serif nova-e-text--spacing-m nova-e-text--color-inherit lite-page__footer-index-title">Business solutions</div><div class="nova-o-stack nova-o-stack--gutter-xs nova-o-stack--spacing-none nova-o-stack--no-gutter-outside lite-page__footer-index-column"><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="https://solutions.researchgate.net/advertising/?utm_source=researchgate&amp;utm_medium=community-loggedout&amp;utm_campaign=new-footer&amp;utm_content=advertising">Advertising</a></div><div class="nova-o-stack__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-decorated" href="scientific-recruitment?utm_source=researchgate&amp;utm_medium=community-loggedout&amp;utm_campaign=new-footer&amp;utm_content=recruiting">Recruiting</a></div></div></div></div></div><div class="nova-l-flex__item nova-l-flex__item--shrink lite-page__footer-ad-slot"><div id="lite-ad-6zl2fzxjzlg" class="lite-page-ad lite-page-ad--has-label lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-6zl2fzxjzlg-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-6zl2fzxjzlg-close').onclick = function(e) {document.querySelector('#lite-ad-6zl2fzxjzlg').style.display = 'none';};</script><div id="6zl2fzxjzlg"><rg-ad id="yvn2mnvoot" ad-unit="LoggedOut_PublicationWithoutFulltext_Footer" ad-unit-path="/83500759/LoggedOut_PublicationWithoutFulltext_Footer" ad-location="Footer" refresh-timeout="-1" load-on-init="false" collapse="never" class="rg-ad" ad-unit-sizes="[[336,280],[300,250],[250,250],&quot;fluid&quot;]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="yvn2mnvoot" targeting-sequence="1"><rg-ad-size size="[336,280]"></rg-ad-size><rg-ad-size size="[300,250]"></rg-ad-size><rg-ad-size size="[250,250]"></rg-ad-size><rg-ad-size size="&quot;fluid&quot;"></rg-ad-size><rg-ad-sizeset viewport-size="[0,0]" slot-sizes="[]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[768,0]" slot-sizes="[[336,280],[300,250],[250,250],&quot;fluid&quot;]"></rg-ad-sizeset></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("yvn2mnvoot").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-6zl2fzxjzlg').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-6zl2fzxjzlg");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-6zl2fzxjzlg').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-6zl2fzxjzlg");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div></div></div></div><div class="lite-page__footer-index"><div class="nova-l-flex__item nova-l-flex nova-l-flex--gutter-m nova-l-flex--direction-row@s-up nova-l-flex--align-items-stretch@s-up nova-l-flex--justify-content-flex-start@s-up nova-l-flex--wrap-wrap@s-up lite-page constrained-box-sizing"><div class="nova-l-flex__item"><div class="nova-e-text nova-e-text--size-s nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit">© 2008-2020 ResearchGate GmbH. All rights reserved.</div></div><div class="nova-l-flex__item nova-l-flex__item--grow lite-page__footer-base-right"><ul class="nova-e-list nova-e-list--size-s nova-e-list--type-inline nova-e-list--spacing-auto"><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="terms-of-service">Terms</a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="privacy-policy">Privacy</a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="ip-policy">Copyright</a></li><li class="nova-e-list__item"><a class="nova-e-link nova-e-link--color-inherit nova-e-link--theme-bare" href="imprint">Imprint</a></li></ul></div></div></div></footer><div id="lite-ad-q0hodtpzm1" class="lite-page-ad lite-page-ad--has-label lite-page-ad--catfish lite-page-ad--empty"><div class="lite-page-ad__inner"><div class="lite-page-ad__close"><button class="nova-c-button nova-c-button--align-center nova-c-button--radius-full nova-c-button--size-xs nova-c-button--color-grey nova-c-button--theme-solid nova-c-button--width-square" type="button" id="lite-ad-q0hodtpzm1-close"><svg aria-hidden="true" class="nova-e-icon nova-e-icon--size-s nova-e-icon--theme-bare nova-e-icon--color-inherit nova-e-icon--luminosity-medium nova-c-button__icon"><use xlink:href="/m/454327524547536/images/icons/nova/icon-stack-s.svg#close-s"></use></svg></button></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.querySelector('#lite-ad-q0hodtpzm1-close').onclick = function(e) {document.querySelector('#lite-ad-q0hodtpzm1').style.display = 'none';};</script><div id="q0hodtpzm1"><rg-ad id="8k77svtc7ie" ad-unit="LoggedOut_PublicationWithoutFulltext_Catfish" ad-unit-path="/83500759/LoggedOut_PublicationWithoutFulltext_Catfish" ad-location="Catfish" refresh-timeout="-1" load-on-init="true" collapse="beforeFetch" class="rg-ad" ad-unit-sizes="[[300,50],[300,100],[320,50],[320,100],&quot;fluid&quot;]" targeting-variants="nativo,prd,lite-page" targeting-snippet_id="8k77svtc7ie" targeting-sequence="1"><rg-ad-size size="[300,50]"></rg-ad-size><rg-ad-size size="[300,100]"></rg-ad-size><rg-ad-size size="[320,50]"></rg-ad-size><rg-ad-size size="[320,100]"></rg-ad-size><rg-ad-size size="&quot;fluid&quot;"></rg-ad-size><rg-ad-sizeset viewport-size="[0,0]" slot-sizes="[[320,50],[320,100],[300,50],[300,100]]"></rg-ad-sizeset><rg-ad-sizeset viewport-size="[768,0]" slot-sizes="[]"></rg-ad-sizeset></rg-ad><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">document.getElementById("8k77svtc7ie").addEventListener('adUnitEvent', function(event){if(event.detail.type==="slotRenderEnded"){ if(event.detail.gptEvent.isEmpty){document.querySelector('#lite-ad-q0hodtpzm1').classList.remove('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-q0hodtpzm1");elem && elem.classList.add('lite-page-ad--empty');}else{document.querySelector('#lite-ad-q0hodtpzm1').classList.add('lite-page-ad--rendered');var elem=document.querySelector("#lite-ad-q0hodtpzm1");elem && elem.classList.remove('lite-page-ad--empty');}}})</script></div></div></div><div class="js-target-questionnaire"></div></div><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">var r=new XMLHttpRequest;r.open("GET","/refreshToken?c="+(requestCachedConfig.correlationId||requestConfig.correlationId));r.setRequestHeader("x-rg-referer",window.location.href);r.onreadystatechange=function(){if(4===this.readyState&&200<=this.status&&400>this.status){var a=JSON.parse(this.responseText),b=document.getElementById("Rg-Request-Token");b&&b.setAttribute("content",a.requestToken);a.result&&a.result.trackingConfig&&(window.trackingConfig=a.result.trackingConfig,window.useQuantcast||window.loadDeferredObjects&&window.loadDeferredObjects())}};r.send();r=null;</script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">requestConfig.backendTime = 848</script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w" async src="https://www.researchgate.net/activeTriggers.js"></script>
<script src="https://c5.rgstatic.net/m/4142001695121077/javascript/lite/lite.js" crossorigin="anonymous" defer></script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">(function () {
if (document.cookie.indexOf('ga_opt_out') !== -1) {
    window['ga-disable-UA-58591210-1'] = true;
}
})();</script><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","https://www.google-analytics.com/analytics.js","ga");
ga("create","UA-58591210-1"); ga("set","anonymizeIp",true); ga("set", "dimension1", "publication no texts - redesigned");ga("set", "dimension2", "Book");ga("set", "dimension3", "Logged out");ga("set", "dimension4", "lite-page");ga("set", "dimension6", "n\/a");ga("set", "dimension7", "n\/a");ga("set", "dimension8", "n\/a");ga("set", "dimension9", "n\/a");ga("set", "dimension10", "Biology");ga("set", "dimension11", "Artificial_Intelligence");ga("set", "dimension12", "Biology Engineering Computer_Science");ga("set", "dimension13", "Artificial_Intelligence Human-computer_Interaction Computer_Engineering");(typeof requestCachedConfig!==typeof undefined&&requestCachedConfig.isCachedPage===true)?ga("set", "dimension19", "cache"):ga("set", "dimension19", "no cache");  ga("send","pageview");</script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">
document.addEventListener('DOMContentLoaded', function() {
if (typeof RGCommons === 'undefined') {
    window.rgCriticalScriptFailure = true;
    var e = new Error('Critical scripts were not loaded');
    e.name = 'CriticalScriptsError';
    throw e;
} else {
    RGCommons.parameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","utm_term","utm_content","el","ci","mfl"]);
}
});
</script><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">
window.addEventListener('load', function() {
if(typeof RGCommons!=="undefined"){if(typeof trackingConfig!==typeof undefined&&trackingConfig.isCrawler===false){(typeof requestCachedConfig!==typeof undefined&&requestCachedConfig.isCachedPage===true)?RGCommons.monitoring.monitorPage("https:\/\/glassmoni.researchgate.net", ["lite.PublicationDetails.run.html.loggedOut.get","lite.PublicationDetails.run.html.loggedOut.get.no-text","lite.PublicationDetails.run.html.loggedOut.get.subcategory-cached","lite.PublicationDetails.run.html.loggedOut.get.subcategory-no-text-cached"], "9723c55cb9c841461916947f0fb722912dc2fa1e"):RGCommons.monitoring.monitorPage("https:\/\/glassmoni.researchgate.net", ["lite.PublicationDetails.run.html.loggedOut.get","lite.PublicationDetails.run.html.loggedOut.get.no-text","lite.PublicationDetails.run.html.loggedOut.get.subcategory-non-cached","lite.PublicationDetails.run.html.loggedOut.get.subcategory-no-text-non-cached"], "ebe70b41ef77e145eeb36ff2c9d9af10c2203172");}if(typeof trackingConfig!==typeof undefined&&trackingConfig.isCrawler===true){RGCommons.monitoring.monitorPage("https:\/\/glassmoni.researchgate.net", ["lite.PublicationDetails.run.html.loggedOut.get","lite.PublicationDetails.run.html.loggedOut.get.no-text","lite.PublicationDetails.run.html.loggedOut.get.subcategory-crawler"], "683f719c433019b604fe7e70c5ad7b15a62b3a69");}}
});
</script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w"> dataLayer = [{"pageCategory":"publication no texts - redesigned","publicationType":"Book","eventCategory":"Publication page"}]; </script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">!function(a9,a,p,s,t,A,g){if(a[a9])return;function q(c,r){a[a9]._Q.push([c,r])}a[a9]={init:function(){q("i",arguments)},fetchBids:function(){q("f",arguments)},setDisplayBids:function(){},targetingKeys:function(){return[]},_Q:[]};A=p.createElement(s);A.async=!0;A.src=t;g=p.getElementsByTagName(s)[0];g.parentNode.insertBefore(A,g)}("apstag",window,document,"script","//c.amazon-adsystem.com/aax2/apstag.js");</script><script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">var rgDelivery=rgDelivery||{};rgDelivery.cmd=rgDelivery.cmd||[];var deliveryManager=deliveryManager||{_q:[]};deliveryManager.start=deliveryManager.start||function(){deliveryManager._q.push(['s', arguments]);};(function(){var tag=document.createElement("script");tag.async=true;tag.type="text/javascript";tag.src="https://www.researchgate.net/delivery/manager.js?rgKey=PB%3A290749036&qc=1";var node=document.getElementsByTagName("script")[0];node.parentNode.insertBefore(tag,node);})();</script>
<script nonce="dzGrak0sE1o680xOjuVVqjUd3rEbha7w">
document.addEventListener('DOMContentLoaded', function() {
deliveryManager.start();
});
</script></body></html>
